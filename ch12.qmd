# Chapter 12: Unsupervised Learning 
Unsupervised learning only observes features $X_1, X_2, \cdots, X_p$ (**unlabeled data**), no associated response $Y$. Obtaining such data may be easier (and cheaper) in contrast to **labeled data** which may require *costly* human intervention. The goal is to learn interesting information about the predictors. For instance, 1) reduce the dimension to 2 or 3 to visualize data in EDA 2) discover subgroups (clustering). 

Two methods: 
- PCA: for data visualization and pre-processing for supervised learning
- clustering: discovering unknown subgroups. 

Challenges: 
- no simple goal (response), hence may be more subjective

Important Applications:
- discover subgroups by the data itself: such as movie viewers, shoppers, patients

## PCA
PCA produces a low-dimensional representation of a dataset that explains a good fraction of the variance. It finds a sequence of new variables each of which is a linear combination of the original variables. Those new variables are mutually uncorrelated and are ordered in a decreasing order of variance. 

- PCA is used for data visualization and pre-processing for supervised learning, such as PCR. 
- *first PCA*: is the result choosing the loadings $\phi_{j1}$, for $1\le j \le p$ such that the new variable $Z_1$ has teh largest sample variance. 
$$
Z_1 =\phi_{11}X_1 +\phi_{21}X_2+\cdots +\phi_{p1}X_p
$$
has the largest variance, where the coefficients $\phi_{11}, \cdots, \phi_{p1}$ are the PCA *loadings* **normalized** such that $\sum_{j=1}^p \phi_{j1}^2=1$. The normalization avoid the variance of $Z_1$ becomes arbitrary large.  The vector $\phi_1=(\phi_{11}, \cdots, \phi_{p1})^T$ is called the *PCA loading vector* or (PCA direction) in the feature space along which data vary the most. If we project the $N$ data points $x_1, \cdots, x_N$ onto this direction, then the projected values are the principle component **scores** $z_{11} , z_{21}, \cdots, z_{n1}$ for each of the data points. 



- second PCA: similarly, $Z_2$ is a linear combination of the original variables that is uncorrelated from $Z_1$ and has the second largest sample variance. The uncorrelation of $Z_2$ and $Z_1$ is equivalent to $\phi_2$ is orthogonal to $\phi_1$. 

- Each loading vector and the corresponding PC are unique up to sign flips, this is because the after flipping the sign, the loading vector still defines the same direction. 

- PCA *biplot*: display both PCA scores and loadings of the first two PCA for each data points. The middle point $(0,0)$ indicate an observation with average levels of both principal components. 

- Scaling the variables matters. If the variables are in different units, it is recommended to scale them to have s.d equal to one. 

- PCA may be understood as   converting the original (**demeaned**) variables   $X=(X_1, X_2, \cdots, X_p)^T$  to new variables $Z=(Z_1, \cdots, Z_p)^T=V^TX$ via an orthogonal transformation matrix $V$, in order to  un-correlate the original variables, such that the new variables $Z$ are uncorrelated, that is ( Note that,  ${\bf Z}$ and ${\bf X}$ are both data matrix of size $N\times p$, and  ${\bf Z}={\bf X}V$ )
$$
{\bf Z}^T{\bf Z} = V^T({\bf X}^T{\bf X})V= D^2. 
$$
where $V$ and $D$ are defined by the eigen-decomposition of 
$$
{\bf X}^T{\bf X}=VD^2V^T
$${#eq-eigen-decom}
that is, $V=[v_1, \cdots, v_p]$ is an $p\times p$ orthogonal matrix  formed by the orthonormal eigenvectors of the matrix ${\bf X}^T{\bf X}$ which is the  sample covariance matrix (multiplied by $N-1$) , and $D$ is a $p\times p$ diagonal matrix of singular values of ${\bf X}$ (or equivalently, square roots of the eigenvalues of ${\bf X}^T{\bf X}$).  The $v_j$'s are called the *right singular vector* of ${\bf X}$ and are ordered in decreasing order of the singular values $d_j$. 



- SVD: The construction of $V$ and $D$ in @eq-eigen-decom allows the further construction of 
the SVD of ${\bf X}$  by 
$${\bf X}=UDV^T =\sum_{j=1}^p d_ju_jv_j^T = \sum_{j=1}^p {\bf X}v_jv_j^T. $${#eq-svd}
The $N\times p$ matrix $U$ consists of orthonormal vectors $u_j=\frac{{\bf X}v_j}{d_j}$, and spans the column space of ${\bf X}$. The $v_j$ spans the row space of ${\bf X}$. 

Note with the SVD of ${\bf X}=UDV^T$, ${\bf Z}= [z_1, \cdots, z_p]={\bf X}V=UD$.Hence, $z_i, z_j$ are orthogonal.  

- PCA can also be understood as the solution to the following optimization problem to find the best $M$-dimensional approximation $x_{ij}\approx \sum_{m=1}^M a_{im} b_{jm}$, assuming the data matrix is centered
$$
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{j=1}^p\sum_{i=1}^n \left(x_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
$$ {#eq-pca-approx}
It turns out that $\hat{\bf A}$ and $\hat{\bf B}$ solved the above problem are in fact the first $M$ PCA scores and loading vectors. 

Thus, the first PCA loading vector $\phi_1$ defines a direction (line) in the $p$-dimensional feature space that is *closest* to the $n$ observations in terms of the average squared Euclidean distance. Similarly the first two PCA loading vectors span the plane that is closest to the $n$ observations, in terms of the average squared Euclidean distance. 

- total variance: The total variance 
$$
\sum_{j=1}^p Var[X_j] = \sum_{j=1}^pVar[Z_j]=tr({\bf Z}^T{\bf Z})=tr({\bf X}^T{\bf X})
$$
The Proportion of Variance explained by the $m$-th PCA is given by (assuming the data are centered)
$$
PVE_m=\frac{\sum_{i=1}^n z_{im}^2}{\sum_{j=1}^p\sum_{i=1}^n x_{ij}^2}
$${#eq-PVE}
and $\sum_{m=1}^p PVE_m = 1$. 

The total variance of the data can be decomposed as 
$$
\text{var. of data }= \text{var. of first M PCs } + \text{MSE of M-dim approximation}
$$
i.e., 
$$
\sum_{j=1}^p\frac{1}{n} \sum_{i=1}^n x_{ij}^2 = \sum_{m=1}^M\frac{1}{n} \sum_{i=1}^n z_{im}^2 + \frac{1}{n} \sum_{j=1}^p \sum_{i=1}^n \left( x_{ij}- \sum_{m=1}^M z_{im}\phi_{jm}\right)^2
$$
So maximizing the variance of the first $M$-PCs, is equivalent to minimize the MSE of the $M$-dim approximation, and vice versa.  Using the PVE defined in @eq-PVE and diving the above equation by the total variance, we obtain
$$
1-\frac{\sum_{j=1}^p \sum_{i=1}^n \left( x_{ij}- \sum_{m=1}^M z_{im}\phi_{jm}\right)^2}{\sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}=1-\frac{RSS}{RSS}=R^2
$$
So the PVE explained by the first $M$ principle components can be interpreted as the $R^2$ of the approximation for ${\bf X} $ using the first $M$ PC's. 

- How many PCA's to choose? no simple answer, as the cross-validation can not be used because we have to use the entire data set. The *"scree plot"* which plots PVE of each PC's can be used to look for an "elbow". Note 
$X=VZ$. If you $M<p$ PCA is chosen, then 
$$x_i=(x_{i1}, \cdots, x_{ip})^T \approx \sum_{m=1}^M v_m z_{im},\qquad x_{ij}\approx \sum_{m=1}^M v_{jm} z_{im}
$$

## Matrix completion by using PCA to  impute missing values
Missing values may be simply dropped or replaced with mean or median. But we can do better by exploiting the correlation between the variables, if the missingness is random (nor informative, such as missing value was because it was too large). Consider a modified version of @eq-pca-approx, 
$$
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{(i,j)\in \mathcal{O}} \left(x_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
$$
where $\mathcal{O}$ is the set of all *observed* indices $(i,j)$, a subset of all possible $n\times p$ pairs. The problem can be solved approximately using the "Hard-Impute" algorithm. The algorithm not only estimates $\hat{a}_{im}$ and $\hat{b}_{jm}$, but can approximately recover the $M$ PC scores and loadings. 

**(Hard-Impute) Algorithm**:

1. Create a complete matrix $\tilde{\bf X}$ with missing elements $\tilde{x}_{ij}$ replaced by the mean $\bar{x}_j$ of the observed for the $j$-th variable. 
2. repeat *a)-(c) until the objective @eq-matrix-complete fails to decarese:
  (a) Solve 
  $$
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{j=1}^p\sum_{i=1}^n \left(\tilde{x}_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
$$ 
by computing the PC's of $\tilde{\bf X}$. 
  (b) imputing the missing values by set $\tilde{x}_{ij}\leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}$ for a fixed $M$. 
  (c) Computing the objective error using the observed data (a supervised way) 
  $$
  \sum_{(i,j)\in \mathcal{O}} \left(x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm}   \right)
  $${#eq-matrix-complete}
3. Return the estimated missing entries $\tilde{x}_{ij}, (i,j)\notin \mathcal{O}$. 


The algorithm may be used in powering a recommender systems. 


## Clustering: finding subgroups or clusters 
Seek a partition of the data into distinct subgroups so that the observations are quite similar to each other within each subgroup. Clustering looks for *homogeneous subgroups* among the observations. 
Example: market segmentation, gene expressions

similarity are  domain-specifically defined. 

There are two types of clustering tasks: 1) cluster observations on the basis of the features in order to identify subgroups among observations. 2) Cluster the features on the basis of the observations in order to discover subgroups among features. 

Clustering often are not robust. 

### K-means clustering
Partition data into a pre-specific number of  disjoint clusters. Let $C_k$, $1\le k \le K$ denotes the mutually *disjoint* sets of indices of the observations and their union is the entire data set. The goal is to find K-clusters such that the *within-cluster variation* ($WCV(C_k)$) is least, i.e., we want to solve minimize the total WCV of all $K$ clusters 
$$
\min_{C_1, \cdots, C_K} \left\{ \sum_{k=1}^K WCV(C_k) \right\}.
$${#eq-clustering-var}
where, 
$$
WCV(C_k)=\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j'})^2
$$
The problem is difficult to solve because there are $K^n$ (each data point can have $K$ choices ) ways to partition $n$ data points into $K$ clusters. 

**K-means Algorithm**:

1. initial clustering: randomly assign a cluster to an observation. 
2. Iterate until the cluster assignments stop changing:
  2.1 (centering) For each of the $K$ cluster, computer *centroid*, which is simply the average of all $p$-feature observations in the $k$-the cluster. 
  2.2 (clustering assignment) Assign each observation to the cluster whose centro id is the closest (in terms of Euclidean distance). 

- The algorithm is guaranteed to reduce the total variance in @eq-clustering-var at each step. This can be seen from 
$$
\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j'})^2=2\sum_{i\in C_k}\sum_{j=1}^p (x_{ij}-\bar{x}_{kj})^2
$$
where $\bar{x}_{kj}=\frac{1}{|C_k|}\sum_{i\in C_k}x_{ij}$ is the mean for feature $j$ in cluster $C_k$. 

- But it is not guaranteed to give the global minimum. It only finds a local minimum. 

- In practice, multiple times of the K-clustering should be performed and the best one can be chosen, this is because K-means only finds a local minimum that depends on the initial clustering. 

### Hierarchical clustering
Ends up with a tree-like *dendrogram* of the observations, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 (no cut line) to $n$ (cut at height 0). After a horizontal cut line is drawn, the distinct sets of observations beneath the cut can be interpreted as clusters. The cut height serves the same role as $K$ in K-means clustering. 

The most common type of hierarchical clustering uses a *bottom-up* or *agglomerative* clustering: a dendrogram is built starting from the leaves and combining clusters up to the trunk. 

The term *hierarchical* refers to the fact that clusters obtained by cutting the deprogram at a given height are necessarily nested within the clusters obtained by cutting at a greater height. However, this assumption may be unrealistic. for example, our observations correspond to a group of men and women, evenly split among Americans, Japanese, and French. We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality. In this case, the ture clusters are not nested, in the sense that the best divsion into three groups does not result from taking the best division into two groups and splitting up one of those groups. 

So hierarchical clustering can sometiems yield *worse* results than K-means clustering. 

**Algorithm**:
1. start with each point in its own cluster.
2. identify the *closest* two clusters and merge them. The height of a fusion point of these two clusters, indicate how different they are. Note one can not conclude the similarity of two observations based on their horizontal proximity. 
3. repeat
4. Ends when all points are in a single cluster. 

In general scaling is recommended. 

### Types of linkage
Linkage is used to calculate the dissimilarity between a pair of clusters with more than one observations. Average and complete linkages are generally preferred, as they tend to yield more balanced dendrograms.

- Complete: maximal inter-cluster dissimilarity: use the maximum pairwise dissimilarities between observations in two clusters. 
- single: Minimal inter-cluster dissimilarity. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time. 
- Average: mean inter-cluster dissimilarity. 
- Centroid: dissimilarity between the centroids of two classes. May result in undesirable *inversions*, where two clusters are fused at a height *below* either of the individual clusters. 

### Choice of dissimilarity measure and variable scaling
- Euclidean distance
- correlation-based distance: correlation between two observations (not the usual correlation  between variables). Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes. 
- one should carefully consider if the variables  need to be scaled. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance. 



## Homework:
- Conceptual: 1--
- Applied: At least one. 

## Code Snippet
### Python
```


```

### Numpy
```
pcaUS.explained_variance_ratio_.cumsum()

U, D, V = np.linalg.svd(X, full_matrices=False)#note V is the V^T in the math formula

np.linalg.norm(V, 2, axis=1)

np.allclose(U.dot(np.diag(D)).dot(V)-X, 0)

r_idx = np.random.choice(np.arange(X.shape[0]),
                         n_omit,
                         replace=False)

Xbar = np.nanmean(Xhat, axis=0) # ignoring nan

ismiss = np.isnan(Xna)

np.corrcoef(Xapp[ismiss], X[ismiss])

np.random.standard_normal((50,2));

np.multiply.outer(np.ones(X.shape[0]), X[0]) #make X.shape[0] copies of the first row X[0]

#computing pairwise distance between rows of X:
D = np.zeros((X.shape[0], X.shape[0]));
for i in range(X.shape[0]):
    x_ = np.multiply.outer(np.ones(X.shape[0]), X[i]) # each row of X is one point (obs)
    D[i] = np.sqrt(np.sum((X - x_)**2, 1)); # D[i]: ith row of D. distance from the i-th row of X.

```

### Pandas
```
USArrests.mean() # column means
USArrests.var() # colmn variance

pd.crosstab(nci_labs['label'],
            pd.Series(comp_cut.reshape(-1), name='Complete'))

```

### Graphics
```
ax.set_xticks(ticks)


ax.axhline(140, c='r', linewidth=4);

```

### ISLP and statsmodels
```

```


### sklearn
#### PCA
```
scaler = StandardScaler(with_std=True,
                        with_mean=True)
USArrests_scaled = scaler.fit_transform(USArrests)
pcaUS = PCA()
pcaUS.fit(USArrests_scaled)
pcaUS.mean_
scores = pcaUS.transform(USArrests_scaled)
pcaUS.components_  # loadings
scores.std(0, ddof=1) #axis=0, divided by N-ddof=N-1
pcaUS.explained_variance_
pcaUS.explained_variance_ratio_
pcaUS.n_components_


```
#### PCA plot
```
fig, axes = plt.subplots(1, 2, figsize=(15,6))
ax = axes[0]
ax.scatter(nci_scores[:,0],
           nci_scores[:,1],
           c=nci_groups,
           marker='o',
           s=50)
ax.set_xlabel('PC1'); ax.set_ylabel('PC2')
```

#### PCA biplot
```
scale_arrow = s_ = 2
scores[:,1] *= -1      #flip the scores and loadings at the same time. 
pcaUS.components_[1] *= -1 # flip the y-axis
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
ax.scatter(scores[:,0], scores[:,1])
ax.set_xlabel('PC%d' % (i+1))
ax.set_ylabel('PC%d' % (j+1))
for k in range(pcaUS.components_.shape[1]):
    ax.arrow(0, 0, s_*pcaUS.components_[i,k], s_*pcaUS.components_[j,k])
    ax.text(s_*pcaUS.components_[i,k],
            s_*pcaUS.components_[j,k],
            USArrests.columns[k])
```
#### PCA plot for variance explained
```
fig, axes = plt.subplots(1, 2, figsize=(15, 6))
ticks = np.arange(pcaUS.n_components_)+1
ax = axes[0]
ax.plot(ticks,
        pcaUS.explained_variance_ratio_,
        marker='o')
ax.set_xlabel('Principal Component');
ax.set_ylabel('Proportion of Variance Explained')
ax.set_ylim([0,1])
ax.set_xticks(ticks)

ax = axes[1]
ax.plot(ticks,
        pcaUS.explained_variance_ratio_.cumsum(),
        marker='o')
ax.set_xlabel('Principal Component')
ax.set_ylabel('Cumulative Proportion of Variance Explained')
ax.set_ylim([0, 1])
ax.set_xticks(ticks)
fig
```

#### Matrix completion
```
def low_rank(X, M=1): #low rank approx to X
    U, D, V = np.linalg.svd(X)
    L = U[:,:M] * D[None,:M]
    return L.dot(V[:M])
    
Xhat = Xna.copy()
Xbar = np.nanmean(Xhat, axis=0) 
Xhat[r_idx, c_idx] = Xbar[c_idx]

thresh = 1e-7
rel_err = 1
count = 0
ismiss = np.isnan(Xna)#Xna: original matrix with nan; Xhat: matrix completed with imputed values
mssold = np.mean(Xhat[~ismiss]**2)
mss0 = np.mean(Xna[~ismiss]**2)

while rel_err > thresh:
    count += 1
    # Step 2(a)
    Xapp = low_rank(Xhat, M=1)
    # Step 2(b)
    Xhat[ismiss] = Xapp[ismiss]
    # Step 2(c)
    mss = np.mean(((Xna - Xapp)[~ismiss])**2)
    rel_err = (mssold - mss) / mss0
    mssold = mss
    print("Iteration: {0}, MSS:{1:.3f}, Rel.Err {2:.2e}"
          .format(count, mss, rel_err))
          
np.corrcoef(Xapp[ismiss], X[ismiss])
```

### K-Clustering
```
from sklearn.cluster import \
     (KMeans,
      AgglomerativeClustering)
      
kmeans = KMeans(n_clusters=2,
                random_state=2,
                n_init=20).fit(X)
                
kmeans.labels_  #cluster assignments                
                
```     
### Hierarchical Clustering
```
from sklearn.cluster import \
     (KMeans,
      AgglomerativeClustering)
from scipy.cluster.hierarchy import \
     (dendrogram,
      cut_tree)
from ISLP.cluster import compute_linkage

HClust = AgglomerativeClustering
hc_comp = HClust(distance_threshold=0, # same as None. min. distance to not merge
                 n_clusters=None,
                 linkage='complete')
hc_comp.fit(X)

hc_avg = HClust(distance_threshold=0,
                n_clusters=None,
                linkage='average');
hc_avg.fit(X)
hc_sing = HClust(distance_threshold=0,
                 n_clusters=None,
                 linkage='single');
hc_sing.fit(X);

cut_tree(linkage_comp, n_clusters=4).T #linkage_comp is the linkage-matrix represenation

cut_tree(linkage_comp, height=5).T

```

#### Use precomputed matrix for hierarchical clustering
```
D = np.zeros((X.shape[0], X.shape[0]));
for i in range(X.shape[0]):
    x_ = np.multiply.outer(np.ones(X.shape[0]), X[i]) # each row of X is one point (obs)
    D[i] = np.sqrt(np.sum((X - x_)**2, 1)); # D[i]: ith row of D. distance from the i-th row of X.
    
hc_sing_pre = HClust(distance_threshold=0,
                     n_clusters=None,
                     metric='precomputed',
                     linkage='single')
hc_sing_pre.fit(D) # use precomputed distance matrix
    
``` 
#### Plotting dendrogram
```
cargs = {'color_threshold':-np.inf, # not using the default coloring method
         'above_threshold_color':'black'}
linkage_comp = compute_linkage(hc_comp) # compute teh linkage-matrix representation
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp,
           ax=ax,
           **cargs);
# coloring           
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp,
           ax=ax,
           color_threshold=4,
           above_threshold_color='black');


dendrogram(linkage_pca,
           labels=np.asarray(nci_labs),
           leaf_font_size=10,
           ax=ax,
           **cargs)
```
#### a function to plot dendrogram
```
def plot_nci(linkage, ax, cut=-np.inf):
    cargs = {'above_threshold_color':'black',
             'color_threshold':cut}
    hc = HClust(n_clusters=None,
                distance_threshold=0,
                linkage=linkage.lower()).fit(nci_scaled)
    linkage_ = compute_linkage(hc)
    dendrogram(linkage_,
               ax=ax,
               labels=np.asarray(nci_labs),
               leaf_font_size=10,
               **cargs)
    ax.set_title('%s Linkage' % linkage)
    return hc
    
plot_nci('Complete', ax, cut=140)
ax.axhline(140, c='r', linewidth=4);
    
```    

#### Use scaled features for hierarchical clustering
```
scaler = StandardScaler()
X_scale = scaler.fit_transform(X)
hc_comp_scale = HClust(distance_threshold=0,
                       n_clusters=None,
                       linkage='complete').fit(X_scale)
linkage_comp_scale = compute_linkage(hc_comp_scale)
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp_scale, ax=ax, **cargs)
ax.set_title("Hierarchical Clustering with Scaled Features");

```


### Useful code snippets

