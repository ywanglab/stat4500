# Chapter 8: Tree-Based Methods
(Decision) tree-based methods stratify or segment the predictor space into a number of simple regions using tree-based rules. The predictor space is subdivided into distinct and non-overlapping high-dimensional boxes along each axis. Such methods are simple and easy for interpretation. Their predicting accuracy is not as good as  the best supervised learning approaches. However, through **ensemble method** such as *bagging, random forests, boosting, Bayesian additive regression trees*, by growing and combining large number of trees (weak learners) to yield a single consensus prediction may result in dramatic improvement in prediction accuracy, at the expense of some loss in interpretation. Decision trees often overfit the training data: a small change in the date might cause a large change in the tree. A small tree may offer small variance and better interpretation. Tree can easily handle a categorical variable  without creating dummy variables. 



A decision tree is typically drawn *upside down*. An *internal node* is a point along the tree where the predictor space is split, and a *terminal node* (leaf node) is a point along the tree that do not split. These terminal nodes are the final split regions of the predictor space. A *branch* connects nodes. 

Decision trees can be applied to both regression and classification problems. 

## Regression tree
For a  Regression tree, 
The value at a leaf node equals to the average of the $Y$ values of all examples in the leaf node. The objective is to minimize the RSS 
$$
RSS =\sum_{j=1}^J \sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2
$${#eq-tree-objective}
where, $\hat{y}_{R_j}$ is the mean response for the training observations in the $j$-th box $R_j$ that corresponds to the $j$-th leaf node. 

The first node (root) is the most important predictor, and so on.

It is computationally intractable to consider every possible partition  of the feature space into $J$ boxes in objective @eq-tree-objective. The solution is to use a *top-down* and *greedy* **recursive binary splitting**. It is greedy (myopic) because at each step of the tree-building process, the best split is decided at that particular step by choosing a predictor $X_j$ (consider all predictors) and a cut-point $s$ (consider all values of that predictor) that leads to the greatest reduction in RSS, rather than looking ahead and picking a split that will lead to a better tree in some future step.  The greedy splitting amounts to at each step, only the current region is split by a feature. This process is repeated within each of the resulting regions, until a stopping criterion. 

**Stopping criterion**: 
- max number of observations in each leaf
- max number of depth
- RSS decrease smaller than a threshold. 



## Classificaiton tree
For a  Classification tree: 
An example is classified as the class which is the mode of the examples in the leaf node. The training objective is similar to @eq-tree-objective, but with RSS replaced with 

- classification error rate $E=1- \max_k(\hat{p}_{mk})$, where $\hat{p}_{mk}$ is the fraction of training examples in the $m$th region that are in the $k$-th class. But this measure is not sufficiently sensitive for tree-growing (node-splitting). 
- Gini index that measures node purity (total variance): $G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})$. The Gini index takes on small value if all of the $\hat{p}_{mk}$'s are close to zero or one, indicating that a node contains predominantly observations from a single class. 
- Cross-entropy: very similar to Gini index numerically,  $D=-\sum_{k=1}^K\hat{p}_{mk}\log \hat{p}_{mk}$. Cross-entropy is always non-negative.  Cross entropy is the expectation of the information contained in a probability information. 

Gini index and Cross-entropy are preferred when splitting a node, while Classification error rate is preferred when pruning a tree if the prediction accuracy is the final goal. 

Two leaf nodes might have the same predicted value resulting from a split, this is because the two leaf nodes have different node purity, which amounts to the certainty of a predicted value. In this case, an observation falls into the leaf node with higher purity renders higher certainty. 

## Prunning a tree
Using stopping criteria directly to obtain a small tree may be near-sighted: A seemingly worthless spit early on might be followed by a very good split with large RSS reduction. A better approach is to grow a very large tree $T_0$ such that each leaf only has some minimum number of observations, and then **prune** it back in order to obtain a *subtree* with the least test error via cross-validation or validation approach.  *Cost complexity pruning* (weakest link pruning) is used to do this by minimizing the following with a tuning $\alpha$:
$$
\sum_{m=1}^{|T_\alpha|}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2+\alpha |T_\alpha|
$$
where, $|T_alpha|$ is the number of leaf nodes in $T_\alpha$, which is the best subtree that minimize the above objective. As we increase $\alpha$ from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the sequence of $T_\alpha$ is easy.  The formulation is similar to lasso.  And then An optimal $\hat{\alpha}$ is chosen by cross-validation, and the corresponding $T_{\hat{\alpha}}$. 

## Bagging
*Bootstrapping aggregation*, or *bagging* is a general  purpose procedure for reducing variance of a statistical learning method because of the Law of Large Numbers: given a set of $n$ independent observations $Z_1, \cdots, Z_n$ with variance $\sigma^2$, the variance of the mean $\bar{Z}$ is $\sigma^2/n$. In other words, averaging a set of observations reduces variance.

But in practice, we typically do not have access to multiple training sets. Instead, we **bootstrap** the training set to obtain $B$ (usually hundreds or even thousands) bootstrapped training sets, and then fit a separate tree (usually deep and not pruned, hence with low bias) independently for the   $b$-th bootstrapped training set to get the prediction $\hat{f}^{*b}(x)$ at $x$ for each $b$, and then finally combine all the trees by averaging all the predictions to obtain
$$
\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x).
$$
The above formula works for regression. For classification, the average is replaced by *majority vote*. Using large $B$ in bagging (including RF) typically does not lead to overfitting. But small $B$ may underfit. Bagging often leads to correlated (similar) trees, and can get caught in local optima and thus fail to  explore the model space, and thus averaging may not lead to large reduction in variance. One way to remedy this is by RF. 

### Out-of-Bag Error Estimate
On average, each bagged tree makes use of 2/3 of the total observations. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as the *out-of-bag* (OOB) observations. For each observation, it is an OOB observation  in around $B/3$ trees, and hence the average of the predictions of those $B/3$ trees for the $i$-the observation can be used as a cross-validation error for observation $i$. The overall OOB error can be calculated this way for all $n$ observations. 

When $B$ is large, such as $B=3n$, then this is essentially the LOO cross-validation error for bagging. This is cheap way to evaluate test error without the need of cross-validation (which may be onerous) or validation approach. 

### Random Forests

Bagging results in correlated trees and thus the variance may not be reduced by the average. Random forests still grows independent trees using   bootstrapped data sets of the original data set, but RF  *decorrelates* the trees by *randomly selecting* $m$ predictors ($m<p$) each time a split in a tree is considered. Typically $m\approx \sqrt{p}$. Thereby leading to a more thorough exploration of model space. Bagging is the case when $m=p$. On average, $(p-m)/p$ of the splits will not consider a specific predictor. Using a small value of $m$ in building RF will typically helpful when there are a large number of correlated predictors. 

Large $B$ will not lead to RF to overfit, but small $B$ may underfit. 

### Variable Importance Measure (VI)

For bagged/RF regression trees, VI is the total amount of RSS decreased due to splits over a given predictor, averaged over all $B$ trees. A large VI value indicates an important predictor. For bagged/RF classification trees, replacing RSS with Gini index or cross-entorpy. 

## Boosting
Like Bagging, boosting is a general approach that can be applied to many statistical learning methods for regression and classification. Boosting does not involve bootstrap sampling;  Boosting grows trees *sequentially* by **slow** learning: each new tree is grown  by fitting a new tree to the residuals (modified version of the original data set) left over from the previous trees, and then a shrunken version of the new tree is added to the model. Each new tree tries to capture signal that is not yet accounted for by the current set of trees. 

### Boosting Algorithm for regression trees
1. Set $\hat{f}(x)=0$, and $r_i=y_i$ for all $i$ in the training set. 
2. For $b=1, \cdots, B$, repeat
  2.1. Fit a tree $\hat{F}^b$ with $d$ splits ($d+1$ terminal nodes, can involve at most $d$ variables) to the training data $(X,r)$. 
  2.2. Update $\hat(f)$ by adding a shrunken version of the new tree:
   $$ 
   \hat{f}(x) \leftarrow \hat{f}(x) +\lambda \hat{f}^b(x).
   $$
   2.3. Update the residuals
   $$
   r_i \leftarrow r_i -\lambda \hat{f}^b(x_i).
   $$
3. Output the boosted model
 $$ 
  \hat{f}(x)=\sum_{b=1}^B \lambda \hat{f}^b (x). 
 $$
Each new tree can be rather small ($d=1$ or 2, hence with low variance) controlled by the parameter $d$. By fitting a small tree to the residual, we slowly improve $\hat{f}$ in areas where it does not perform well. The shrinkage $\lambda$ slows the learning, allowing more and different shaped trees to attack the residuals. 

### Tuning parameters for Boosting
- The number of trees $B$: unlike bagging and random forests, boosting can overfit if $B$ is too large, although overfitting tends to occur slowly. $B$ is selected with cross-validation. 
- The shrinkage parameter $\lambda$: Typical values are 0.01 or 0.001. Very small $\lambda$ may require very large $B$. 
- The number of splits $d$: $d$ controls the complexity of the boosted ensemble. Often $d=1$ works well, in which case each tree is a *stump*, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only one variable, hence easy to interpret. 
$d$ is the *interaction depth*, controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables. 
  
## Bayesian Additive Regression Trees (BART)
BART is related to the approaches used by both bagging and boosting. We only make use of the original data (not using bootstrap) and their modified version (residuals from other trees) , and grow trees sequentially. 

- each tree tries to capture the signal not yet accounted by for by the current model, as in boosting. 
- each tree is constructed in a random manner as in bagging and RF

The main novelty of BART is the way in which new trees are generated. Assume there are $K$ trees and $B$ iterations. Let $\hat{f}^b_k(x)$ be the prediction at $x$ for the $k$th tree used in the $b$th iteration. 

Initially, BART initializes all trees to be a single root node, with $\hat{f}^1_k(x)=\frac{1}{nK}\sum_{i=1}^n y_i$. Thus $\hat{f}^1(x)= \sum_{k=1}^K\hat{f}^1_k(x)=\frac{1}{n}\sum_{i=1}^n y_i$. 

In subsequent iteration, BART updates each of the $K$ trees, one at a time. In the $b$-th iteration, to update the $k$th tree, obtain a *partial residual* by subtracting from each response $y_i$ the predictions from all but the $k$-th tree, 
$$
r_i = y_i -\sum_{k'<k}\hat{f}^b_{k'}(x_i) - \sum_{k'>k}\hat{f}^{b-1}_{k'}(x_i)
$$
for each observation $i=1, \cdots, n$. Note when $k'<k$, the trees are updated already in the $b$-th iteration, and for $k'>k$, the trees are from the previous iteration $b-1$. Rather than fitting a fresh tree to this partial residual $r_i$, BART obtain a new tree $\hat{f}^b_k$ by *randomly perturb* the tree $\hat{f}^{b-1}_k$ from the $(b-1)$-th iteration via the following operations: 
  
  1. change the structure of $\hat{f}^{b-1}_k$ by adding or pruning branches
  2. keep the same structure of $\hat{f}^{b-1}_k$ but perturb the prediction values. 

Perturbations that improve the fit are favored. The perturbation only modifies the previous tree slightly hence guard against overfitting. In addition, the size of each tree is limited to avoid overfitting. The perturbation can be interpreted as drawing a new tree from a *posterior* distribution via *Markov chain Monte Carlo* sampling. The perturbation avoids local minima and achieve a more thorough exploration of the model space. 


At the end of each iteration, the $K$ trees from that iteration will be summed, i.e. $\hat{f}^b(x)=\sum_{k=1}^K \hat{f}^b_k(x)$ for $b=1, \cdots, B$. 

Finally, computer the mean (or other quantities such as percentile) after $L$ burn-in samples:
$$
\hat{f}(x) = \frac{1}{B-L} \sum^B_{b=L+1} \hat{f}^b(x). 
$$
During the *burn-in* period- the first $L$ iterations, $\hat{f}^{\ell}$, $\ell <=L$ tends not to provide good results, hence are discarded. 


BART has very impressive out-of-box performance: perform well (not overfitting) with minimal tuning. 

Parameters:

- number of trees, $K$: e.g., $K=200$
- number of iterations: $B$: e.g., $B=1000$
- burn-in iterations $L$: e.g., $L=100$. 


## Homework:
- Conceptual: 
- Applied: At least one. 

## Code Snippet
### Python
```


```

### Numpy
```
np.asarray(D)


```

### Pandas
```
feature_imp.sort_values(by='importance', ascending=False)
```


### Graphics
```


```

### ISLP and statsmodels
```


```

### sklearn

### Useful code snippets
#### Classification Decision Tree
```
from sklearn.metrics import (accuracy_score,
                             log_loss)
                             
clf = DTC(criterion='entropy',
          max_depth=3,
          random_state=0)        
clf.fit(X, High)
accuracy_score(High, clf.predict(X))
resid_dev = log_loss(High, clf.predict_proba(X))
ax = subplots(figsize=(12,12))[1]
plot_tree(clf,
          feature_names=feature_names,
          ax=ax);
print(export_text(clf,
                  feature_names=feature_names,
                  show_weights=True))
                  
# Using validaiton approach to train and test the model
validation = skm.ShuffleSplit(n_splits=1,
                              test_size=200,
                              random_state=0)
results = skm.cross_validate(clf,
                             D,
                             High,
                             cv=validation)
results['test_score']

```
#### Pruning a classifcaiton Decision tree
```
(X_train,
 X_test,
 High_train,
 High_test) = skm.train_test_split(X,
                                   High,
                                   test_size=0.5,
                                   random_state=0)
clf = DTC(criterion='entropy', random_state=0)
clf.fit(X_train, High_train)
accuracy_score(High_test, clf.predict(X_test))
ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)
kfold = skm.KFold(10,
                  random_state=1,
                  shuffle=True)
grid = skm.GridSearchCV(clf,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True, # Refit the best estimator with the entire dataset
                        cv=kfold,
                        scoring='accuracy')
grid.fit(X_train, High_train)
grid.best_score_
best_ = grid.best_estimator_
best_.tree_.n_leaves
print(accuracy_score(High_test,
                     best_.predict(X_test)))
confusion = confusion_table(best_.predict(X_test),
                            High_test)

```
#### Fitting a regression tree
```
reg = DTR(max_depth=3)
reg.fit(X_train, y_train)
ax = subplots(figsize=(12,12))[1]
plot_tree(reg,
          feature_names=feature_names,
          ax=ax);

```

#### Pruning a regression tree
```
ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)
kfold = skm.KFold(5,
                  shuffle=True,
                  random_state=10)
grid = skm.GridSearchCV(reg,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
G = grid.fit(X_train, y_train)
best_ = grid.best_estimator_
np.mean((y_test - best_.predict(X_test))**2)

```

#### Bagging  and RG
```
bag_boston = RF(max_features=X_train.shape[1], random_state=0, n_estimators=500)
bag_boston.fit(X_train, y_train)
y_hat_bag = bag_boston.predict(X_test)


#RF
RF_boston = RF(max_features=6,
               random_state=0).fit(X_train, y_train)
y_hat_RF = RF_boston.predict(X_test)
np.mean((y_test - y_hat_RF)**2)

#VI
feature_imp = pd.DataFrame(
    {'importance':RF_boston.feature_importances_},
    index=feature_names)
feature_imp.sort_values(by='importance', ascending=False)

```

#### Gradient Boosting
```
boost_boston = GBR(n_estimators=5000,
                   learning_rate=0.001,
                   max_depth=3,
                   random_state=0)
boost_boston.fit(X_train, y_train)
test_error = np.zeros_like(boost_boston.train_score_)
for idx, y_ in enumerate(boost_boston.staged_predict(X_test)):
   test_error[idx] = np.mean((y_test - y_)**2)

plot_idx = np.arange(boost_boston.train_score_.shape[0])
ax = subplots(figsize=(8,8))[1]
ax.plot(plot_idx,
        boost_boston.train_score_,
        'b',
        label='Training')
ax.plot(plot_idx,
        test_error,
        'r',
        label='Test')
ax.legend();

```

#### BART 
```
bart_boston = BART(random_state=0, burnin=5, ndraw=15) #num_trees=200, max_states=100
# ndraw: number of iterations or samples to draw from the posterior distribution after the burn-in 
bart_boston.fit(X_train, y_train)
yhat_test = bart_boston.predict(X_test.astype(np.float32))
np.mean((y_test - yhat_test)**2)

# Variable Inclusion
var_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),
                               index=D.columns)

```