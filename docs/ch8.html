<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.398">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stat4500notes - 8&nbsp; Chapter 8: Tree-Based Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch9.html" rel="next">
<link href="./ch7.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch8.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Tree-Based Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">stat4500notes</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting up Python Computing Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Moving Beyond Linearity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Tree-Based Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Chapter 9: Support Vector Machine</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Chapter 10: Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chapter 11: Survival Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Class Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#regression-tree" id="toc-regression-tree" class="nav-link active" data-scroll-target="#regression-tree"><span class="header-section-number">8.1</span> Regression tree</a></li>
  <li><a href="#classificaiton-tree" id="toc-classificaiton-tree" class="nav-link" data-scroll-target="#classificaiton-tree"><span class="header-section-number">8.2</span> Classificaiton tree</a></li>
  <li><a href="#prunning-a-tree" id="toc-prunning-a-tree" class="nav-link" data-scroll-target="#prunning-a-tree"><span class="header-section-number">8.3</span> Prunning a tree</a></li>
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging"><span class="header-section-number">8.4</span> Bagging</a>
  <ul class="collapse">
  <li><a href="#out-of-bag-error-estimate" id="toc-out-of-bag-error-estimate" class="nav-link" data-scroll-target="#out-of-bag-error-estimate"><span class="header-section-number">8.4.1</span> Out-of-Bag Error Estimate</a></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">8.4.2</span> Random Forests</a></li>
  <li><a href="#variable-importance-measure-vi" id="toc-variable-importance-measure-vi" class="nav-link" data-scroll-target="#variable-importance-measure-vi"><span class="header-section-number">8.4.3</span> Variable Importance Measure (VI)</a></li>
  </ul></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="header-section-number">8.5</span> Boosting</a>
  <ul class="collapse">
  <li><a href="#boosting-algorithm-for-regression-trees" id="toc-boosting-algorithm-for-regression-trees" class="nav-link" data-scroll-target="#boosting-algorithm-for-regression-trees"><span class="header-section-number">8.5.1</span> Boosting Algorithm for regression trees</a></li>
  <li><a href="#tuning-parameters-for-boosting" id="toc-tuning-parameters-for-boosting" class="nav-link" data-scroll-target="#tuning-parameters-for-boosting"><span class="header-section-number">8.5.2</span> Tuning parameters for Boosting</a></li>
  </ul></li>
  <li><a href="#bayesian-additive-regression-trees-bart" id="toc-bayesian-additive-regression-trees-bart" class="nav-link" data-scroll-target="#bayesian-additive-regression-trees-bart"><span class="header-section-number">8.6</span> Bayesian Additive Regression Trees (BART)</a></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework"><span class="header-section-number">8.7</span> Homework:</a></li>
  <li><a href="#code-snippet" id="toc-code-snippet" class="nav-link" data-scroll-target="#code-snippet"><span class="header-section-number">8.8</span> Code Snippet</a>
  <ul class="collapse">
  <li><a href="#python" id="toc-python" class="nav-link" data-scroll-target="#python"><span class="header-section-number">8.8.1</span> Python</a></li>
  <li><a href="#numpy" id="toc-numpy" class="nav-link" data-scroll-target="#numpy"><span class="header-section-number">8.8.2</span> Numpy</a></li>
  <li><a href="#pandas" id="toc-pandas" class="nav-link" data-scroll-target="#pandas"><span class="header-section-number">8.8.3</span> Pandas</a></li>
  <li><a href="#graphics" id="toc-graphics" class="nav-link" data-scroll-target="#graphics"><span class="header-section-number">8.8.4</span> Graphics</a></li>
  <li><a href="#islp-and-statsmodels" id="toc-islp-and-statsmodels" class="nav-link" data-scroll-target="#islp-and-statsmodels"><span class="header-section-number">8.8.5</span> ISLP and statsmodels</a></li>
  <li><a href="#sklearn" id="toc-sklearn" class="nav-link" data-scroll-target="#sklearn"><span class="header-section-number">8.8.6</span> sklearn</a></li>
  <li><a href="#useful-code-snippets" id="toc-useful-code-snippets" class="nav-link" data-scroll-target="#useful-code-snippets"><span class="header-section-number">8.8.7</span> Useful code snippets</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Tree-Based Methods</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>(Decision) tree-based methods stratify or segment the predictor space into a number of simple regions using tree-based rules. The predictor space is subdivided into distinct and non-overlapping high-dimensional boxes along each axis. Such methods are simple and easy for interpretation. Their predicting accuracy is not as good as the best supervised learning approaches. However, through <strong>ensemble method</strong> such as <em>bagging, random forests, boosting, Bayesian additive regression trees</em>, by growing and combining large number of trees (weak learners) to yield a single consensus prediction may result in dramatic improvement in prediction accuracy, at the expense of some loss in interpretation. Decision trees often overfit the training data: a small change in the date might cause a large change in the tree. A small tree may offer small variance and better interpretation. Tree can easily handle a categorical variable without creating dummy variables.</p>
<p>A decision tree is typically drawn <em>upside down</em>. An <em>internal node</em> is a point along the tree where the predictor space is split, and a <em>terminal node</em> (leaf node) is a point along the tree that do not split. These terminal nodes are the final split regions of the predictor space. A <em>branch</em> connects nodes.</p>
<p>Decision trees can be applied to both regression and classification problems.</p>
<section id="regression-tree" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="regression-tree"><span class="header-section-number">8.1</span> Regression tree</h2>
<p>For a Regression tree, The value at a leaf node equals to the average of the <span class="math inline">\(Y\)</span> values of all examples in the leaf node. The objective is to minimize the RSS <span id="eq-tree-objective"><span class="math display">\[
RSS =\sum_{j=1}^J \sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2
\tag{8.1}\]</span></span> where, <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations in the <span class="math inline">\(j\)</span>-th box <span class="math inline">\(R_j\)</span> that corresponds to the <span class="math inline">\(j\)</span>-th leaf node.</p>
<p>The first node (root) is the most important predictor, and so on.</p>
<p>It is computationally intractable to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes in objective <a href="#eq-tree-objective" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>. The solution is to use a <em>top-down</em> and <em>greedy</em> <strong>recursive binary splitting</strong>. It is greedy (myopic) because at each step of the tree-building process, the best split is decided at that particular step by choosing a predictor <span class="math inline">\(X_j\)</span> (consider all predictors) and a cut-point <span class="math inline">\(s\)</span> (consider all values of that predictor) that leads to the greatest reduction in RSS, rather than looking ahead and picking a split that will lead to a better tree in some future step. The greedy splitting amounts to at each step, only the current region is split by a feature. This process is repeated within each of the resulting regions, until a stopping criterion.</p>
<p><strong>Stopping criterion</strong>: - max number of observations in each leaf - max number of depth - RSS decrease smaller than a threshold.</p>
</section>
<section id="classificaiton-tree" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="classificaiton-tree"><span class="header-section-number">8.2</span> Classificaiton tree</h2>
<p>For a Classification tree: An example is classified as the class which is the mode of the examples in the leaf node. The training objective is similar to <a href="#eq-tree-objective" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>, but with RSS replaced with</p>
<ul>
<li>classification error rate <span class="math inline">\(E=1- \max_k(\hat{p}_{mk})\)</span>, where <span class="math inline">\(\hat{p}_{mk}\)</span> is the fraction of training examples in the <span class="math inline">\(m\)</span>th region that are in the <span class="math inline">\(k\)</span>-th class. But this measure is not sufficiently sensitive for tree-growing (node-splitting).</li>
<li>Gini index that measures node purity (total variance): <span class="math inline">\(G=\sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\)</span>. The Gini index takes on small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>’s are close to zero or one, indicating that a node contains predominantly observations from a single class.</li>
<li>Cross-entropy: very similar to Gini index numerically, <span class="math inline">\(D=-\sum_{k=1}^K\hat{p}_{mk}\log \hat{p}_{mk}\)</span>. Cross-entropy is always non-negative. Cross entropy is the expectation of the information contained in a probability information.</li>
</ul>
<p>Gini index and Cross-entropy are preferred when splitting a node, while Classification error rate is preferred when pruning a tree if the prediction accuracy is the final goal.</p>
<p>Two leaf nodes might have the same predicted value resulting from a split, this is because the two leaf nodes have different node purity, which amounts to the certainty of a predicted value. In this case, an observation falls into the leaf node with higher purity renders higher certainty.</p>
</section>
<section id="prunning-a-tree" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="prunning-a-tree"><span class="header-section-number">8.3</span> Prunning a tree</h2>
<p>Using stopping criteria directly to obtain a small tree may be near-sighted: A seemingly worthless spit early on might be followed by a very good split with large RSS reduction. A better approach is to grow a very large tree <span class="math inline">\(T_0\)</span> such that each leaf only has some minimum number of observations, and then <strong>prune</strong> it back in order to obtain a <em>subtree</em> with the least test error via cross-validation or validation approach. <em>Cost complexity pruning</em> (weakest link pruning) is used to do this by minimizing the following with a tuning <span class="math inline">\(\alpha\)</span>: <span class="math display">\[
\sum_{m=1}^{|T_\alpha|}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2+\alpha |T_\alpha|
\]</span> where, <span class="math inline">\(|T_alpha|\)</span> is the number of leaf nodes in <span class="math inline">\(T_\alpha\)</span>, which is the best subtree that minimize the above objective. As we increase <span class="math inline">\(\alpha\)</span> from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the sequence of <span class="math inline">\(T_\alpha\)</span> is easy. The formulation is similar to lasso. And then An optimal <span class="math inline">\(\hat{\alpha}\)</span> is chosen by cross-validation, and the corresponding <span class="math inline">\(T_{\hat{\alpha}}\)</span>.</p>
</section>
<section id="bagging" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="bagging"><span class="header-section-number">8.4</span> Bagging</h2>
<p><em>Bootstrapping aggregation</em>, or <em>bagging</em> is a general purpose procedure for reducing variance of a statistical learning method because of the Law of Large Numbers: given a set of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(Z_1, \cdots, Z_n\)</span> with variance <span class="math inline">\(\sigma^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> is <span class="math inline">\(\sigma^2/n\)</span>. In other words, averaging a set of observations reduces variance.</p>
<p>But in practice, we typically do not have access to multiple training sets. Instead, we <strong>bootstrap</strong> the training set to obtain <span class="math inline">\(B\)</span> (usually hundreds or even thousands) bootstrapped training sets, and then fit a separate tree (usually deep and not pruned, hence with low bias) independently for the <span class="math inline">\(b\)</span>-th bootstrapped training set to get the prediction <span class="math inline">\(\hat{f}^{*b}(x)\)</span> at <span class="math inline">\(x\)</span> for each <span class="math inline">\(b\)</span>, and then finally combine all the trees by averaging all the predictions to obtain <span class="math display">\[
\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x).
\]</span> The above formula works for regression. For classification, the average is replaced by <em>majority vote</em>. Using large <span class="math inline">\(B\)</span> in bagging (including RF) typically does not lead to overfitting. But small <span class="math inline">\(B\)</span> may underfit. Bagging often leads to correlated (similar) trees, and can get caught in local optima and thus fail to explore the model space, and thus averaging may not lead to large reduction in variance. One way to remedy this is by RF.</p>
<section id="out-of-bag-error-estimate" class="level3" data-number="8.4.1">
<h3 data-number="8.4.1" class="anchored" data-anchor-id="out-of-bag-error-estimate"><span class="header-section-number">8.4.1</span> Out-of-Bag Error Estimate</h3>
<p>On average, each bagged tree makes use of 2/3 of the total observations. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as the <em>out-of-bag</em> (OOB) observations. For each observation, it is an OOB observation in around <span class="math inline">\(B/3\)</span> trees, and hence the average of the predictions of those <span class="math inline">\(B/3\)</span> trees for the <span class="math inline">\(i\)</span>-the observation can be used as a cross-validation error for observation <span class="math inline">\(i\)</span>. The overall OOB error can be calculated this way for all <span class="math inline">\(n\)</span> observations.</p>
<p>When <span class="math inline">\(B\)</span> is large, such as <span class="math inline">\(B=3n\)</span>, then this is essentially the LOO cross-validation error for bagging. This is cheap way to evaluate test error without the need of cross-validation (which may be onerous) or validation approach.</p>
</section>
<section id="random-forests" class="level3" data-number="8.4.2">
<h3 data-number="8.4.2" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">8.4.2</span> Random Forests</h3>
<p>Bagging results in correlated trees and thus the variance may not be reduced by the average. Random forests still grows independent trees using bootstrapped data sets of the original data set, but RF <em>decorrelates</em> the trees by <em>randomly selecting</em> <span class="math inline">\(m\)</span> predictors (<span class="math inline">\(m&lt;p\)</span>) each time a split in a tree is considered. Typically <span class="math inline">\(m\approx \sqrt{p}\)</span>. Thereby leading to a more thorough exploration of model space. Bagging is the case when <span class="math inline">\(m=p\)</span>. On average, <span class="math inline">\((p-m)/p\)</span> of the splits will not consider a specific predictor. Using a small value of <span class="math inline">\(m\)</span> in building RF will typically helpful when there are a large number of correlated predictors.</p>
<p>Large <span class="math inline">\(B\)</span> will not lead to RF to overfit, but small <span class="math inline">\(B\)</span> may underfit.</p>
</section>
<section id="variable-importance-measure-vi" class="level3" data-number="8.4.3">
<h3 data-number="8.4.3" class="anchored" data-anchor-id="variable-importance-measure-vi"><span class="header-section-number">8.4.3</span> Variable Importance Measure (VI)</h3>
<p>For bagged/RF regression trees, VI is the total amount of RSS decreased due to splits over a given predictor, averaged over all <span class="math inline">\(B\)</span> trees. A large VI value indicates an important predictor. For bagged/RF classification trees, replacing RSS with Gini index or cross-entorpy.</p>
</section>
</section>
<section id="boosting" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="boosting"><span class="header-section-number">8.5</span> Boosting</h2>
<p>Like Bagging, boosting is a general approach that can be applied to many statistical learning methods for regression and classification. Boosting does not involve bootstrap sampling; Boosting grows trees <em>sequentially</em> by <strong>slow</strong> learning: each new tree is grown by fitting a new tree to the residuals (modified version of the original data set) left over from the previous trees, and then a shrunken version of the new tree is added to the model. Each new tree tries to capture signal that is not yet accounted for by the current set of trees.</p>
<section id="boosting-algorithm-for-regression-trees" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="boosting-algorithm-for-regression-trees"><span class="header-section-number">8.5.1</span> Boosting Algorithm for regression trees</h3>
<ol type="1">
<li>Set <span class="math inline">\(\hat{f}(x)=0\)</span>, and <span class="math inline">\(r_i=y_i\)</span> for all <span class="math inline">\(i\)</span> in the training set.</li>
<li>For <span class="math inline">\(b=1, \cdots, B\)</span>, repeat 2.1. Fit a tree <span class="math inline">\(\hat{F}^b\)</span> with <span class="math inline">\(d\)</span> splits (<span class="math inline">\(d+1\)</span> terminal nodes, can involve at most <span class="math inline">\(d\)</span> variables) to the training data <span class="math inline">\((X,r)\)</span>. 2.2. Update <span class="math inline">\(\hat(f)\)</span> by adding a shrunken version of the new tree: <span class="math display">\[
\hat{f}(x) \leftarrow \hat{f}(x) +\lambda \hat{f}^b(x).
\]</span> 2.3. Update the residuals <span class="math display">\[
r_i \leftarrow r_i -\lambda \hat{f}^b(x_i).
\]</span></li>
<li>Output the boosted model <span class="math display">\[
  \hat{f}(x)=\sum_{b=1}^B \lambda \hat{f}^b (x).
\]</span> Each new tree can be rather small (<span class="math inline">\(d=1\)</span> or 2, hence with low variance) controlled by the parameter <span class="math inline">\(d\)</span>. By fitting a small tree to the residual, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well. The shrinkage <span class="math inline">\(\lambda\)</span> slows the learning, allowing more and different shaped trees to attack the residuals.</li>
</ol>
</section>
<section id="tuning-parameters-for-boosting" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="tuning-parameters-for-boosting"><span class="header-section-number">8.5.2</span> Tuning parameters for Boosting</h3>
<ul>
<li>The number of trees <span class="math inline">\(B\)</span>: unlike bagging and random forests, boosting can overfit if <span class="math inline">\(B\)</span> is too large, although overfitting tends to occur slowly. <span class="math inline">\(B\)</span> is selected with cross-validation.</li>
<li>The shrinkage parameter <span class="math inline">\(\lambda\)</span>: Typical values are 0.01 or 0.001. Very small <span class="math inline">\(\lambda\)</span> may require very large <span class="math inline">\(B\)</span>.</li>
<li>The number of splits <span class="math inline">\(d\)</span>: <span class="math inline">\(d\)</span> controls the complexity of the boosted ensemble. Often <span class="math inline">\(d=1\)</span> works well, in which case each tree is a <em>stump</em>, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only one variable, hence easy to interpret. <span class="math inline">\(d\)</span> is the <em>interaction depth</em>, controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most <span class="math inline">\(d\)</span> variables.</li>
</ul>
</section>
</section>
<section id="bayesian-additive-regression-trees-bart" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="bayesian-additive-regression-trees-bart"><span class="header-section-number">8.6</span> Bayesian Additive Regression Trees (BART)</h2>
<p>BART is related to the approaches used by both bagging and boosting. We only make use of the original data (not using bootstrap) and their modified version (residuals from other trees) , and grow trees sequentially.</p>
<ul>
<li>each tree tries to capture the signal not yet accounted by for by the current model, as in boosting.</li>
<li>each tree is constructed in a random manner as in bagging and RF</li>
</ul>
<p>The main novelty of BART is the way in which new trees are generated. Assume there are <span class="math inline">\(K\)</span> trees and <span class="math inline">\(B\)</span> iterations. Let <span class="math inline">\(\hat{f}^b_k(x)\)</span> be the prediction at <span class="math inline">\(x\)</span> for the <span class="math inline">\(k\)</span>th tree used in the <span class="math inline">\(b\)</span>th iteration.</p>
<p>Initially, BART initializes all trees to be a single root node, with <span class="math inline">\(\hat{f}^1_k(x)=\frac{1}{nK}\sum_{i=1}^n y_i\)</span>. Thus <span class="math inline">\(\hat{f}^1(x)= \sum_{k=1}^K\hat{f}^1_k(x)=\frac{1}{n}\sum_{i=1}^n y_i\)</span>.</p>
<p>In subsequent iteration, BART updates each of the <span class="math inline">\(K\)</span> trees, one at a time. In the <span class="math inline">\(b\)</span>-th iteration, to update the <span class="math inline">\(k\)</span>th tree, obtain a <em>partial residual</em> by subtracting from each response <span class="math inline">\(y_i\)</span> the predictions from all but the <span class="math inline">\(k\)</span>-th tree, <span class="math display">\[
r_i = y_i -\sum_{k'&lt;k}\hat{f}^b_{k'}(x_i) - \sum_{k'&gt;k}\hat{f}^{b-1}_{k'}(x_i)
\]</span> for each observation <span class="math inline">\(i=1, \cdots, n\)</span>. Note when <span class="math inline">\(k'&lt;k\)</span>, the trees are updated already in the <span class="math inline">\(b\)</span>-th iteration, and for <span class="math inline">\(k'&gt;k\)</span>, the trees are from the previous iteration <span class="math inline">\(b-1\)</span>. Rather than fitting a fresh tree to this partial residual <span class="math inline">\(r_i\)</span>, BART obtain a new tree <span class="math inline">\(\hat{f}^b_k\)</span> by <em>randomly perturb</em> the tree <span class="math inline">\(\hat{f}^{b-1}_k\)</span> from the <span class="math inline">\((b-1)\)</span>-th iteration via the following operations:</p>
<ol type="1">
<li>change the structure of <span class="math inline">\(\hat{f}^{b-1}_k\)</span> by adding or pruning branches</li>
<li>keep the same structure of <span class="math inline">\(\hat{f}^{b-1}_k\)</span> but perturb the prediction values.</li>
</ol>
<p>Perturbations that improve the fit are favored. The perturbation only modifies the previous tree slightly hence guard against overfitting. In addition, the size of each tree is limited to avoid overfitting. The perturbation can be interpreted as drawing a new tree from a <em>posterior</em> distribution via <em>Markov chain Monte Carlo</em> sampling. The perturbation avoids local minima and achieve a more thorough exploration of the model space.</p>
<p>At the end of each iteration, the <span class="math inline">\(K\)</span> trees from that iteration will be summed, i.e.&nbsp;<span class="math inline">\(\hat{f}^b(x)=\sum_{k=1}^K \hat{f}^b_k(x)\)</span> for <span class="math inline">\(b=1, \cdots, B\)</span>.</p>
<p>Finally, computer the mean (or other quantities such as percentile) after <span class="math inline">\(L\)</span> burn-in samples: <span class="math display">\[
\hat{f}(x) = \frac{1}{B-L} \sum^B_{b=L+1} \hat{f}^b(x).
\]</span> During the <em>burn-in</em> period- the first <span class="math inline">\(L\)</span> iterations, <span class="math inline">\(\hat{f}^{\ell}\)</span>, <span class="math inline">\(\ell &lt;=L\)</span> tends not to provide good results, hence are discarded.</p>
<p>BART has very impressive out-of-box performance: perform well (not overfitting) with minimal tuning.</p>
<p>Parameters:</p>
<ul>
<li>number of trees, <span class="math inline">\(K\)</span>: e.g., <span class="math inline">\(K=200\)</span></li>
<li>number of iterations: <span class="math inline">\(B\)</span>: e.g., <span class="math inline">\(B=1000\)</span></li>
<li>burn-in iterations <span class="math inline">\(L\)</span>: e.g., <span class="math inline">\(L=100\)</span>.</li>
</ul>
</section>
<section id="homework" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="homework"><span class="header-section-number">8.7</span> Homework:</h2>
<ul>
<li>Conceptual: 1–6</li>
<li>Applied: At least one.</li>
</ul>
</section>
<section id="code-snippet" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="code-snippet"><span class="header-section-number">8.8</span> Code Snippet</h2>
<section id="python" class="level3" data-number="8.8.1">
<h3 data-number="8.8.1" class="anchored" data-anchor-id="python"><span class="header-section-number">8.8.1</span> Python</h3>
<pre><code>
</code></pre>
</section>
<section id="numpy" class="level3" data-number="8.8.2">
<h3 data-number="8.8.2" class="anchored" data-anchor-id="numpy"><span class="header-section-number">8.8.2</span> Numpy</h3>
<pre><code>np.asarray(D)

</code></pre>
</section>
<section id="pandas" class="level3" data-number="8.8.3">
<h3 data-number="8.8.3" class="anchored" data-anchor-id="pandas"><span class="header-section-number">8.8.3</span> Pandas</h3>
<pre><code>feature_imp.sort_values(by='importance', ascending=False)</code></pre>
</section>
<section id="graphics" class="level3" data-number="8.8.4">
<h3 data-number="8.8.4" class="anchored" data-anchor-id="graphics"><span class="header-section-number">8.8.4</span> Graphics</h3>
<pre><code>
</code></pre>
</section>
<section id="islp-and-statsmodels" class="level3" data-number="8.8.5">
<h3 data-number="8.8.5" class="anchored" data-anchor-id="islp-and-statsmodels"><span class="header-section-number">8.8.5</span> ISLP and statsmodels</h3>
<pre><code>
</code></pre>
</section>
<section id="sklearn" class="level3" data-number="8.8.6">
<h3 data-number="8.8.6" class="anchored" data-anchor-id="sklearn"><span class="header-section-number">8.8.6</span> sklearn</h3>
</section>
<section id="useful-code-snippets" class="level3" data-number="8.8.7">
<h3 data-number="8.8.7" class="anchored" data-anchor-id="useful-code-snippets"><span class="header-section-number">8.8.7</span> Useful code snippets</h3>
<section id="classification-decision-tree" class="level4" data-number="8.8.7.1">
<h4 data-number="8.8.7.1" class="anchored" data-anchor-id="classification-decision-tree"><span class="header-section-number">8.8.7.1</span> Classification Decision Tree</h4>
<pre><code>from sklearn.metrics import (accuracy_score,
                             log_loss)
                             
clf = DTC(criterion='entropy',
          max_depth=3,
          random_state=0)        
clf.fit(X, High)
accuracy_score(High, clf.predict(X))
resid_dev = log_loss(High, clf.predict_proba(X))
ax = subplots(figsize=(12,12))[1]
plot_tree(clf,
          feature_names=feature_names,
          ax=ax);
print(export_text(clf,
                  feature_names=feature_names,
                  show_weights=True))
                  
# Using validaiton approach to train and test the model
validation = skm.ShuffleSplit(n_splits=1,
                              test_size=200,
                              random_state=0)
results = skm.cross_validate(clf,
                             D,
                             High,
                             cv=validation)
results['test_score']
</code></pre>
</section>
<section id="pruning-a-classifcaiton-decision-tree" class="level4" data-number="8.8.7.2">
<h4 data-number="8.8.7.2" class="anchored" data-anchor-id="pruning-a-classifcaiton-decision-tree"><span class="header-section-number">8.8.7.2</span> Pruning a classifcaiton Decision tree</h4>
<pre><code>(X_train,
 X_test,
 High_train,
 High_test) = skm.train_test_split(X,
                                   High,
                                   test_size=0.5,
                                   random_state=0)
clf = DTC(criterion='entropy', random_state=0)
clf.fit(X_train, High_train)
accuracy_score(High_test, clf.predict(X_test))
ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)
kfold = skm.KFold(10,
                  random_state=1,
                  shuffle=True)
grid = skm.GridSearchCV(clf,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True, # Refit the best estimator with the entire dataset
                        cv=kfold,
                        scoring='accuracy')
grid.fit(X_train, High_train)
grid.best_score_
best_ = grid.best_estimator_
best_.tree_.n_leaves
print(accuracy_score(High_test,
                     best_.predict(X_test)))
confusion = confusion_table(best_.predict(X_test),
                            High_test)
</code></pre>
</section>
<section id="fitting-a-regression-tree" class="level4" data-number="8.8.7.3">
<h4 data-number="8.8.7.3" class="anchored" data-anchor-id="fitting-a-regression-tree"><span class="header-section-number">8.8.7.3</span> Fitting a regression tree</h4>
<pre><code>reg = DTR(max_depth=3)
reg.fit(X_train, y_train)
ax = subplots(figsize=(12,12))[1]
plot_tree(reg,
          feature_names=feature_names,
          ax=ax);
</code></pre>
</section>
<section id="pruning-a-regression-tree" class="level4" data-number="8.8.7.4">
<h4 data-number="8.8.7.4" class="anchored" data-anchor-id="pruning-a-regression-tree"><span class="header-section-number">8.8.7.4</span> Pruning a regression tree</h4>
<pre><code>ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)
kfold = skm.KFold(5,
                  shuffle=True,
                  random_state=10)
grid = skm.GridSearchCV(reg,
                        {'ccp_alpha': ccp_path.ccp_alphas},
                        refit=True,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
G = grid.fit(X_train, y_train)
best_ = grid.best_estimator_
np.mean((y_test - best_.predict(X_test))**2)
</code></pre>
</section>
<section id="bagging-and-rg" class="level4" data-number="8.8.7.5">
<h4 data-number="8.8.7.5" class="anchored" data-anchor-id="bagging-and-rg"><span class="header-section-number">8.8.7.5</span> Bagging and RG</h4>
<pre><code>bag_boston = RF(max_features=X_train.shape[1], random_state=0, n_estimators=500)
bag_boston.fit(X_train, y_train)
y_hat_bag = bag_boston.predict(X_test)


#RF
RF_boston = RF(max_features=6,
               random_state=0).fit(X_train, y_train)
y_hat_RF = RF_boston.predict(X_test)
np.mean((y_test - y_hat_RF)**2)

#VI
feature_imp = pd.DataFrame(
    {'importance':RF_boston.feature_importances_},
    index=feature_names)
feature_imp.sort_values(by='importance', ascending=False)
</code></pre>
</section>
<section id="gradient-boosting" class="level4" data-number="8.8.7.6">
<h4 data-number="8.8.7.6" class="anchored" data-anchor-id="gradient-boosting"><span class="header-section-number">8.8.7.6</span> Gradient Boosting</h4>
<pre><code>boost_boston = GBR(n_estimators=5000,
                   learning_rate=0.001,
                   max_depth=3,
                   random_state=0)
boost_boston.fit(X_train, y_train)
test_error = np.zeros_like(boost_boston.train_score_)
for idx, y_ in enumerate(boost_boston.staged_predict(X_test)):
   test_error[idx] = np.mean((y_test - y_)**2)

plot_idx = np.arange(boost_boston.train_score_.shape[0])
ax = subplots(figsize=(8,8))[1]
ax.plot(plot_idx,
        boost_boston.train_score_,
        'b',
        label='Training')
ax.plot(plot_idx,
        test_error,
        'r',
        label='Test')
ax.legend();
</code></pre>
</section>
<section id="bart" class="level4" data-number="8.8.7.7">
<h4 data-number="8.8.7.7" class="anchored" data-anchor-id="bart"><span class="header-section-number">8.8.7.7</span> BART</h4>
<pre><code>bart_boston = BART(random_state=0, burnin=5, ndraw=15) #num_trees=200, max_states=100
# ndraw: number of iterations or samples to draw from the posterior distribution after the burn-in 
bart_boston.fit(X_train, y_train)
yhat_test = bart_boston.predict(X_test.astype(np.float32))
np.mean((y_test - yhat_test)**2)

# Variable Inclusion
var_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),
                               index=D.columns)
</code></pre>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch7.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Moving Beyond Linearity</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch9.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Chapter 9: Support Vector Machine</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>