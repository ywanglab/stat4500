<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.398">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stat4500notes - 12&nbsp; Chapter 12: Unsupervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./class_project.html" rel="next">
<link href="./ch11.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch12.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Chapter 12: Unsupervised Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">stat4500notes</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting up Python Computing Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Moving Beyond Linearity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 8: Tree-Based Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Chapter 9: Support Vector Machine</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Chapter 10: Deep Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chapter 11: Survival Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch12.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Chapter 12: Unsupervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Class Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#pca" id="toc-pca" class="nav-link active" data-scroll-target="#pca"><span class="header-section-number">12.1</span> PCA</a></li>
  <li><a href="#matrix-completion-by-using-pca-to-impute-missing-values" id="toc-matrix-completion-by-using-pca-to-impute-missing-values" class="nav-link" data-scroll-target="#matrix-completion-by-using-pca-to-impute-missing-values"><span class="header-section-number">12.2</span> Matrix completion by using PCA to impute missing values</a></li>
  <li><a href="#clustering-finding-subgroups-or-clusters" id="toc-clustering-finding-subgroups-or-clusters" class="nav-link" data-scroll-target="#clustering-finding-subgroups-or-clusters"><span class="header-section-number">12.3</span> Clustering: finding subgroups or clusters</a>
  <ul class="collapse">
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="header-section-number">12.3.1</span> K-means clustering</a></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="header-section-number">12.3.2</span> Hierarchical clustering</a></li>
  <li><a href="#types-of-linkage" id="toc-types-of-linkage" class="nav-link" data-scroll-target="#types-of-linkage"><span class="header-section-number">12.3.3</span> Types of linkage</a></li>
  <li><a href="#choice-of-dissimilarity-measure-and-variable-scaling" id="toc-choice-of-dissimilarity-measure-and-variable-scaling" class="nav-link" data-scroll-target="#choice-of-dissimilarity-measure-and-variable-scaling"><span class="header-section-number">12.3.4</span> Choice of dissimilarity measure and variable scaling</a></li>
  </ul></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework"><span class="header-section-number">12.4</span> Homework:</a></li>
  <li><a href="#code-snippet" id="toc-code-snippet" class="nav-link" data-scroll-target="#code-snippet"><span class="header-section-number">12.5</span> Code Snippet</a>
  <ul class="collapse">
  <li><a href="#python" id="toc-python" class="nav-link" data-scroll-target="#python"><span class="header-section-number">12.5.1</span> Python</a></li>
  <li><a href="#numpy" id="toc-numpy" class="nav-link" data-scroll-target="#numpy"><span class="header-section-number">12.5.2</span> Numpy</a></li>
  <li><a href="#pandas" id="toc-pandas" class="nav-link" data-scroll-target="#pandas"><span class="header-section-number">12.5.3</span> Pandas</a></li>
  <li><a href="#graphics" id="toc-graphics" class="nav-link" data-scroll-target="#graphics"><span class="header-section-number">12.5.4</span> Graphics</a></li>
  <li><a href="#islp-and-statsmodels" id="toc-islp-and-statsmodels" class="nav-link" data-scroll-target="#islp-and-statsmodels"><span class="header-section-number">12.5.5</span> ISLP and statsmodels</a></li>
  <li><a href="#sklearn" id="toc-sklearn" class="nav-link" data-scroll-target="#sklearn"><span class="header-section-number">12.5.6</span> sklearn</a></li>
  <li><a href="#k-clustering" id="toc-k-clustering" class="nav-link" data-scroll-target="#k-clustering"><span class="header-section-number">12.5.7</span> K-Clustering</a></li>
  <li><a href="#hierarchical-clustering-1" id="toc-hierarchical-clustering-1" class="nav-link" data-scroll-target="#hierarchical-clustering-1"><span class="header-section-number">12.5.8</span> Hierarchical Clustering</a></li>
  <li><a href="#useful-code-snippets" id="toc-useful-code-snippets" class="nav-link" data-scroll-target="#useful-code-snippets"><span class="header-section-number">12.5.9</span> Useful code snippets</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Chapter 12: Unsupervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Unsupervised learning only observes features <span class="math inline">\(X_1, X_2, \cdots, X_p\)</span> (<strong>unlabeled data</strong>), no associated response <span class="math inline">\(Y\)</span>. Obtaining such data may be easier (and cheaper) in contrast to <strong>labeled data</strong> which may require <em>costly</em> human intervention. The goal is to learn interesting information about the predictors. For instance, 1) reduce the dimension to 2 or 3 to visualize data in EDA 2) discover subgroups (clustering).</p>
<p>Two methods: - PCA: for data visualization and pre-processing for supervised learning - clustering: discovering unknown subgroups.</p>
<p>Challenges: - no simple goal (response), hence may be more subjective</p>
<p>Important Applications: - discover subgroups by the data itself: such as movie viewers, shoppers, patients</p>
<section id="pca" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="pca"><span class="header-section-number">12.1</span> PCA</h2>
<p>PCA produces a low-dimensional representation of a dataset that explains a good fraction of the variance. It finds a sequence of new variables each of which is a linear combination of the original variables. Those new variables are mutually uncorrelated and are ordered in a decreasing order of variance.</p>
<ul>
<li><p>PCA is used for data visualization and pre-processing for supervised learning, such as PCR.</p></li>
<li><p><em>first PCA</em>: is the result choosing the loadings <span class="math inline">\(\phi_{j1}\)</span>, for <span class="math inline">\(1\le j \le p\)</span> such that the new variable <span class="math inline">\(Z_1\)</span> has teh largest sample variance. <span class="math display">\[
Z_1 =\phi_{11}X_1 +\phi_{21}X_2+\cdots +\phi_{p1}X_p
\]</span> has the largest variance, where the coefficients <span class="math inline">\(\phi_{11}, \cdots, \phi_{p1}\)</span> are the PCA <em>loadings</em> <strong>normalized</strong> such that <span class="math inline">\(\sum_{j=1}^p \phi_{j1}^2=1\)</span>. The normalization avoid the variance of <span class="math inline">\(Z_1\)</span> becomes arbitrary large. The vector <span class="math inline">\(\phi_1=(\phi_{11}, \cdots, \phi_{p1})^T\)</span> is called the <em>PCA loading vector</em> or (PCA direction) in the feature space along which data vary the most. If we project the <span class="math inline">\(N\)</span> data points <span class="math inline">\(x_1, \cdots, x_N\)</span> onto this direction, then the projected values are the principle component <strong>scores</strong> <span class="math inline">\(z_{11} , z_{21}, \cdots, z_{n1}\)</span> for each of the data points.</p></li>
<li><p>second PCA: similarly, <span class="math inline">\(Z_2\)</span> is a linear combination of the original variables that is uncorrelated from <span class="math inline">\(Z_1\)</span> and has the second largest sample variance. The uncorrelation of <span class="math inline">\(Z_2\)</span> and <span class="math inline">\(Z_1\)</span> is equivalent to <span class="math inline">\(\phi_2\)</span> is orthogonal to <span class="math inline">\(\phi_1\)</span>.</p></li>
<li><p>Each loading vector and the corresponding PC are unique up to sign flips, this is because the after flipping the sign, the loading vector still defines the same direction.</p></li>
<li><p>PCA <em>biplot</em>: display both PCA scores and loadings of the first two PCA for each data points. The middle point <span class="math inline">\((0,0)\)</span> indicate an observation with average levels of both principal components.</p></li>
<li><p>Scaling the variables matters. If the variables are in different units, it is recommended to scale them to have s.d equal to one.</p></li>
<li><p>PCA may be understood as converting the original (<strong>demeaned</strong>) variables <span class="math inline">\(X=(X_1, X_2, \cdots, X_p)^T\)</span> to new variables <span class="math inline">\(Z=(Z_1, \cdots, Z_p)^T=V^TX\)</span> via an orthogonal transformation matrix <span class="math inline">\(V\)</span>, in order to un-correlate the original variables, such that the new variables <span class="math inline">\(Z\)</span> are uncorrelated, that is ( Note that, <span class="math inline">\({\bf Z}\)</span> and <span class="math inline">\({\bf X}\)</span> are both data matrix of size <span class="math inline">\(N\times p\)</span>, and <span class="math inline">\({\bf Z}={\bf X}V\)</span> ) <span class="math display">\[
{\bf Z}^T{\bf Z} = V^T({\bf X}^T{\bf X})V= D^2.
\]</span> where <span class="math inline">\(V\)</span> and <span class="math inline">\(D\)</span> are defined by the eigen-decomposition of <span id="eq-eigen-decom"><span class="math display">\[
{\bf X}^T{\bf X}=VD^2V^T
\tag{12.1}\]</span></span> that is, <span class="math inline">\(V=[v_1, \cdots, v_p]\)</span> is an <span class="math inline">\(p\times p\)</span> orthogonal matrix formed by the orthonormal eigenvectors of the matrix <span class="math inline">\({\bf X}^T{\bf X}\)</span> which is the sample covariance matrix (multiplied by <span class="math inline">\(N-1\)</span>) , and <span class="math inline">\(D\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix of singular values of <span class="math inline">\({\bf X}\)</span> (or equivalently, square roots of the eigenvalues of <span class="math inline">\({\bf X}^T{\bf X}\)</span>). The <span class="math inline">\(v_j\)</span>’s are called the <em>right singular vector</em> of <span class="math inline">\({\bf X}\)</span> and are ordered in decreasing order of the singular values <span class="math inline">\(d_j\)</span>.</p></li>
<li><p>SVD: The construction of <span class="math inline">\(V\)</span> and <span class="math inline">\(D\)</span> in <a href="#eq-eigen-decom" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-eigen-decom</span></a> allows the further construction of the SVD of <span class="math inline">\({\bf X}\)</span> by <span id="eq-svd"><span class="math display">\[{\bf X}=UDV^T =\sum_{j=1}^p d_ju_jv_j^T = \sum_{j=1}^p {\bf X}v_jv_j^T.  \tag{12.2}\]</span></span> The <span class="math inline">\(N\times p\)</span> matrix <span class="math inline">\(U\)</span> consists of orthonormal vectors <span class="math inline">\(u_j=\frac{{\bf X}v_j}{d_j}\)</span>, and spans the column space of <span class="math inline">\({\bf X}\)</span>. The <span class="math inline">\(v_j\)</span> spans the row space of <span class="math inline">\({\bf X}\)</span>.</p></li>
</ul>
<p>Note with the SVD of <span class="math inline">\({\bf X}=UDV^T\)</span>, <span class="math inline">\({\bf Z}= [z_1, \cdots, z_p]={\bf X}V=UD\)</span>.Hence, <span class="math inline">\(z_i, z_j\)</span> are orthogonal.</p>
<ul>
<li>PCA can also be understood as the solution to the following optimization problem to find the best <span class="math inline">\(M\)</span>-dimensional approximation <span class="math inline">\(x_{ij}\approx \sum_{m=1}^M a_{im} b_{jm}\)</span>, assuming the data matrix is centered <span id="eq-pca-approx"><span class="math display">\[
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{j=1}^p\sum_{i=1}^n \left(x_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
\tag{12.3}\]</span></span> It turns out that <span class="math inline">\(\hat{\bf A}\)</span> and <span class="math inline">\(\hat{\bf B}\)</span> solved the above problem are in fact the first <span class="math inline">\(M\)</span> PCA scores and loading vectors.</li>
</ul>
<p>Thus, the first PCA loading vector <span class="math inline">\(\phi_1\)</span> defines a direction (line) in the <span class="math inline">\(p\)</span>-dimensional feature space that is <em>closest</em> to the <span class="math inline">\(n\)</span> observations in terms of the average squared Euclidean distance. Similarly the first two PCA loading vectors span the plane that is closest to the <span class="math inline">\(n\)</span> observations, in terms of the average squared Euclidean distance.</p>
<ul>
<li>total variance: The total variance <span class="math display">\[
\sum_{j=1}^p Var[X_j] = \sum_{j=1}^pVar[Z_j]=tr({\bf Z}^T{\bf Z})=tr({\bf X}^T{\bf X})
\]</span> The Proportion of Variance explained by the <span class="math inline">\(m\)</span>-th PCA is given by (assuming the data are centered) <span id="eq-PVE"><span class="math display">\[
PVE_m=\frac{\sum_{i=1}^n z_{im}^2}{\sum_{j=1}^p\sum_{i=1}^n x_{ij}^2}
\tag{12.4}\]</span></span> and <span class="math inline">\(\sum_{m=1}^p PVE_m = 1\)</span>.</li>
</ul>
<p>The total variance of the data can be decomposed as <span class="math display">\[
\text{var. of data }= \text{var. of first M PCs } + \text{MSE of M-dim approximation}
\]</span> i.e., <span class="math display">\[
\sum_{j=1}^p\frac{1}{n} \sum_{i=1}^n x_{ij}^2 = \sum_{m=1}^M\frac{1}{n} \sum_{i=1}^n z_{im}^2 + \frac{1}{n} \sum_{j=1}^p \sum_{i=1}^n \left( x_{ij}- \sum_{m=1}^M z_{im}\phi_{jm}\right)^2
\]</span> So maximizing the variance of the first <span class="math inline">\(M\)</span>-PCs, is equivalent to minimize the MSE of the <span class="math inline">\(M\)</span>-dim approximation, and vice versa. Using the PVE defined in <a href="#eq-PVE" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-PVE</span></a> and diving the above equation by the total variance, we obtain <span class="math display">\[
1-\frac{\sum_{j=1}^p \sum_{i=1}^n \left( x_{ij}- \sum_{m=1}^M z_{im}\phi_{jm}\right)^2}{\sum_{j=1}^p \sum_{i=1}^n x_{ij}^2}=1-\frac{RSS}{RSS}=R^2
\]</span> So the PVE explained by the first <span class="math inline">\(M\)</span> principle components can be interpreted as the <span class="math inline">\(R^2\)</span> of the approximation for ${} $ using the first <span class="math inline">\(M\)</span> PC’s.</p>
<ul>
<li>How many PCA’s to choose? no simple answer, as the cross-validation can not be used because we have to use the entire data set. The <em>“scree plot”</em> which plots PVE of each PC’s can be used to look for an “elbow”. Note <span class="math inline">\(X=VZ\)</span>. If you <span class="math inline">\(M&lt;p\)</span> PCA is chosen, then <span class="math display">\[x_i=(x_{i1}, \cdots, x_{ip})^T \approx \sum_{m=1}^M v_m z_{im},\qquad x_{ij}\approx \sum_{m=1}^M v_{jm} z_{im}
\]</span></li>
</ul>
</section>
<section id="matrix-completion-by-using-pca-to-impute-missing-values" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="matrix-completion-by-using-pca-to-impute-missing-values"><span class="header-section-number">12.2</span> Matrix completion by using PCA to impute missing values</h2>
<p>Missing values may be simply dropped or replaced with mean or median. But we can do better by exploiting the correlation between the variables, if the missingness is random (nor informative, such as missing value was because it was too large). Consider a modified version of <a href="#eq-pca-approx" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-pca-approx</span></a>, <span class="math display">\[
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{(i,j)\in \mathcal{O}} \left(x_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
\]</span> where <span class="math inline">\(\mathcal{O}\)</span> is the set of all <em>observed</em> indices <span class="math inline">\((i,j)\)</span>, a subset of all possible <span class="math inline">\(n\times p\)</span> pairs. The problem can be solved approximately using the “Hard-Impute” algorithm. The algorithm not only estimates <span class="math inline">\(\hat{a}_{im}\)</span> and <span class="math inline">\(\hat{b}_{jm}\)</span>, but can approximately recover the <span class="math inline">\(M\)</span> PC scores and loadings.</p>
<p><strong>(Hard-Impute) Algorithm</strong>:</p>
<ol type="1">
<li>Create a complete matrix <span class="math inline">\(\tilde{\bf X}\)</span> with missing elements <span class="math inline">\(\tilde{x}_{ij}\)</span> replaced by the mean <span class="math inline">\(\bar{x}_j\)</span> of the observed for the <span class="math inline">\(j\)</span>-th variable.</li>
<li>repeat *a)-(c) until the objective <a href="#eq-matrix-complete" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-matrix-complete</span></a> fails to decarese:</li>
</ol>
<ol type="a">
<li>Solve <span class="math display">\[
\min_{{\bf A}\in \mathbb{R}^{n\times M}, {\bf B}\in \mathbb{R}^{p\times M}} \left\{ \sum_{j=1}^p\sum_{i=1}^n \left(\tilde{x}_{ij} - \sum_{m=1}^M a_{im} b_{jm}   \right) \right\}.
\]</span> by computing the PC’s of <span class="math inline">\(\tilde{\bf X}\)</span>.</li>
<li>imputing the missing values by set <span class="math inline">\(\tilde{x}_{ij}\leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)</span> for a fixed <span class="math inline">\(M\)</span>.</li>
<li>Computing the objective error using the observed data (a supervised way) <span id="eq-matrix-complete"><span class="math display">\[
  \sum_{(i,j)\in \mathcal{O}} \left(x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm}   \right)
   \tag{12.5}\]</span></span></li>
</ol>
<ol start="3" type="1">
<li>Return the estimated missing entries <span class="math inline">\(\tilde{x}_{ij}, (i,j)\notin \mathcal{O}\)</span>.</li>
</ol>
<p>The algorithm may be used in powering a recommender systems.</p>
</section>
<section id="clustering-finding-subgroups-or-clusters" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="clustering-finding-subgroups-or-clusters"><span class="header-section-number">12.3</span> Clustering: finding subgroups or clusters</h2>
<p>Seek a partition of the data into distinct subgroups so that the observations are quite similar to each other within each subgroup. Clustering looks for <em>homogeneous subgroups</em> among the observations. Example: market segmentation, gene expressions</p>
<p>similarity are domain-specifically defined.</p>
<p>There are two types of clustering tasks: 1) cluster observations on the basis of the features in order to identify subgroups among observations. 2) Cluster the features on the basis of the observations in order to discover subgroups among features.</p>
<p>Clustering often are not robust.</p>
<section id="k-means-clustering" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">12.3.1</span> K-means clustering</h3>
<p>Partition data into a pre-specific number of disjoint clusters. Let <span class="math inline">\(C_k\)</span>, <span class="math inline">\(1\le k \le K\)</span> denotes the mutually <em>disjoint</em> sets of indices of the observations and their union is the entire data set. The goal is to find K-clusters such that the <em>within-cluster variation</em> (<span class="math inline">\(WCV(C_k)\)</span>) is least, i.e., we want to solve minimize the total WCV of all <span class="math inline">\(K\)</span> clusters <span id="eq-clustering-var"><span class="math display">\[
\min_{C_1, \cdots, C_K} \left\{ \sum_{k=1}^K WCV(C_k) \right\}.
\tag{12.6}\]</span></span> where, <span class="math display">\[
WCV(C_k)=\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j'})^2
\]</span> The problem is difficult to solve because there are <span class="math inline">\(K^n\)</span> (each data point can have <span class="math inline">\(K\)</span> choices ) ways to partition <span class="math inline">\(n\)</span> data points into <span class="math inline">\(K\)</span> clusters.</p>
<p><strong>K-means Algorithm</strong>:</p>
<ol type="1">
<li>initial clustering: randomly assign a cluster to an observation.</li>
<li>Iterate until the cluster assignments stop changing: 2.1 (centering) For each of the <span class="math inline">\(K\)</span> cluster, computer <em>centroid</em>, which is simply the average of all <span class="math inline">\(p\)</span>-feature observations in the <span class="math inline">\(k\)</span>-the cluster. 2.2 (clustering assignment) Assign each observation to the cluster whose centro id is the closest (in terms of Euclidean distance).</li>
</ol>
<ul>
<li><p>The algorithm is guaranteed to reduce the total variance in <a href="#eq-clustering-var" class="quarto-xref">Equation&nbsp;<span class="quarto-unresolved-ref">eq-clustering-var</span></a> at each step. This can be seen from <span class="math display">\[
\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p(x_{ij}-x_{i'j'})^2=2\sum_{i\in C_k}\sum_{j=1}^p (x_{ij}-\bar{x}_{kj})^2
\]</span> where <span class="math inline">\(\bar{x}_{kj}=\frac{1}{|C_k|}\sum_{i\in C_k}x_{ij}\)</span> is the mean for feature <span class="math inline">\(j\)</span> in cluster <span class="math inline">\(C_k\)</span>.</p></li>
<li><p>But it is not guaranteed to give the global minimum. It only finds a local minimum.</p></li>
<li><p>In practice, multiple times of the K-clustering should be performed and the best one can be chosen, this is because K-means only finds a local minimum that depends on the initial clustering.</p></li>
</ul>
</section>
<section id="hierarchical-clustering" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">12.3.2</span> Hierarchical clustering</h3>
<p>Ends up with a tree-like <em>dendrogram</em> of the observations, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 (no cut line) to <span class="math inline">\(n\)</span> (cut at height 0). After a horizontal cut line is drawn, the distinct sets of observations beneath the cut can be interpreted as clusters. The cut height serves the same role as <span class="math inline">\(K\)</span> in K-means clustering.</p>
<p>The most common type of hierarchical clustering uses a <em>bottom-up</em> or <em>agglomerative</em> clustering: a dendrogram is built starting from the leaves and combining clusters up to the trunk.</p>
<p>The term <em>hierarchical</em> refers to the fact that clusters obtained by cutting the deprogram at a given height are necessarily nested within the clusters obtained by cutting at a greater height. However, this assumption may be unrealistic. for example, our observations correspond to a group of men and women, evenly split among Americans, Japanese, and French. We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality. In this case, the ture clusters are not nested, in the sense that the best divsion into three groups does not result from taking the best division into two groups and splitting up one of those groups.</p>
<p>So hierarchical clustering can sometiems yield <em>worse</em> results than K-means clustering.</p>
<p><strong>Algorithm</strong>: 1. start with each point in its own cluster. 2. identify the <em>closest</em> two clusters and merge them. The height of a fusion point of these two clusters, indicate how different they are. Note one can not conclude the similarity of two observations based on their horizontal proximity. 3. repeat 4. Ends when all points are in a single cluster.</p>
<p>In general scaling is recommended.</p>
</section>
<section id="types-of-linkage" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="types-of-linkage"><span class="header-section-number">12.3.3</span> Types of linkage</h3>
<p>Linkage is used to calculate the dissimilarity between a pair of clusters with more than one observations. Average and complete linkages are generally preferred, as they tend to yield more balanced dendrograms.</p>
<ul>
<li>Complete: maximal inter-cluster dissimilarity: use the maximum pairwise dissimilarities between observations in two clusters.</li>
<li>single: Minimal inter-cluster dissimilarity. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.</li>
<li>Average: mean inter-cluster dissimilarity.</li>
<li>Centroid: dissimilarity between the centroids of two classes. May result in undesirable <em>inversions</em>, where two clusters are fused at a height <em>below</em> either of the individual clusters.</li>
</ul>
</section>
<section id="choice-of-dissimilarity-measure-and-variable-scaling" class="level3" data-number="12.3.4">
<h3 data-number="12.3.4" class="anchored" data-anchor-id="choice-of-dissimilarity-measure-and-variable-scaling"><span class="header-section-number">12.3.4</span> Choice of dissimilarity measure and variable scaling</h3>
<ul>
<li>Euclidean distance</li>
<li>correlation-based distance: correlation between two observations (not the usual correlation between variables). Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes.</li>
<li>one should carefully consider if the variables need to be scaled. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance.</li>
</ul>
</section>
</section>
<section id="homework" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="homework"><span class="header-section-number">12.4</span> Homework:</h2>
<ul>
<li>Conceptual: 1–</li>
<li>Applied: At least one.</li>
</ul>
</section>
<section id="code-snippet" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="code-snippet"><span class="header-section-number">12.5</span> Code Snippet</h2>
<section id="python" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="python"><span class="header-section-number">12.5.1</span> Python</h3>
<pre><code>
</code></pre>
</section>
<section id="numpy" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="numpy"><span class="header-section-number">12.5.2</span> Numpy</h3>
<pre><code>pcaUS.explained_variance_ratio_.cumsum()

U, D, V = np.linalg.svd(X, full_matrices=False)#note V is the V^T in the math formula

np.linalg.norm(V, 2, axis=1)

np.allclose(U.dot(np.diag(D)).dot(V)-X, 0)

r_idx = np.random.choice(np.arange(X.shape[0]),
                         n_omit,
                         replace=False)

Xbar = np.nanmean(Xhat, axis=0) # ignoring nan

ismiss = np.isnan(Xna)

np.corrcoef(Xapp[ismiss], X[ismiss])

np.random.standard_normal((50,2));

np.multiply.outer(np.ones(X.shape[0]), X[0]) #make X.shape[0] copies of the first row X[0]

#computing pairwise distance between rows of X:
D = np.zeros((X.shape[0], X.shape[0]));
for i in range(X.shape[0]):
    x_ = np.multiply.outer(np.ones(X.shape[0]), X[i]) # each row of X is one point (obs)
    D[i] = np.sqrt(np.sum((X - x_)**2, 1)); # D[i]: ith row of D. distance from the i-th row of X.
</code></pre>
</section>
<section id="pandas" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="pandas"><span class="header-section-number">12.5.3</span> Pandas</h3>
<pre><code>USArrests.mean() # column means
USArrests.var() # colmn variance

pd.crosstab(nci_labs['label'],
            pd.Series(comp_cut.reshape(-1), name='Complete'))
</code></pre>
</section>
<section id="graphics" class="level3" data-number="12.5.4">
<h3 data-number="12.5.4" class="anchored" data-anchor-id="graphics"><span class="header-section-number">12.5.4</span> Graphics</h3>
<pre><code>ax.set_xticks(ticks)


ax.axhline(140, c='r', linewidth=4);
</code></pre>
</section>
<section id="islp-and-statsmodels" class="level3" data-number="12.5.5">
<h3 data-number="12.5.5" class="anchored" data-anchor-id="islp-and-statsmodels"><span class="header-section-number">12.5.5</span> ISLP and statsmodels</h3>
<pre><code></code></pre>
</section>
<section id="sklearn" class="level3" data-number="12.5.6">
<h3 data-number="12.5.6" class="anchored" data-anchor-id="sklearn"><span class="header-section-number">12.5.6</span> sklearn</h3>
<section id="pca-1" class="level4" data-number="12.5.6.1">
<h4 data-number="12.5.6.1" class="anchored" data-anchor-id="pca-1"><span class="header-section-number">12.5.6.1</span> PCA</h4>
<pre><code>scaler = StandardScaler(with_std=True,
                        with_mean=True)
USArrests_scaled = scaler.fit_transform(USArrests)
pcaUS = PCA()
pcaUS.fit(USArrests_scaled)
pcaUS.mean_
scores = pcaUS.transform(USArrests_scaled)
pcaUS.components_  # loadings
scores.std(0, ddof=1) #axis=0, divided by N-ddof=N-1
pcaUS.explained_variance_
pcaUS.explained_variance_ratio_
pcaUS.n_components_

</code></pre>
</section>
<section id="pca-plot" class="level4" data-number="12.5.6.2">
<h4 data-number="12.5.6.2" class="anchored" data-anchor-id="pca-plot"><span class="header-section-number">12.5.6.2</span> PCA plot</h4>
<pre><code>fig, axes = plt.subplots(1, 2, figsize=(15,6))
ax = axes[0]
ax.scatter(nci_scores[:,0],
           nci_scores[:,1],
           c=nci_groups,
           marker='o',
           s=50)
ax.set_xlabel('PC1'); ax.set_ylabel('PC2')</code></pre>
</section>
<section id="pca-biplot" class="level4" data-number="12.5.6.3">
<h4 data-number="12.5.6.3" class="anchored" data-anchor-id="pca-biplot"><span class="header-section-number">12.5.6.3</span> PCA biplot</h4>
<pre><code>scale_arrow = s_ = 2
scores[:,1] *= -1      #flip the scores and loadings at the same time. 
pcaUS.components_[1] *= -1 # flip the y-axis
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
ax.scatter(scores[:,0], scores[:,1])
ax.set_xlabel('PC%d' % (i+1))
ax.set_ylabel('PC%d' % (j+1))
for k in range(pcaUS.components_.shape[1]):
    ax.arrow(0, 0, s_*pcaUS.components_[i,k], s_*pcaUS.components_[j,k])
    ax.text(s_*pcaUS.components_[i,k],
            s_*pcaUS.components_[j,k],
            USArrests.columns[k])</code></pre>
</section>
<section id="pca-plot-for-variance-explained" class="level4" data-number="12.5.6.4">
<h4 data-number="12.5.6.4" class="anchored" data-anchor-id="pca-plot-for-variance-explained"><span class="header-section-number">12.5.6.4</span> PCA plot for variance explained</h4>
<pre><code>fig, axes = plt.subplots(1, 2, figsize=(15, 6))
ticks = np.arange(pcaUS.n_components_)+1
ax = axes[0]
ax.plot(ticks,
        pcaUS.explained_variance_ratio_,
        marker='o')
ax.set_xlabel('Principal Component');
ax.set_ylabel('Proportion of Variance Explained')
ax.set_ylim([0,1])
ax.set_xticks(ticks)

ax = axes[1]
ax.plot(ticks,
        pcaUS.explained_variance_ratio_.cumsum(),
        marker='o')
ax.set_xlabel('Principal Component')
ax.set_ylabel('Cumulative Proportion of Variance Explained')
ax.set_ylim([0, 1])
ax.set_xticks(ticks)
fig</code></pre>
</section>
<section id="matrix-completion" class="level4" data-number="12.5.6.5">
<h4 data-number="12.5.6.5" class="anchored" data-anchor-id="matrix-completion"><span class="header-section-number">12.5.6.5</span> Matrix completion</h4>
<pre><code>def low_rank(X, M=1): #low rank approx to X
    U, D, V = np.linalg.svd(X)
    L = U[:,:M] * D[None,:M]
    return L.dot(V[:M])
    
Xhat = Xna.copy()
Xbar = np.nanmean(Xhat, axis=0) 
Xhat[r_idx, c_idx] = Xbar[c_idx]

thresh = 1e-7
rel_err = 1
count = 0
ismiss = np.isnan(Xna)#Xna: original matrix with nan; Xhat: matrix completed with imputed values
mssold = np.mean(Xhat[~ismiss]**2)
mss0 = np.mean(Xna[~ismiss]**2)

while rel_err &gt; thresh:
    count += 1
    # Step 2(a)
    Xapp = low_rank(Xhat, M=1)
    # Step 2(b)
    Xhat[ismiss] = Xapp[ismiss]
    # Step 2(c)
    mss = np.mean(((Xna - Xapp)[~ismiss])**2)
    rel_err = (mssold - mss) / mss0
    mssold = mss
    print("Iteration: {0}, MSS:{1:.3f}, Rel.Err {2:.2e}"
          .format(count, mss, rel_err))
          
np.corrcoef(Xapp[ismiss], X[ismiss])</code></pre>
</section>
</section>
<section id="k-clustering" class="level3" data-number="12.5.7">
<h3 data-number="12.5.7" class="anchored" data-anchor-id="k-clustering"><span class="header-section-number">12.5.7</span> K-Clustering</h3>
<pre><code>from sklearn.cluster import \
     (KMeans,
      AgglomerativeClustering)
      
kmeans = KMeans(n_clusters=2,
                random_state=2,
                n_init=20).fit(X)
                
kmeans.labels_  #cluster assignments                
                </code></pre>
</section>
<section id="hierarchical-clustering-1" class="level3" data-number="12.5.8">
<h3 data-number="12.5.8" class="anchored" data-anchor-id="hierarchical-clustering-1"><span class="header-section-number">12.5.8</span> Hierarchical Clustering</h3>
<pre><code>from sklearn.cluster import \
     (KMeans,
      AgglomerativeClustering)
from scipy.cluster.hierarchy import \
     (dendrogram,
      cut_tree)
from ISLP.cluster import compute_linkage

HClust = AgglomerativeClustering
hc_comp = HClust(distance_threshold=0, # same as None. min. distance to not merge
                 n_clusters=None,
                 linkage='complete')
hc_comp.fit(X)

hc_avg = HClust(distance_threshold=0,
                n_clusters=None,
                linkage='average');
hc_avg.fit(X)
hc_sing = HClust(distance_threshold=0,
                 n_clusters=None,
                 linkage='single');
hc_sing.fit(X);

cut_tree(linkage_comp, n_clusters=4).T #linkage_comp is the linkage-matrix represenation

cut_tree(linkage_comp, height=5).T
</code></pre>
<section id="use-precomputed-matrix-for-hierarchical-clustering" class="level4" data-number="12.5.8.1">
<h4 data-number="12.5.8.1" class="anchored" data-anchor-id="use-precomputed-matrix-for-hierarchical-clustering"><span class="header-section-number">12.5.8.1</span> Use precomputed matrix for hierarchical clustering</h4>
<pre><code>D = np.zeros((X.shape[0], X.shape[0]));
for i in range(X.shape[0]):
    x_ = np.multiply.outer(np.ones(X.shape[0]), X[i]) # each row of X is one point (obs)
    D[i] = np.sqrt(np.sum((X - x_)**2, 1)); # D[i]: ith row of D. distance from the i-th row of X.
    
hc_sing_pre = HClust(distance_threshold=0,
                     n_clusters=None,
                     metric='precomputed',
                     linkage='single')
hc_sing_pre.fit(D) # use precomputed distance matrix
    </code></pre>
</section>
<section id="plotting-dendrogram" class="level4" data-number="12.5.8.2">
<h4 data-number="12.5.8.2" class="anchored" data-anchor-id="plotting-dendrogram"><span class="header-section-number">12.5.8.2</span> Plotting dendrogram</h4>
<pre><code>cargs = {'color_threshold':-np.inf, # not using the default coloring method
         'above_threshold_color':'black'}
linkage_comp = compute_linkage(hc_comp) # compute teh linkage-matrix representation
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp,
           ax=ax,
           **cargs);
# coloring           
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp,
           ax=ax,
           color_threshold=4,
           above_threshold_color='black');


dendrogram(linkage_pca,
           labels=np.asarray(nci_labs),
           leaf_font_size=10,
           ax=ax,
           **cargs)</code></pre>
</section>
<section id="a-function-to-plot-dendrogram" class="level4" data-number="12.5.8.3">
<h4 data-number="12.5.8.3" class="anchored" data-anchor-id="a-function-to-plot-dendrogram"><span class="header-section-number">12.5.8.3</span> a function to plot dendrogram</h4>
<pre><code>def plot_nci(linkage, ax, cut=-np.inf):
    cargs = {'above_threshold_color':'black',
             'color_threshold':cut}
    hc = HClust(n_clusters=None,
                distance_threshold=0,
                linkage=linkage.lower()).fit(nci_scaled)
    linkage_ = compute_linkage(hc)
    dendrogram(linkage_,
               ax=ax,
               labels=np.asarray(nci_labs),
               leaf_font_size=10,
               **cargs)
    ax.set_title('%s Linkage' % linkage)
    return hc
    
plot_nci('Complete', ax, cut=140)
ax.axhline(140, c='r', linewidth=4);
    </code></pre>
</section>
<section id="use-scaled-features-for-hierarchical-clustering" class="level4" data-number="12.5.8.4">
<h4 data-number="12.5.8.4" class="anchored" data-anchor-id="use-scaled-features-for-hierarchical-clustering"><span class="header-section-number">12.5.8.4</span> Use scaled features for hierarchical clustering</h4>
<pre><code>scaler = StandardScaler()
X_scale = scaler.fit_transform(X)
hc_comp_scale = HClust(distance_threshold=0,
                       n_clusters=None,
                       linkage='complete').fit(X_scale)
linkage_comp_scale = compute_linkage(hc_comp_scale)
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp_scale, ax=ax, **cargs)
ax.set_title("Hierarchical Clustering with Scaled Features");
</code></pre>
</section>
</section>
<section id="useful-code-snippets" class="level3" data-number="12.5.9">
<h3 data-number="12.5.9" class="anchored" data-anchor-id="useful-code-snippets"><span class="header-section-number">12.5.9</span> Useful code snippets</h3>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch11.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chapter 11: Survival Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./class_project.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Class Project</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>