<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.398">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stat4500notes - 6&nbsp; Chapter 6: Linear Model Selection and Regrularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch7.html" rel="next">
<link href="./ch5.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">stat4500notes</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting up Python Computing Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Moving Beyond Linearity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Class Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#best-subset-selecttion" id="toc-best-subset-selecttion" class="nav-link active" data-scroll-target="#best-subset-selecttion"><span class="header-section-number">6.1</span> Best Subset Selecttion</a></li>
  <li><a href="#stepwise-selection" id="toc-stepwise-selection" class="nav-link" data-scroll-target="#stepwise-selection"><span class="header-section-number">6.2</span> Stepwise selection</a>
  <ul class="collapse">
  <li><a href="#forward-stepwise-selection" id="toc-forward-stepwise-selection" class="nav-link" data-scroll-target="#forward-stepwise-selection"><span class="header-section-number">6.2.1</span> Forward Stepwise Selection</a></li>
  <li><a href="#backward-stepwise-selection" id="toc-backward-stepwise-selection" class="nav-link" data-scroll-target="#backward-stepwise-selection"><span class="header-section-number">6.2.2</span> Backward Stepwise Selection</a></li>
  </ul></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">6.3</span> Model selection</a></li>
  <li><a href="#shrinkage-methods-for-variable-selection" id="toc-shrinkage-methods-for-variable-selection" class="nav-link" data-scroll-target="#shrinkage-methods-for-variable-selection"><span class="header-section-number">6.4</span> Shrinkage methods for Variable selection</a>
  <ul class="collapse">
  <li><a href="#ridge-regression-minimize-the-following-objective" id="toc-ridge-regression-minimize-the-following-objective" class="nav-link" data-scroll-target="#ridge-regression-minimize-the-following-objective"><span class="header-section-number">6.4.1</span> Ridge regression: minimize the following objective</a></li>
  <li><a href="#the-lasso-least-absolute-shrinkage-and-selection-operator" id="toc-the-lasso-least-absolute-shrinkage-and-selection-operator" class="nav-link" data-scroll-target="#the-lasso-least-absolute-shrinkage-and-selection-operator"><span class="header-section-number">6.4.2</span> The Lasso (Least Absolute Shrinkage and Selection Operator)</a></li>
  </ul></li>
  <li><a href="#dimension-reduction-methods-transforming-x_j." id="toc-dimension-reduction-methods-transforming-x_j." class="nav-link" data-scroll-target="#dimension-reduction-methods-transforming-x_j."><span class="header-section-number">6.5</span> Dimension reduction methods: transforming <span class="math inline">\(X_j\)</span>.</a>
  <ul class="collapse">
  <li><a href="#pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" id="toc-pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" class="nav-link" data-scroll-target="#pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors"><span class="header-section-number">6.5.1</span> PCA regression: first use PCA to obtain <span class="math inline">\(M\)</span>- PCA as linear combinations (directions) of the original <span class="math inline">\(p\)</span> predictors:</a></li>
  <li><a href="#partial-least-squares" id="toc-partial-least-squares" class="nav-link" data-scroll-target="#partial-least-squares"><span class="header-section-number">6.5.2</span> Partial Least Squares</a></li>
  </ul></li>
  <li><a href="#homework" id="toc-homework" class="nav-link" data-scroll-target="#homework"><span class="header-section-number">6.6</span> Homework:</a></li>
  <li><a href="#code-snippet" id="toc-code-snippet" class="nav-link" data-scroll-target="#code-snippet"><span class="header-section-number">6.7</span> Code Snippet</a>
  <ul class="collapse">
  <li><a href="#python" id="toc-python" class="nav-link" data-scroll-target="#python"><span class="header-section-number">6.7.1</span> Python</a></li>
  <li><a href="#numpy" id="toc-numpy" class="nav-link" data-scroll-target="#numpy"><span class="header-section-number">6.7.2</span> Numpy</a></li>
  <li><a href="#pandas" id="toc-pandas" class="nav-link" data-scroll-target="#pandas"><span class="header-section-number">6.7.3</span> Pandas</a></li>
  <li><a href="#graphics" id="toc-graphics" class="nav-link" data-scroll-target="#graphics"><span class="header-section-number">6.7.4</span> Graphics</a></li>
  <li><a href="#islp-and-statsmodels" id="toc-islp-and-statsmodels" class="nav-link" data-scroll-target="#islp-and-statsmodels"><span class="header-section-number">6.7.5</span> ISLP and statsmodels</a></li>
  <li><a href="#sklearn" id="toc-sklearn" class="nav-link" data-scroll-target="#sklearn"><span class="header-section-number">6.7.6</span> sklearn</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Linear models are <em>interpretable</em> and often shows small variance. They are fitted by <em>OLS</em>. There are other methods that can either provide alternatives or improve linear regression models in terms of <em>prediction accuracy</em> (especially when <span class="math inline">\(p&gt;n\)</span>) and automatic <em>feature selection</em> for improved interpretability.</p>
<p>There are three classes of methods:</p>
<ul>
<li>subset selection: pick a subset of the <span class="math inline">\(p\)</span> predictors that best explains the response.</li>
<li>Shrinkage (regularization). With an added regularizing term, estimated parameters are shrunken to zero relative the OLS estimates. If <span class="math inline">\(L^2\)</span> regularization is used, all coefficients are shrunk toward zero by the same proportion; while if <span class="math inline">\(L^1\)</span> is used, then some coefficients will become zero, leading to actual <em>variable selection</em> or <em>sparse representation</em>.</li>
<li>Dimension reduction. Project <span class="math inline">\(p\)</span>-predictors to a <span class="math inline">\(M\)</span> (<span class="math inline">\(M&lt;p\)</span>) dimensional subspace. Each new direction is a linear combination (or projection) of the <span class="math inline">\(p\)</span>-variables. These <span class="math inline">\(M\)</span>-projections can then be used to fit a linear regression model(<strong>PCR</strong> if the <span class="math inline">\(M\)</span>-projections are obtained in an unsupervised way; or <strong>PLR</strong> if these <span class="math inline">\(M\)</span>-projections are obtained in a supervised way))</li>
</ul>
<section id="best-subset-selecttion" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="best-subset-selecttion"><span class="header-section-number">6.1</span> Best Subset Selecttion</h2>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>null model</em> <span class="math inline">\(\mathcal{M}_0\)</span>, which contains no predictors. This model simply set <span class="math inline">\(Y=\text{mean}(y_i)\)</span>.</li>
<li>for <span class="math inline">\(k=1, 2, \cdots, p\)</span>: fit <span class="math inline">\(p \choose k\)</span> models containing exactly <span class="math inline">\(k\)</span> predictors. Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> (or <em>deviance</em> for classification problem, i.e., <span class="math inline">\(-2\max \log (\text{likelihood})\)</span> on the <em>training set</em>, called <span class="math inline">\(\mathcal{M}_k\)</span>. Note for each categorical variable with <span class="math inline">\(L\)</span>-level, there are <span class="math inline">\(L-1\)</span> dummy variables.</li>
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>. If cross-validation is used, then Step 2 is repeated on each training fold, and the validation errors are averaged to select the best <span class="math inline">\(k\)</span>. The the model <span class="math inline">\(\mathcal{M}_k\)</span> fit on the full training set is delivered for chosen <span class="math inline">\(k\)</span>.</li>
</ol>
<p>Best subset selection suffers - high computation: needs to compute <span class="math inline">\(2^p\)</span> models - overfitting due to the large search space of models</p>
</section>
<section id="stepwise-selection" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="stepwise-selection"><span class="header-section-number">6.2</span> Stepwise selection</h2>
<p>Both Forward and Backward selection are stepwise selection. They are used when <span class="math inline">\(p\)</span> is large. They searches over <span class="math inline">\(1+p(p+1)/2\)</span> models and are <em>greedy</em> algorithm and is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models.</p>
<ul>
<li>Backward selection requires <span class="math inline">\(n&gt;p\)</span> (so that the full model can be fit);</li>
<li>Forward selection can be used when <span class="math inline">\(n&lt;p\)</span> but only fits up to models with <span class="math inline">\(n\)</span> variables.</li>
<li>One can combine forward and backward selection to a hybrid selection.</li>
</ul>
<section id="forward-stepwise-selection" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="forward-stepwise-selection"><span class="header-section-number">6.2.1</span> Forward Stepwise Selection</h3>
<p>Adding one variable at at time that offers the greatest additional improvement.</p>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>null model</em> <span class="math inline">\(\mathcal{M}_0\)</span>, which contains no predictors. This model simply set <span class="math inline">\(Y=\text{mean}(y_i)\)</span>.</li>
<li>for <span class="math inline">\(k=1, 2, \cdots, p-1\)</span>:</li>
</ol>
<ul>
<li>fit all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor.</li>
<li>Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> on the training set, called <span class="math inline">\(\mathcal{M}_{k+1}\)</span>.</li>
</ul>
<ol start="3" type="1">
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
<section id="backward-stepwise-selection" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="backward-stepwise-selection"><span class="header-section-number">6.2.2</span> Backward Stepwise Selection</h3>
<p>It begins with the full model with all variables, and iteratively removing one variable at at time.</p>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>full model</em> <span class="math inline">\(\mathcal{M}_p\)</span>, which contains all predictors.</li>
<li>for <span class="math inline">\(k=p, p-1, \cdots, 1\)</span>:</li>
</ol>
<ul>
<li>fit all <span class="math inline">\(k\)</span> models that contains all but one of the predictors in <span class="math inline">\(\mathcal{M}_k\)</span>.</li>
<li>Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> on the training set, called <span class="math inline">\(\mathcal{M}_{k-1}\)</span>.</li>
</ul>
<ol start="3" type="1">
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
</section>
<section id="model-selection" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">6.3</span> Model selection</h2>
<p>Models with all predictors always have the smallest <span class="math inline">\(RSS\)</span> or largest <span class="math inline">\(R^2\)</span> on the <em>training set</em>. Therefore they are not suitable to choose the best one among models with different number of predictors.</p>
<p>We ought to <em>estimate the test error</em> on a test set. This may be done - indirectly by adjusting the training error to account for the bias due to overfitting: <span class="math inline">\(C_p\)</span> (equivalently, AIC in case of linear model with Gaussian errors), BIC and adjusted <span class="math inline">\(R^2\)</span>.</p>
<ul>
<li><p>Mallow’s <span class="math inline">\(C_p\)</span>: <span class="math display">\[
  C_p =\frac{1}{n}(RSS+2d\hat{\sigma}^2)
  \]</span> where, <span class="math inline">\(d\)</span> us the number of parameters and <span class="math inline">\(\hat{\sigma}^2 \approx Var{\epsilon}\)</span>, typically estimated by using the <em>full model</em> containing all variables. <span class="math inline">\(C_p\)</span> is an unbiased estimate of test MSE.</p></li>
<li><p>AIC is defined for models fit by maximmum likelihood. <span class="math display">\[
  AIC = -2\log L + 2d
  \]</span> where, <span class="math inline">\(L\)</span> is the maximum likelihood function for the estimated model. For linear regression with Gaussian error, <span class="math inline">\(AIC\propto C_p\)</span>.</p></li>
<li><p>BIC <span class="math display">\[
  BIC = \frac{1}{n}( RSS + \log (n)d \hat{\sigma}^2 )
  \]</span> Since <span class="math inline">\(\log n&gt;\)</span> for <span class="math inline">\(n&gt;7\)</span>, the BIC places a higher penalty on models with many variables, and hence select smaller models than <span class="math inline">\(C_p\)</span>.</p></li>
<li><p>Adjusted <span class="math inline">\(R^2\)</span> (larger value is better)</p>
<p><span class="math display">\[
\text{Adjusted }R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</span> <span class="math inline">\(RSS/(n-d-1)\)</span> may increase or decrease depends on <span class="math inline">\(d\)</span>. Unlike <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span> pays a price for the inclusion of unnecessary variables in a model.</p>
<p><span class="math inline">\(C_p\)</span>, AIC, BIC, adjusted <span class="math inline">\(R^2\)</span>,are not appropriate in high-dimentional setting, as the estimated <span class="math inline">\(\hat{\sigma}^2 \approx 0\)</span> (when <span class="math inline">\(p\ge n\)</span>).</p></li>
<li><p>directly by cross-validation (or validation). It does not require estimate <span class="math inline">\(\sigma^2\)</span>. It has a wide range of usage, as it may difficult to estimate <span class="math inline">\(d\)</span> or <span class="math inline">\(\sigma^2\)</span>. One can choose the model that has the smallest test error or using the <em>one-standard-error rule</em> to select the model that has a smaller size:</p>
<ul>
<li>calculate the SE of the estimated test MSE for each model size.</li>
<li>identify the lowest test MSE</li>
<li>choose the smallest model for which its test MSE is within one SE of the lowest point.</li>
</ul></li>
</ul>
</section>
<section id="shrinkage-methods-for-variable-selection" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="shrinkage-methods-for-variable-selection"><span class="header-section-number">6.4</span> Shrinkage methods for Variable selection</h2>
<p>The shrinkage offers an alternative to selecting variables by adjusting a hyperparameter that trades-off RSS and the model parameter magnitudes. Shrinkage methods will produce a different set of coefficients for a different <span class="math inline">\(\lambda\)</span>. Cross-validation may be used to select the best <span class="math inline">\(\lambda\)</span>. After the <span class="math inline">\(\lambda\)</span> is selected, one can fit a final model using the entire training data set.</p>
<p>The reason shrinkage methods may perform better than OLS is rooted in bias-variance trade-off: as <span class="math inline">\(\lambda\)</span> increases, the flexibility of the model decreases b ecause of shrunk coefficients, leading to decreased variance but increased bias.</p>
<section id="ridge-regression-minimize-the-following-objective" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="ridge-regression-minimize-the-following-objective"><span class="header-section-number">6.4.1</span> Ridge regression: minimize the following objective</h3>
<p><span class="math display">\[
RSS + \lambda \sum_{j=1}^p \beta_j^2
\]</span> The ridge regression is equivalent to <span class="math display">\[
\text{minimize } RSS \qquad \text{subject to } \sum_{j=1}^p \beta_j^2 \le s
\]</span> for some <span class="math inline">\(s\ge 0\)</span>.</p>
<ul>
<li><p>It encourages the model parameters to shrink toward zero and find a balance between RSS and model parameter magnitudes. Cross-validation is used to find the best tuning parameter <span class="math inline">\(\lambda\)</span>. When <span class="math inline">\(\lambda\)</span> is large, <span class="math inline">\(\beta_j\to 0\)</span>. Note, Ridge shrinks all coefficients and include all <span class="math inline">\(p\)</span> variables.</p></li>
<li><p>The OLS coefficients estimates are <em>scale equivariant</em>: regardless of how <span class="math inline">\(X_j\)</span> is scaled, <span class="math inline">\(X_j\hat{\beta}_j\)</span> remain the same: if <span class="math inline">\(X_j\)</span> is multiplied by <span class="math inline">\(c\)</span>, this will simply leads to <span class="math inline">\(\hat{\beta}_j\)</span> be scaled by a factor of <span class="math inline">\(1/c\)</span>.</p></li>
<li><p>In contrast, when multiplying <span class="math inline">\(X_j\)</span> by a factor, this may significantly change the ridge coefficients. Ridge coefficients depends on <span class="math inline">\(\lambda\)</span> and the scale of <span class="math inline">\(X_j\)</span>, and may even on the scaling of other predictors. Therefore, it is best practice to <em>standardize the predictors</em> before fitting a ridge model: <span class="math display">\[
\tilde{x}_{ij} =\frac{x_{ij}}{\frac{1}{n} \sum_{i=1}^n(x_{ij} - \bar{x}_j)}
\]</span></p></li>
<li><p>Ridge regression works best in situations where the OLS estimates have high variance, especially when <span class="math inline">\(p\)</span> is large.</p></li>
<li><p>Ridge will include all <span class="math inline">\(p\)</span> variables in the final model.</p></li>
</ul>
</section>
<section id="the-lasso-least-absolute-shrinkage-and-selection-operator" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="the-lasso-least-absolute-shrinkage-and-selection-operator"><span class="header-section-number">6.4.2</span> The Lasso (Least Absolute Shrinkage and Selection Operator)</h3>
<ul>
<li><p>The Lasso replaces the <span class="math inline">\(\ell^2\)</span> error with <span class="math inline">\(\ell^1\)</span> penalty. Lasso can force some coefficients to become exactly zero when <span class="math inline">\(\lambda\)</span> is large enough. Thus it can actually performs <em>variable selection</em> hence better interpretation. Again, cross-validation is employed to select <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>The reason Lasso can perform variable selection is because the objective function is equivalent to</p></li>
</ul>
<p><span class="math display">\[\text {minimizing RSS}, \text{subject to } \sum_{j=1}^p |\beta_j| \le s
  \]</span> for some <span class="math inline">\(s\)</span>. The contour of RSS in general only touch the <span class="math inline">\(\ell_1\)</span> ball at its vertex, at which a minimum is obtained with some variables vanishes. In contrast, in the ridge situation, the <span class="math inline">\(\ell_2\)</span> ball is round, and in general, the contour of the RSS function only touches the sphere at a surface point where a minimum is obtained with no variable vanishes.</p>
<ul>
<li><p>Neither ridge nor the lasso will universally dominate the other. When the response depends on a small number of predictors, one may expect lasso performs better; but in practice, this is never known in advance.</p></li>
<li><p>Combining ridge and lasso leads to <em>elastic net</em> method.</p></li>
<li><p>it is well known that ridge tends to give similar values coefficient values to correlated variables, while lasso may give quite different coefficient values to correlated variables.</p></li>
<li><p>ridge regression shrinks all coefficients by the same proportion. While lasso perform <strong>soft-thresholding</strong>, shrink all coefficients by similar amount, and sufficient small coefficients are shrunk all the way to zero.</p>
<p>both ridge and lasso can be considered as computationally feasible approximation to the <em>best subset selection</em> which can be equivalently formulated as: <span class="math display">\[
\text{minimize } RSS \qquad \text{subject to } \sum_{j=1}^p I(\beta_j\ne 0) \le s.
\]</span></p></li>
<li><p><strong>Bayesian formulation</strong>: Both ridge and lasso can be interpreted as maximize the <strong>posterior probability</strong> (MAP) <span class="math display">\[
p(\beta|X,Y)\propto f(Y|X,\beta) p(\beta|X)=f(Y|X,\beta)p(\beta)
\]</span> where <span class="math inline">\(p(\beta)= \prod_{j=1}^p g(\beta_j)\)</span> with some density function <span class="math inline">\(g\)</span> is the believed prior on <span class="math inline">\(\beta\)</span>.</p>
<ul>
<li>if <span class="math inline">\(g\)</span> is Gaussian with mean zero and standard deviation a function of <span class="math inline">\(\lambda\)</span>, then it follows the solution <span class="math inline">\(\beta\)</span> given by the ridge is the same as maximizing the posterior <span class="math inline">\(p(\beta|X,Y)\)</span>, that is, <span class="math inline">\(\beta\)</span> is the posterior mode. In fact, <span class="math inline">\(\beta\)</span> is also the posterior mean. Since the Gaussian prior is flat at near zero, ridge assumes the coefficients are randomly distributed about zero.</li>
<li>if <span class="math inline">\(g\)</span> is double-exponential (Laplace) with mean zero and scale parameter a function of <span class="math inline">\(\lambda\)</span>, then it follows the solution <span class="math inline">\(\beta\)</span> given by the lasso is the same as maximizing the posterior <span class="math inline">\(p(\beta|X,Y)\)</span>, that is, <span class="math inline">\(\beta\)</span> is the posterior mode. In this case <span class="math inline">\(\beta\)</span> is <strong>not</strong> the posterior mean. Since the Laplacian prior is steeply peaked at zero, lasso expects a priori that many coefficients are (exactly) zero.</li>
</ul></li>
</ul>
</section>
</section>
<section id="dimension-reduction-methods-transforming-x_j." class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="dimension-reduction-methods-transforming-x_j."><span class="header-section-number">6.5</span> Dimension reduction methods: transforming <span class="math inline">\(X_j\)</span>.</h2>
<p>There are two types of dimension reduction methods for regression: a) PCA regression, b) Partial list squares PLS. Both are designed to handle when the OLS breaks down due to that there are large number of correlated variables.</p>
<section id="pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors"><span class="header-section-number">6.5.1</span> PCA regression: first use PCA to obtain <span class="math inline">\(M\)</span>- PCA as linear combinations (directions) of the original <span class="math inline">\(p\)</span> predictors:</h3>
<p><span id="eq-PCA"><span class="math display">\[
  Z_m =\sum_{j=1}^p \phi_{mj} X_j, \qquad 1\le m \le M,  
   \tag{6.1}\]</span></span> where, the <span class="math inline">\(\phi_{mj}\)</span> are called <strong>PCA loadings</strong>, and subject to the norm <span class="math inline">\(\sum_{j=1}^p\phi_{mj}^2=1\)</span> for each <span class="math inline">\(m\)</span>. Note <span class="math inline">\(Z_m\)</span> is a vector of length equal to the length of <span class="math inline">\(X_j\)</span>, which is the number of data points <span class="math inline">\(n\)</span>. The component of <span class="math inline">\(Z_m\)</span>: <span class="math inline">\(z_{im}\)</span>, <span class="math inline">\(1\le i \le n\)</span> are called <strong>PCA scores</strong>. <span class="math inline">\(z_{im}\)</span> is a <em>single number summary</em> of the <span class="math inline">\(p\)</span> predictors with the <span class="math inline">\(m\)</span>-th PCA for the <span class="math inline">\(i\)</span>-th observation. PCA is not a feature selection method.</p>
<p>The first PCA defines the direction that contains the largest variance in <span class="math inline">\(X\)</span>, and minimize the sum of squared perpendicular distances to each point (the projection error on the PCA), that is it defines the line that is <em>as close as possible</em> to the data; (In fact, the first PCA is given by the eigenvector of the largest eigenvalue of the covariance matrix <span class="math inline">\(\frac{1}{n-1}X^TX\)</span>). The second PCA is orthogonal to the first PCA and has the second largest variance and is uncorrelated with the first, and so on. These directions are obtained in an <em>unsupervised way</em>, as <span class="math inline">\(Y\)</span> is not used to obtain these components. Consequently, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.</p>
<p>PCA is typically conducted after standardizing the data <span class="math inline">\(X\)</span>, as without scaling, the high variance variables will tend to have higher influence on the obtained PCAs.</p>
<p>We then use OLS to fit a linear regression model <span id="eq-PCAR"><span class="math display">\[
y_i =\theta_0 +\sum_{m=1}^M \theta_m z_{im}+\epsilon_i, \qquad i=1,2,\cdots, n
\tag{6.2}\]</span></span></p>
<p>After substitute <a href="#eq-PCA" class="quarto-xref">Equation&nbsp;<span>6.1</span></a> into equation <a href="#eq-PCAR" class="quarto-xref">Equation&nbsp;<span>6.2</span></a>, one can find that <span class="math display">\[
\sum_{m=1}^M \theta_mz_{im} =\sum_{j=1}^p \beta_jx_{ij}
\]</span> with <span id="eq-pcar-beta"><span class="math display">\[
\beta_j = \sum_{m=1}^M \theta_m\phi_{mj}.
\tag{6.3}\]</span></span> Eq. <a href="#eq-pcar-beta" class="quarto-xref">Equation&nbsp;<span>6.3</span></a> has the potential to bias the coefficient estimates, but selecting <span class="math inline">\(M&lt;&lt; p\)</span> can significantly reduce the variance. So model <a href="#eq-PCAR" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> is a special case of linear regression subject to the constants <a href="#eq-pcar-beta" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>.</p>
<p>PCR and ridge are closely related and one can think of ridge regression as a continuous version of PCR.</p>
</section>
<section id="partial-least-squares" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="partial-least-squares"><span class="header-section-number">6.5.2</span> Partial Least Squares</h3>
<p>Similar to PCAR, PLS also first identifies a new set of features <span class="math inline">\(Z_1, Z_2, \cdots, Z_m\)</span>, each of which is a linear combinations of the original features, and then fits a linear model via OLS with these new <span class="math inline">\(M\)</span> features.</p>
<p>But PLS identifies these new features in a <em>supervised way</em>, that is, PLS uses <span class="math inline">\(Y\)</span> in order to identify the new features that not only approximate the old features well, but also are <em>related to the response</em>, i.e., these new features explain both the response and the predictors.</p>
<p>First PLS standardizes the <span class="math inline">\(p\)</span> predictors. PLS identifies the first component <span class="math inline">\(Z_1 = \sum_{j=1}^p \phi_{1j}X_j\)</span> by choosing <span class="math inline">\(\phi_{1j}=&lt;X_j, Y&gt;\)</span>, the coefficient from the simple linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X_j\)</span>. Since this coefficient is equal to the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span>, PLS places the highest weight on the variables that are most strongly related to <span class="math inline">\(Y\)</span>. The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response.</p>
<p>Next, PLS orthogonality each <span class="math inline">\(X_j\)</span> with respect to <span class="math inline">\(Z_1\)</span>, that is, replace each <span class="math inline">\(X_j\)</span> with the residual by regressing <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Z_1\)</span>, and then repeat the same process.</p>
<p>When <span class="math inline">\(p\)</span> is large, especially <span class="math inline">\(p&gt;n\)</span>, the forward selection method, shrinkage methods (lasso or ridge), PCR, PLR fit a <em>less flexible</em> model, hence particularly useful in performing regression in high-dimensional settings.</p>
</section>
</section>
<section id="homework" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="homework"><span class="header-section-number">6.6</span> Homework:</h2>
<ul>
<li>Conceptual: 1–4</li>
<li>Applied: At least one.</li>
</ul>
</section>
<section id="code-snippet" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="code-snippet"><span class="header-section-number">6.7</span> Code Snippet</h2>
<section id="python" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="python"><span class="header-section-number">6.7.1</span> Python</h3>
<pre><code>np.isnan(Hitters['Salary']).sum()
</code></pre>
</section>
<section id="numpy" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="numpy"><span class="header-section-number">6.7.2</span> Numpy</h3>
<pre><code>np.linalg.norm(beta_hat) #L2 norm. ord=1: L1  ord='inf': max norm.</code></pre>
</section>
<section id="pandas" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="pandas"><span class="header-section-number">6.7.3</span> Pandas</h3>
<pre><code>Hitters.dropna();
soln_path = pd.DataFrame(soln_array.T,
                         columns=D.columns,
                         index=-np.log(lambdas))
soln_path.index.name = 'negative log(lambda)'</code></pre>
</section>
<section id="graphics" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="graphics"><span class="header-section-number">6.7.4</span> Graphics</h3>
<pre><code>ax.errorbar(np.arange(n_steps), 
            cv_mse.mean(1), #mean of each row (model)
            cv_mse.std(1) / np.sqrt(K), #estimate standard error of the mean
            label='Cross-validated',
            c='r') # color red
            
ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--') # plot a verticalline</code></pre>
</section>
<section id="islp-and-statsmodels" class="level3" data-number="6.7.5">
<h3 data-number="6.7.5" class="anchored" data-anchor-id="islp-and-statsmodels"><span class="header-section-number">6.7.5</span> ISLP and statsmodels</h3>
<pre><code>#Estimate Var(epsilon)

design = MS(Hitters.columns.drop('Salary')).fit(Hitters)
design.terms # to see the variable names in the design matrix
Y = np.array(Hitters['Salary'])
X = design.transform(Hitters)
sigma2 = OLS(Y,X).fit().scale  #.scale: RSE: residual standard error estimating 

# Forward Selection using ISLP.models and a scoring function
from ISLP.models import \
     (Stepwise,
      sklearn_selected,
      sklearn_selection_path)
strategy = Stepwise.first_peak(design,
                               direction='forward',
                               max_terms=len(design.terms))
hitters_Cp = sklearn_selected(OLS,
                               strategy,
                               scoring=neg_Cp)
                               #default scoring MSE, will choose all variables
hitters_Cp.fit(Hitters, Y) # the same as hitters_Cp.fit(Hitters.drop('Salary', axis=1), Y)
hitters_Cp.selected_state_

#Forward selection using cross-validation
strategy = Stepwise.fixed_steps(design,
                                len(design.terms),
                                direction='forward')
full_path = sklearn_selection_path(OLS, strategy) #using default scoring MSE
full_path.fit(Hitters, Y) # there are , 19 variables, 20 models
Yhat_in = full_path.predict(Hitters)

#calculate in-sample mse

mse_fig, ax = subplots(figsize=(8,8))
insample_mse = ((Yhat_in - Y[:,None])**2).mean(0) #Y[:,None]: add a second axis, create a column vector
                        #[yw] mean(0): calculate mean along row, i.e., for each col. mean(1): calculate mean for each row

#Cross-validation
K = 5
kfold = skm.KFold(K,
                  random_state=0,
                  shuffle=True)
Yhat_cv = skm.cross_val_predict(full_path,
                                Hitters,
                                Y,
                                cv=kfold)
# Cross-validation mse
cv_mse = []
for train_idx, test_idx in kfold.split(Y):
    errors = (Yhat_cv[test_idx] - Y[test_idx,None])**2
    cv_mse.append(errors.mean(0)) # column means
cv_mse = np.array(cv_mse).T

#validation approach using ShuffleSplit
validation = skm.ShuffleSplit(n_splits=1, # only split one time. 
                              test_size=0.2,
                              random_state=0)
for train_idx, test_idx in validation.split(Y):
    full_path.fit(Hitters.iloc[train_idx], #note needing to use iloc
                  Y[train_idx])
    Yhat_val = full_path.predict(Hitters.iloc[test_idx])
    errors = (Yhat_val - Y[test_idx,None])**2
    validation_mse = errors.mean(0)
</code></pre>
</section>
<section id="sklearn" class="level3" data-number="6.7.6">
<h3 data-number="6.7.6" class="anchored" data-anchor-id="sklearn"><span class="header-section-number">6.7.6</span> sklearn</h3>
<pre><code>rom sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression

#Best subset selection using 10bnb

D = design.fit_transform(Hitters)
D = D.drop('intercept', axis=1) #needs to drop intercept
X = np.asarray(D)
path = fit_path(X, 
                Y,
                max_nonzeros=X.shape[1]) #fit_path: a funciton from l0nb. use all variables
                # max_nonzeros: max nonzero coefficients in the fitted model.

# Ridge Regression
soln_array = skl.ElasticNet.path(Xs, # standardized, no intercept
                                 Y,
                                 l1_ratio=0., #ridge
                                 alphas=lambdas)
# Using pipline
ridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)
scaler = StandardScaler(with_mean=True,  with_std=True)
pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])
pipe.fit(X, Y)
ridge.coef_

# Validation

validation = skm.ShuffleSplit(n_splits=1,
                              test_size=0.5,
                              random_state=0) # validation is a generator
ridge.alpha = 0.01
results = skm.cross_validate(ridge,
                             X,
                             Y,
                             scoring='neg_mean_squared_error',
                             cv=validation) # using the strategy defined in validation
-results['test_score']

# GridSearchCV()
param_grid = {'ridge__alpha': lambdas}
grid = skm.GridSearchCV(pipe,
                        param_grid,
                        cv=validation, # or use cv=kfold (5-fold CV defined separately)
                        scoring='neg_mean_squared_error') #default scoring=R^2
grid.fit(X, Y)
grid.best_params_['ridge__alpha']
grid.best_estimator_
grid.cv_results_['mean_test_score']
grid.cv_results_['std_test_score']


#Plot CV MSE
ridge_fig, ax = subplots(figsize=(8,8))
ax.errorbar(-np.log(lambdas),
            -grid.cv_results_['mean_test_score'],
            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))
ax.set_ylim([50000,250000])
ax.set_xlabel('$-\log(\lambda)$', fontsize=20)
ax.set_ylabel('Cross-validated MSE', fontsize=20);

# Use ElasticNetCV()
ridgeCV = skl.ElasticNetCV(alphas=lambdas, # ElasticNetCV accepts a sequence of alphas
                           l1_ratio=0,
                           cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler), # scaling is done once. 
                         ('ridge', ridgeCV)])
pipeCV.fit(X, Y)
tuned_ridge = pipeCV.named_steps['ridge']
tuned_ridge.mse_path_
tuned_ridge.alpha_ # best alpha
tuned_ridge.coef_

# Evaluating test Error of Cross-validated Ridge 

outer_valid = skm.ShuffleSplit(n_splits=1, 
                               test_size=0.25,
                               random_state=1)
inner_cv = skm.KFold(n_splits=5,
                     shuffle=True,
                     random_state=2)
ridgeCV = skl.ElasticNetCV(alphas=lambdas, # a sequence of lambdas
                           l1_ratio=0,
                           cv=inner_cv) # K-fold validation
pipeCV = Pipeline(steps=[('scaler', scaler),
                         ('ridge', ridgeCV)]);
                         
                         
results = skm.cross_validate(pipeCV, 
                             X,
                             Y,
                             cv=outer_valid,
                             scoring='neg_mean_squared_error')
-results['test_score']

# Lasso regression
lassoCV = skl.ElasticNetCV(n_alphas=100, #test 100 alpha values
                           l1_ratio=1,
                           cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler),
                         ('lasso', lassoCV)])
pipeCV.fit(X, Y)
tuned_lasso = pipeCV.named_steps['lasso']
tuned_lasso.alpha_
tuned_lasso.coef_
np.min(tuned_lasso.mse_path_.mean(1)) # miminum avg mse

#to get the soln path
lambdas, soln_array = skl.Lasso.path(Xs, # standarsized, no -intercept
                                    Y,
                                    l1_ratio=1,
                                    n_alphas=100)[:2]

#PCA and PCR
pca = PCA(n_components=2)
linreg = skl.LinearRegression()
pipe = Pipeline([('scaler', scaler), 
                 ('pca', pca),
                 ('linreg', linreg)])
pipe.fit(X, Y)
pipe.named_steps['linreg'].coef_
pipe.named_steps['pca'].explained_variance_ratio_

# perform Grid search
param_grid = {'pca__n_components': range(1, 20)} #PCA needs n_components &gt;0
grid = skm.GridSearchCV(pipe,
                        param_grid,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
grid.fit(X, Y)

# cross-validation a null model
cv_null = skm.cross_validate(linreg,
                             Xn,
                             Y,
                             cv=kfold,
                             scoring='neg_mean_squared_error')
-cv_null['test_score'].mean()

#PLS
pls = PLSRegression(n_components=2, 
                    scale=True) # standarsize the data 
pls.fit(X, Y) # X has no-intercept 

# Cross-validation
param_grid = {'n_components':range(1, 20)}
grid = skm.GridSearchCV(pls,
                        param_grid,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
grid.fit(X, Y)

</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch5.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch7.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 7: Moving Beyond Linearity</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>