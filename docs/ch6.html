<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.398">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stat4500notes - 6&nbsp; Chapter 6: Linear Model Selection and Regrularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./class_project.html" rel="next">
<link href="./ch5.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch6.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">stat4500notes</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting up Python Computing Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./class_project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Class Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#best-subset-selecttion" id="toc-best-subset-selecttion" class="nav-link active" data-scroll-target="#best-subset-selecttion"><span class="header-section-number">6.1</span> Best Subset Selecttion</a></li>
  <li><a href="#stepwise-selection" id="toc-stepwise-selection" class="nav-link" data-scroll-target="#stepwise-selection"><span class="header-section-number">6.2</span> Stepwise selection</a>
  <ul class="collapse">
  <li><a href="#forward-stepwise-selection" id="toc-forward-stepwise-selection" class="nav-link" data-scroll-target="#forward-stepwise-selection"><span class="header-section-number">6.2.1</span> Forward Stepwise Selection</a></li>
  <li><a href="#backward-stepwise-selection" id="toc-backward-stepwise-selection" class="nav-link" data-scroll-target="#backward-stepwise-selection"><span class="header-section-number">6.2.2</span> Backward Stepwise Selection</a></li>
  </ul></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">6.3</span> Model selection</a></li>
  <li><a href="#shrinkage-methods-for-variable-selection" id="toc-shrinkage-methods-for-variable-selection" class="nav-link" data-scroll-target="#shrinkage-methods-for-variable-selection"><span class="header-section-number">6.4</span> Shrinkage methods for Variable selection</a>
  <ul class="collapse">
  <li><a href="#ridge-regression-minimize-the-following-objective" id="toc-ridge-regression-minimize-the-following-objective" class="nav-link" data-scroll-target="#ridge-regression-minimize-the-following-objective"><span class="header-section-number">6.4.1</span> Ridge regression: minimize the following objective</a></li>
  </ul></li>
  <li><a href="#dimension-reduction-methods-transforming-x_j." id="toc-dimension-reduction-methods-transforming-x_j." class="nav-link" data-scroll-target="#dimension-reduction-methods-transforming-x_j."><span class="header-section-number">6.5</span> Dimension reduction methods: transforming <span class="math inline">\(X_j\)</span>.</a>
  <ul class="collapse">
  <li><a href="#pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" id="toc-pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" class="nav-link" data-scroll-target="#pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors"><span class="header-section-number">6.5.1</span> PCA regression: first use PCA to obtain <span class="math inline">\(M\)</span>- PCA as linear combinations (directions) of the original <span class="math inline">\(p\)</span> predictors:</a></li>
  <li><a href="#partial-least-squares" id="toc-partial-least-squares" class="nav-link" data-scroll-target="#partial-least-squares"><span class="header-section-number">6.5.2</span> Partial Least Squares</a></li>
  </ul></li>
  <li><a href="#code-snippet" id="toc-code-snippet" class="nav-link" data-scroll-target="#code-snippet"><span class="header-section-number">6.6</span> Code Snippet</a>
  <ul class="collapse">
  <li><a href="#python" id="toc-python" class="nav-link" data-scroll-target="#python"><span class="header-section-number">6.6.1</span> Python</a></li>
  <li><a href="#numpy" id="toc-numpy" class="nav-link" data-scroll-target="#numpy"><span class="header-section-number">6.6.2</span> Numpy</a></li>
  <li><a href="#pandas" id="toc-pandas" class="nav-link" data-scroll-target="#pandas"><span class="header-section-number">6.6.3</span> Pandas</a></li>
  <li><a href="#graphics" id="toc-graphics" class="nav-link" data-scroll-target="#graphics"><span class="header-section-number">6.6.4</span> Graphics</a></li>
  <li><a href="#islp-and-statsmodels" id="toc-islp-and-statsmodels" class="nav-link" data-scroll-target="#islp-and-statsmodels"><span class="header-section-number">6.6.5</span> ISLP and statsmodels</a></li>
  <li><a href="#sklearn" id="toc-sklearn" class="nav-link" data-scroll-target="#sklearn"><span class="header-section-number">6.6.6</span> sklearn</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 6: Linear Model Selection and Regrularization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Linear models are <em>interpretable</em> and often shows small variance. They are fitted by <em>OLS</em>. There are other methods that can either provide alternatives or improve linear regression models in terms of <em>prediction accuracy</em> (especially when <span class="math inline">\(p&gt;n\)</span>) and automatic <em>feature selection</em>.</p>
<p>There are three classes of methods:</p>
<ul>
<li>subset selection: pick a subset of the <span class="math inline">\(p\)</span> predictors that best explains the response.</li>
<li>Shrinkage (regularization). With an added regularizing term, estimated parameters are shrunken to zero relative the OLS estimates. If <span class="math inline">\(L^2\)</span> regularization is used, all coefficients are shrunk toward zero; while if #L^1$ is used, then some coefficients will become zero, leading to actual <em>variable selection</em> or <em>sparse representation</em>.</li>
<li>Dimension reduction. Project <span class="math inline">\(p\)</span>-predictors to a <span class="math inline">\(M\)</span> (<span class="math inline">\(M&lt;p\)</span>) dimensional subspace. Each new direction is a linear combination (or projection) of the <span class="math inline">\(p\)</span>-variables. These <span class="math inline">\(M\)</span>-projections can then be used to fit a linear regression model(<strong>PCR</strong> if the <span class="math inline">\(M\)</span>-projections are obtained in an unsupervised way; or <strong>PLR</strong> if thses <span class="math inline">\(M\)</span>-projections are obtained in a supervised way))</li>
</ul>
<section id="best-subset-selecttion" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="best-subset-selecttion"><span class="header-section-number">6.1</span> Best Subset Selecttion</h2>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>null model</em> <span class="math inline">\(\mathcal{M}_0\)</span>, which contains no predictors. This model simply set <span class="math inline">\(Y=\text{mean}(y_i)\)</span>.</li>
<li>for <span class="math inline">\(k=1, 2, \cdots, p\)</span>: fit <span class="math inline">\(p \choose k\)</span> models containing exactly <span class="math inline">\(k\)</span> predictors. Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> on the training set, called <span class="math inline">\(\mathcal{M}_k\)</span>. Note for each categorical variable with <span class="math inline">\(L\)</span>-level, there are <span class="math inline">\(L-1\)</span> dummy variables.</li>
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
<p>Best subset selection suffers - high computation: needs to compute <span class="math inline">\(2^p\)</span> models - overfitting due to the large search space of models</p>
</section>
<section id="stepwise-selection" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="stepwise-selection"><span class="header-section-number">6.2</span> Stepwise selection</h2>
<p>Both Forward and Backward selection are stepwise selection. They are used when <span class="math inline">\(p\)</span> is large. They searches over <span class="math inline">\(1+p(p+1)/2\)</span> models and are <em>greedy</em> algorithm and is not guaranteed to find the best possible model out of all <span class="math inline">\(2^p\)</span> models.</p>
<ul>
<li>Backward selection requires <span class="math inline">\(n&gt;p\)</span> (so that the full model can be fit);</li>
<li>Forwarad selection can be used when <span class="math inline">\(n&lt;p\)</span>.</li>
</ul>
<section id="forward-stepwise-selection" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="forward-stepwise-selection"><span class="header-section-number">6.2.1</span> Forward Stepwise Selection</h3>
<p>Adding one variable at at time that offers the greatest addtional improvement.</p>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>null model</em> <span class="math inline">\(\mathcal{M}_0\)</span>, which contains no predictors. This model simply set <span class="math inline">\(Y=\text{mean}(y_i)\)</span>.</li>
<li>for <span class="math inline">\(k=1, 2, \cdots, p-1\)</span>:</li>
</ol>
<ul>
<li>fit all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor.</li>
<li>Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> on the training set, called <span class="math inline">\(\mathcal{M}_{k+1}\)</span>.</li>
</ul>
<ol start="3" type="1">
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
<section id="backward-stepwise-selection" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="backward-stepwise-selection"><span class="header-section-number">6.2.2</span> Backward Stepwise Selection</h3>
<p>It begins with the full model with all variables, and iteratively removing one variable at at time.</p>
<p><strong>Algorithm</strong></p>
<ol type="1">
<li>Fit the data with the <em>full model</em> <span class="math inline">\(\mathcal{M}_p\)</span>, which contains all predictors.</li>
<li>for <span class="math inline">\(k=p, p-1, \cdots, 1\)</span>:</li>
</ol>
<ul>
<li>fit all <span class="math inline">\(k\)</span> models that contains all but one of the predictors in <span class="math inline">\(\mathcal{M}_k\)</span>.</li>
<li>Pick the best one that having the smallest RSS or largest <span class="math inline">\(R^2\)</span> on the training set, called <span class="math inline">\(\mathcal{M}_{k-1}\)</span>.</li>
</ul>
<ol start="3" type="1">
<li>Select the best one among <span class="math inline">\(\mathcal{M}_0, \cdots, \mathcal{M}_p\)</span> using cross-validation or other measures such as <span class="math inline">\(C_p (AIC)\)</span>, <span class="math inline">\(BIC\)</span> or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
</section>
<section id="model-selection" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">6.3</span> Model selection</h2>
<p>Models with all predictors always have the smallest <span class="math inline">\(RSS\)</span> or largest <span class="math inline">\(R^2\)</span> on the training set. Therefore they are not suitable to choose the best one among models with different number of predictors.</p>
<p>We ought to <em>estimate the test error</em> on a test set. This may be done - indirectly by adjusting the training error to account for the bias due to overfitting: <span class="math inline">\(C_p\)</span> (equivalently, AIC in case of linear model with Gaussian errors), BIC and adjusted <span class="math inline">\(R^2\)</span>. * Mallow’s <span class="math inline">\(C_p\)</span>: <span class="math display">\[
  C_p =\frac{1}{n}(RSS+2d\hat{\sigma}^2)
  \]</span> where, <span class="math inline">\(d\)</span> us the number of parameters and <span class="math inline">\(\hat{\sigma}^2 \approx Var{\epsilon}\)</span>.</p>
<ul>
<li><p>AIC <span class="math display">\[
  AIC = -2\log L + 2d
  \]</span> where, <span class="math inline">\(L\)</span> is the maximum likelihood function for the estimated model.</p></li>
<li><p>BIC <span class="math display">\[
  BIC = \frac{1}{n}( RSS + \log (n)d \hat{\sigma}^2 )
  \]</span> Since <span class="math inline">\(\log n&gt;\)</span> for <span class="math inline">\(n&gt;7\)</span>, the BIC places a higher penatlty on models with many variables, and hence select smalller models than <span class="math inline">\(C_p\)</span>.</p></li>
<li><p>Adjusted <span class="math inline">\(R^2\)</span> (larger value is better)</p>
<p><span class="math display">\[
\text{Adjusted }R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
\]</span> <span class="math inline">\(Rss/(n-d-1)\)</span> may increase or decrease depends on <span class="math inline">\(d\)</span>. Unlike <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span> pays a price for the inclusion of unnecessary variables in a model.</p></li>
<li><p>directly by cross-validation (or validation). It doesnot require estimate <span class="math inline">\(\sigma^2\)</span>. It has a wide range of usage, as it may difficult to estimate <span class="math inline">\(d\)</span> or <span class="math inline">\(\sigma^2\)</span>. Choose the model that has the smallest test error.</p></li>
</ul>
</section>
<section id="shrinkage-methods-for-variable-selection" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="shrinkage-methods-for-variable-selection"><span class="header-section-number">6.4</span> Shrinkage methods for Variable selection</h2>
<p>The shrinkage offers an altertivate to selecting variables by adjusting a hyperparameter that trades-off RSS and the model parameter magnitudes. Cross-validation may be used to select the best <span class="math inline">\(\lambda\)</span>. After the <span class="math inline">\(\lambda\)</span> is selected, one can fit a final model using the entire traning data set.</p>
<section id="ridge-regression-minimize-the-following-objective" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="ridge-regression-minimize-the-following-objective"><span class="header-section-number">6.4.1</span> Ridge regression: minimize the following objective</h3>
<p><span class="math display">\[
RSS + \lambda \sum_{j=1}^p \beta_j^2
\]</span> - It encourages the model parameters to shrink toward zero and find a balance between RSS and model parameter magnitudes. Cross-validation is used to find the best tuning parameter <span class="math inline">\(\lambda\)</span>. When <span class="math inline">\(\lambda\)</span> is large, <span class="math inline">\(\beta_j\to 0\)</span>. Note, Ridge shrinks all coefficients and include all <span class="math inline">\(p\)</span> variables.</p>
<ul>
<li><p>The OLS coefficients estimates are <em>scale equivariant</em>: regardless of how <span class="math inline">\(X_j\)</span> is scaled, <span class="math inline">\(X_j\hat{\beta}_j\)</span> remain the same: if <span class="math inline">\(X_j\)</span> is multiplied by <span class="math inline">\(c\)</span>, this will simply leads to <span class="math inline">\(\hat{\beta}_j\)</span> be scaled by a factor of <span class="math inline">\(1/c\)</span>.</p></li>
<li><p>In contrast, when multiplying <span class="math inline">\(X_j\)</span> by a factor, this may significantly change the ridge coefficients. Therefore, it is best practice to <em>standardize the predictors</em> before fitting a ridge model: <span class="math display">\[
\tilde{x}_{ij} =\frac{x_{ij}}{\frac{1}{n} \sum_{i=1}^n(x_{ij} - \bar{x}_j)}
\]</span> *** The Lasso (Least Absolute Shrinkage and Selection Operator)</p>
<ul>
<li><p>The Lasso replaces the <span class="math inline">\(\ell^2\)</span> error with <span class="math inline">\(\ell^1\)</span> penalty. Lasso can force some coefficients to become exactly zero when <span class="math inline">\(\lambda\)</span> is large enough. Thus it can actually performs <em>variable selection</em>. Agian, cross-validaton is employed to select <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>The reason Lasso can perform variable selection is because the objective function is equivalent to</p></li>
</ul>
<p>$$, _{j=1}^p |_j| s</p>
<p>$$ for some <span class="math inline">\(s\)</span>. The contour of RSS in general only touch the <span class="math inline">\(\ell_1\)</span> ball at its vertex, at which a minimum is obtained with some variables vanishes. In contrast, the <span class="math inline">\(\ell_2\)</span> ball is round, and in general, the countour of the RSS function only touches the sphere at a surface point where a minimum is obtained with no variable vanishes.</p></li>
<li><p>Neither ridge nor the lasso will universally dominate the other. When the response is a small number of predcitors, one may expect lasso performs better, better in practice, this is never known in advance.</p></li>
<li><p>Combining ridge and lasso leades to <em>elastic net</em> method.</p></li>
</ul>
</section>
</section>
<section id="dimension-reduction-methods-transforming-x_j." class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="dimension-reduction-methods-transforming-x_j."><span class="header-section-number">6.5</span> Dimension reduction methods: transforming <span class="math inline">\(X_j\)</span>.</h2>
<p>There are two types of dimension reduction methods for regression: a) PCA regression, b) Partial list squares PLS.</p>
<section id="pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="pca-regression-first-use-pca-to-obtain-m--pca-as-linear-combinations-directions-of-the-original-p-predictors"><span class="header-section-number">6.5.1</span> PCA regression: first use PCA to obtain <span class="math inline">\(M\)</span>- PCA as linear combinations (directions) of the original <span class="math inline">\(p\)</span> predictors:</h3>
<p><span id="eq-PCA"><span class="math display">\[
  Z_m =\sum_{j=1}^p \phi_{mj} X_j, \qquad 1\le m \le M.
   \tag{6.1}\]</span></span> The first PCA contains the largest variance in <span class="math inline">\(X\)</span>, and minimize the sum of squared perpendicular distances to each point (the projection error on the PCA); The second PCA is orthogonal to the first PCA and has the second largest variance and is uncorrelated with the first, and so on. These directions are obtained in an <em>unsupervised way</em>, as <span class="math inline">\(Y\)</span> is not used to obtain these components. Consequently, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.</p>
<p>We then use OLS to fit a linear regression model <span id="eq-PCAR"><span class="math display">\[
y_i =\theta_0 +\sum_{m=1}^M \theta_m z_{im}+\epsilon_i, \qquad i=1,2,\cdots, n
\tag{6.2}\]</span></span></p>
<p>After substitute <a href="#eq-PCA" class="quarto-xref">Equation&nbsp;<span>6.1</span></a> into equation <a href="#eq-PCAR" class="quarto-xref">Equation&nbsp;<span>6.2</span></a>, one can find that <span class="math display">\[
\sum_{m=1}^M \theta_mz_{im} =\sum_{j=1}^p \beta_jx_{ij}
\]</span> with <span id="eq-pcar-beta"><span class="math display">\[
\beta_j = \sum_{m=1}^M \theta_m\phi_{mj}
\tag{6.3}\]</span></span> So model <a href="#eq-PCAR" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> is a special case of linear regression subject to the constants <a href="#eq-pcar-beta" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>.</p>
</section>
<section id="partial-least-squares" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="partial-least-squares"><span class="header-section-number">6.5.2</span> Partial Least Squares</h3>
<p>Similar to PCAR, PLS also first identifies a new set of features <span class="math inline">\(Z_1, Z_2, \cdots, Z_m\)</span>, each of which is a linear combinations of the orginnal features, and then fits a linear model via OLS with these new <span class="math inline">\(M\)</span> features.</p>
<p>But PLS identifies these new features in a <em>supervised way</em>, that is, PLS uses <span class="math inline">\(Y\)</span> in order to identify the new features that not only approximate the old features well, but also are <em>related to the response</em>, i.e., these new features explain both the response and the predictors.</p>
<p>Firt PLS standarizes the <span class="math inline">\(p\)</span> predictors. PLS identifies the first component <span class="math inline">\(Z_1 = \sum_{j=1}^p \phi_{1j}X_j\)</span> by choosing <span class="math inline">\(\phi_{1j}\)</span> equals to the coefficient from the simple linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X_j\)</span>. Since this coefficient is equal to <span class="math inline">\(r_{X_jY}\frac{\sigma_{Y}}{\sigma_{X_j}}\)</span>, PLS places teh highest weight on the variables that are most strongly related to <span class="math inline">\(Y\)</span>.</p>
<p>Next, PLS takes the residuals and then repeat the same process.</p>
</section>
</section>
<section id="code-snippet" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="code-snippet"><span class="header-section-number">6.6</span> Code Snippet</h2>
<section id="python" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="python"><span class="header-section-number">6.6.1</span> Python</h3>
</section>
<section id="numpy" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="numpy"><span class="header-section-number">6.6.2</span> Numpy</h3>
</section>
<section id="pandas" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="pandas"><span class="header-section-number">6.6.3</span> Pandas</h3>
</section>
<section id="graphics" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="graphics"><span class="header-section-number">6.6.4</span> Graphics</h3>
</section>
<section id="islp-and-statsmodels" class="level3" data-number="6.6.5">
<h3 data-number="6.6.5" class="anchored" data-anchor-id="islp-and-statsmodels"><span class="header-section-number">6.6.5</span> ISLP and statsmodels</h3>
</section>
<section id="sklearn" class="level3" data-number="6.6.6">
<h3 data-number="6.6.6" class="anchored" data-anchor-id="sklearn"><span class="header-section-number">6.6.6</span> sklearn</h3>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch5.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./class_project.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Class Project</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>