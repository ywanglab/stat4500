<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.398">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>stat4500notes - 3&nbsp; Chapter 3: Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch4.html" rel="next">
<link href="./ch2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch3.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">stat4500notes</a> 
        <div class="sidebar-tools-main">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./stat4500notes.docx">
              <i class="bi bi-bi-file-word pe-1"></i>
            Download Docx
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Setting up Python Computing Environment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 5: Resampling Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link active" data-scroll-target="#simple-linear-regression"><span class="header-section-number">3.1</span> Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#assessing-the-accuracy-of-the-coefficients" id="toc-assessing-the-accuracy-of-the-coefficients" class="nav-link" data-scroll-target="#assessing-the-accuracy-of-the-coefficients"><span class="header-section-number">3.1.1</span> Assessing the accuracy of the coefficients</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">3.2</span> Multiple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#model-assumption" id="toc-model-assumption" class="nav-link" data-scroll-target="#model-assumption"><span class="header-section-number">3.2.1</span> <strong>Model Assumption</strong></a></li>
  <li><a href="#assessing-existence-of-linear-relationship" id="toc-assessing-existence-of-linear-relationship" class="nav-link" data-scroll-target="#assessing-existence-of-linear-relationship"><span class="header-section-number">3.2.2</span> Assessing existence of linear relationship</a></li>
  <li><a href="#assess-the-accuracy-of-the-future-prediciton" id="toc-assess-the-accuracy-of-the-future-prediciton" class="nav-link" data-scroll-target="#assess-the-accuracy-of-the-future-prediciton"><span class="header-section-number">3.2.3</span> Assess the accuracy of the future prediciton</a></li>
  <li><a href="#assessing-the-overall-accuracy-of-the-model" id="toc-assessing-the-overall-accuracy-of-the-model" class="nav-link" data-scroll-target="#assessing-the-overall-accuracy-of-the-model"><span class="header-section-number">3.2.4</span> Assessing the overall accuracy of the model</a></li>
  </ul></li>
  <li><a href="#model-selectionvariable-selections-balance-training-errors-with-model-size" id="toc-model-selectionvariable-selections-balance-training-errors-with-model-size" class="nav-link" data-scroll-target="#model-selectionvariable-selections-balance-training-errors-with-model-size"><span class="header-section-number">3.3</span> Model Selection/Variable Selections: balance training errors with model size</a></li>
  <li><a href="#handle-categorical-variables-factor-variables" id="toc-handle-categorical-variables-factor-variables" class="nav-link" data-scroll-target="#handle-categorical-variables-factor-variables"><span class="header-section-number">3.4</span> Handle categorical variables (factor variables)</a></li>
  <li><a href="#adding-non-linearity" id="toc-adding-non-linearity" class="nav-link" data-scroll-target="#adding-non-linearity"><span class="header-section-number">3.5</span> Adding non-linearity</a>
  <ul class="collapse">
  <li><a href="#modeling-interactions-synergy" id="toc-modeling-interactions-synergy" class="nav-link" data-scroll-target="#modeling-interactions-synergy"><span class="header-section-number">3.5.1</span> Modeling interactions (synergy)</a></li>
  <li><a href="#adding-terms-of-transformed-predictors" id="toc-adding-terms-of-transformed-predictors" class="nav-link" data-scroll-target="#adding-terms-of-transformed-predictors"><span class="header-section-number">3.5.2</span> Adding terms of transformed predictors</a></li>
  </ul></li>
  <li><a href="#outliers-unusual-y_i-that-is-far-from-haty_i" id="toc-outliers-unusual-y_i-that-is-far-from-haty_i" class="nav-link" data-scroll-target="#outliers-unusual-y_i-that-is-far-from-haty_i"><span class="header-section-number">3.6</span> Outliers (Unusual <span class="math inline">\(y_i\)</span> that is far from <span class="math inline">\(\hat{y}_i\)</span>)</a></li>
  <li><a href="#high-leverage-points-unusual-x_i" id="toc-high-leverage-points-unusual-x_i" class="nav-link" data-scroll-target="#high-leverage-points-unusual-x_i"><span class="header-section-number">3.7</span> High leverage points (unusual <span class="math inline">\(x_i\)</span>)</a></li>
  <li><a href="#compared-to-knn-regression" id="toc-compared-to-knn-regression" class="nav-link" data-scroll-target="#compared-to-knn-regression"><span class="header-section-number">3.8</span> Compared to KNN Regression</a></li>
  <li><a href="#homework-indicates-optional" id="toc-homework-indicates-optional" class="nav-link" data-scroll-target="#homework-indicates-optional"><span class="header-section-number">3.9</span> Homework (* indicates optional):</a></li>
  <li><a href="#code-gist" id="toc-code-gist" class="nav-link" data-scroll-target="#code-gist"><span class="header-section-number">3.10</span> Code Gist</a>
  <ul class="collapse">
  <li><a href="#python" id="toc-python" class="nav-link" data-scroll-target="#python"><span class="header-section-number">3.10.1</span> Python</a></li>
  <li><a href="#numpy" id="toc-numpy" class="nav-link" data-scroll-target="#numpy"><span class="header-section-number">3.10.2</span> Numpy</a></li>
  <li><a href="#pandas" id="toc-pandas" class="nav-link" data-scroll-target="#pandas"><span class="header-section-number">3.10.3</span> Pandas</a></li>
  <li><a href="#graphics" id="toc-graphics" class="nav-link" data-scroll-target="#graphics"><span class="header-section-number">3.10.4</span> Graphics</a></li>
  <li><a href="#using-sns" id="toc-using-sns" class="nav-link" data-scroll-target="#using-sns"><span class="header-section-number">3.10.5</span> Using Sns</a></li>
  <li><a href="#using-sklearn" id="toc-using-sklearn" class="nav-link" data-scroll-target="#using-sklearn"><span class="header-section-number">3.10.6</span> Using Sklearn</a></li>
  <li><a href="#using-statsmodels-and-islp" id="toc-using-statsmodels-and-islp" class="nav-link" data-scroll-target="#using-statsmodels-and-islp"><span class="header-section-number">3.10.7</span> Using statsmodels and ISLP</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 3: Linear Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Linear regression is a simple supervised learning assuming a linear relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. When there is only one predictor, it’s a <strong>simple linear regression</strong>. When there are more than one predictors, it’s called <strong>multiple linear regression</strong>. Note <em>multivariate regression</em> refer to the <span class="math inline">\(Y\)</span> variable is a vector.</p>
<section id="simple-linear-regression" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">3.1</span> Simple Linear Regression</h2>
<p>Assumes the <em>population regression line</em> model <span class="math display">\[
Y = \beta_0 + \beta_1 X +\epsilon,
\]</span> where, <span class="math inline">\(\beta_0\)</span> is the <em>expected</em> value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span>, and <span class="math inline">\(\beta_1\)</span> is the <em>average</em> change in <span class="math inline">\(Y\)</span> with a one-unit increase in <span class="math inline">\(X\)</span>. <span class="math inline">\(\epsilon\)</span> is a “catch all” error term.</p>
<p>After training using the training data, we can obtain the parameter estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. The we can obtain the prediction for <span class="math inline">\(x\)</span> given by the <em>least square line</em>: <span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x
\]</span> The error at a data point <span class="math inline">\(x_i\)</span> is given by <span class="math inline">\(e_i = y_i -\hat{y}_i\)</span>, and the <em>residual sum of squares</em> (RSS) is <span class="math display">\[
\text{RSS} =e_1^2+\cdots +e_n^2.
\]</span> One can use the least square approach to minimize RSS to obtain <span class="math display">\[
\hat{\beta}_1 =\frac{(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=r_{xy}\frac{\sigma_y}{\sigma_x}
\]</span> <span class="math display">\[
\hat{\beta}_0= \bar{y}-\hat{\beta}_1 \bar{x}
\]</span> where, <span class="math inline">\(\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i\)</span> and <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i\)</span>, and the correlation <span id="eq-correlation-rxy"><span class="math display">\[
r_{xy} = \frac{\text{cov}(x,y)}{\sigma_x\sigma_y}=\frac{(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.
\tag{3.1}\]</span></span> is the normalized convariance. Note <span class="math inline">\(-1\le r_{xy} \le 1\)</span>. When there is no intercept, that is <span class="math inline">\(\beta_0=0\)</span>, then <span class="math display">\[
\hat{y}_i=x_i \hat{\beta}=\sum_{i=1}^n a_i y_i
\]</span> where, <span class="math display">\[
\hat{\beta} =\frac{\sum_{i=1}^n x_iy_i}{ \sum_{i=1}^{n} x_i^2}
\]</span> That is, the fitted values are linear combinations of the response values when there is no intercept.</p>
<section id="assessing-the-accuracy-of-the-coefficients" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="assessing-the-accuracy-of-the-coefficients"><span class="header-section-number">3.1.1</span> Assessing the accuracy of the coefficients</h3>
<p>Let <span class="math inline">\(\sigma^2=\text{Var}(\epsilon)\)</span>, that is, <span class="math inline">\(\sigma^2\)</span> is the variance of <span class="math inline">\(Y\)</span>, (estimated by <span class="math inline">\(\sigma^2\approx =\text{RSE} =\text{RSS}/(n-p-1)\)</span>. ) Assume each observation have <em>common variance</em> (homoscedasticity) and are <em>uncorrelated</em>, then the standard errors under repeated sampling <span class="math display">\[
(\text{SE}[\hat{\beta}_1])^2 = \frac{1}{\sigma^2_x}\cdot \frac{\sigma^2}{n}
\]</span> <span class="math display">\[
(\text{SE}[\hat{\beta}_0])^2 = \left[1+ \frac{\bar{x}^2}{\sigma^2_x} \right]\cdot \frac{\sigma^2}{n}
\]</span></p>
<ul>
<li><p>when <span class="math inline">\(x_i\)</span> are more spread out (with large <span class="math inline">\(\sigma_x^2\)</span>), then <span class="math inline">\(\text{SE}[\hat{\beta}_1]\)</span> is small. This is because there are more <em>leverage</em> (of <span class="math inline">\(x\)</span> values) to estimate the slope.</p></li>
<li><p>when <span class="math inline">\(\bar{x} =0\)</span> , then <span class="math inline">\(\text{SE}[\hat{\beta}_0] = \text{SE}[\bar{y}]\)</span>. In this case, <span class="math inline">\(\hat{\beta}_0 = \bar{y}\)</span>.</p></li>
</ul>
<p>Standard errors are used to construct CI and perform hypothesis test for the estimated <span class="math inline">\(\hat{\beta}_0\)</span> or <span class="math inline">\(\hat{\beta}_1\)</span>. Under the assumption of <strong>Gaussian error</strong>, One can construct the CI of significance level <span class="math inline">\(\alpha\)</span> (e.g., <span class="math inline">\(\alpha=0.05\)</span>) as <span class="math display">\[
\hat{\beta}_j = [\hat{\beta}_j- t_{1-\alpha/2,n-p-1}\cdot \text{SE}[\hat{\beta}_j], \hat{\beta}_j+ t_{1-\alpha/2,n-p-1} \cdot \text{SE}[\hat{\beta}_j]  ]
\]</span> Where <span class="math inline">\(j=0, 1\)</span>. Large interval including zero indicates <span class="math inline">\(\beta_j\)</span> is not statistically significant from 0. When <span class="math inline">\(n\)</span> is sufficient large, <span class="math inline">\(t_{0.975,n-p-1} \approx 2\)</span>. With the standard errors of the coefficients, one can also perform <strong>hypothesis test</strong> on the coefficients. For <span class="math inline">\(j=0,1\)</span>,</p>
<p><span class="math display">\[H_0: \beta_j=0\]</span> <span class="math display">\[H_A: \beta_j\ne 0\]</span> The <span class="math inline">\(t\)</span>-statistic of degree <span class="math inline">\(n-p-1\)</span>, given by <span class="math display">\[
t = \frac{\hat{\beta}_j - 0}{\text{SE}[\hat{\beta}_j]}
\]</span> shows how far away <span class="math inline">\(\hat{\beta}_j\)</span> is away from zero, normalized by its error <span class="math inline">\(\text{SE}[\hat{\beta}_j]\)</span>. One can then compute the <span class="math inline">\(p\)</span>-value corresponding to this <span class="math inline">\(t\)</span> and test the hypothesis. Small <span class="math inline">\(p\)</span>-value indicates <strong>strong</strong> relationship.</p>
</section>
</section>
<section id="multiple-linear-regression" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">3.2</span> Multiple Linear Regression</h2>
<p><span class="math display">\[
Y= \beta_0 + \beta_1X_1 +\cdots + \beta_pX_p + \epsilon.
\]</span> The estimate of the coefficients <span class="math inline">\(\hat{\beta_j}\)</span>, <span class="math inline">\(j\in \mathbb{Z}_{p+1}\)</span> are found by using the same least square method to minimize RSS. we interpret <span class="math inline">\(\beta_j\)</span> as the <em>expected</em> (average) effect on <span class="math inline">\(Y\)</span> with one unit increase in <span class="math inline">\(X_j\)</span>, <strong>holding all other predictors fixed</strong>. This interpretation is based on the assumptions that <em>the predictors are uncorrelated</em>, so <em>each predictor can be estimated and tested separately</em>. When there are correlations among predictors, the variance of all coefficients tends to increase, sometimes dramatically, and the previous interpretation becomes hazardous because when <span class="math inline">\(X_j\)</span> changes, everything else changes.</p>
<section id="model-assumption" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="model-assumption"><span class="header-section-number">3.2.1</span> <strong>Model Assumption</strong></h3>
<ul>
<li><p>linearity: <span class="math inline">\(Y\)</span> is linear in <span class="math inline">\(X\)</span>. The change in Y associated with one unit of change in <span class="math inline">\(X_j\)</span> is constant, regardless of the value of <span class="math inline">\(X_j\)</span>. This can be examined visually by plotting the <em>residual plot</em> (<span class="math inline">\(e_i\)</span> vs.&nbsp;<span class="math inline">\(x_i\)</span> for <span class="math inline">\(p=1\)</span> or <span class="math inline">\(e_i\)</span> vs <span class="math inline">\(\hat{y}_i\)</span> for multiple regression). If the linear assumption is true, then the residual plot should not exhibit obvious pattern. If there is a nonlinear relationship suggested by by the residual plot, then a simple approach is to include transformed <span class="math inline">\(X\)</span>, such as <span class="math inline">\(\log X\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, or <span class="math inline">\(X^2\)</span>.</p></li>
<li><p>additive: The association between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span> is independent of other predictors.</p></li>
<li><p>Errors <span class="math inline">\(\epsilon_i\)</span> are uncorrelated. This means <span class="math inline">\(\epsilon_i\)</span> provides no information for <span class="math inline">\(\epsilon_{i+1}\)</span>. Otherwise (for example, frequently observed in a time series, where error terms are positively correlated, and <em>tracking</em> is observed in the residuals, i.e., adjacent error terms take similar values), the estimated standard error will tend to be underestimated, hence leading less confidence in the estimated model.</p></li>
<li><p>Homoscedasticity: <span class="math inline">\(\text{Var}(\epsilon_i) =\sigma^2\)</span>. The error terms have constant variance. If not (heteroscedasticity), one may use transformed <span class="math inline">\(Y\)</span>, such as <span class="math inline">\(\sqrt{Y}\)</span>, or <span class="math inline">\(\log(Y)\)</span> to mitigate this; or use <em>weighted least squares</em> if it’s known that for example <span class="math inline">\(\sigma_i^2=\sigma^2/n_i\)</span>.</p></li>
<li><p>Non-colinearity: two variables are colinear if they are highly correlated with each other. Co-linearity causes a great deal of <em>uncertainty</em> in the coefficient estimates, that is, reducing the accuracy of the coefficient estimates, thus cause the standard error of <span class="math inline">\(\beta_j\)</span> to grow, and hence smaller <span class="math inline">\(t\)</span>-statistic. As a result, we may fail to reject <span class="math inline">\(H_0: \beta_j=0\)</span>. This in turn means the power of Hypothesis test, the probability of correctly detecting a <em>non-zero</em> coefficient is reduced by colinearity. To detect colinearity,</p>
<ul>
<li><p>use the correlation matrix of predictors. Large value of the matrix in absolute value indicates highly correlated variable pairs. But this approach cannnot detect <em>multicolinearity</em>.</p></li>
<li><p>Use VIF (Variance inflation factor, VIF <span class="math inline">\(\ge 1\)</span>) to detect multicolinearity. It is possible for colinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This is the <em>multicolinearity</em> situation.</p></li>
</ul>
<p>VIF is the ratio of the variance of <span class="math inline">\(\hat{\beta}_j\)</span> when fitting the full model divided by the variance of <span class="math inline">\(\hat{\beta}_j\)</span> if fit on its own. It can be calculated by <span class="math display">\[
  \text{VIF}(\hat{\beta}_j) =\frac{1}{1-R^2_{X_j|X_{-j}}}
  \]</span> Where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_j\)</span> onto all of the other predictors. A VIF value exceeds 5 or 10 (i.e., <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> close to 1) indicates colinearity.</p>
<p>To remedy a colinearity problem:</p>
<ul>
<li>drop a redundant variable (variables with colinearity should have similar VIF values. )</li>
<li>Combine the colinear variables into a single predictor, e.g., taking the average of the standardized versions of those variables.</li>
</ul></li>
</ul>
<p><strong>Claims of causality should be avoided for observational data</strong>.</p>
</section>
<section id="assessing-existence-of-linear-relationship" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="assessing-existence-of-linear-relationship"><span class="header-section-number">3.2.2</span> Assessing existence of linear relationship</h3>
<ul>
<li>test Hypothesis (test if there is a linear relationship between the response and predictors) <span class="math display">\[
H_0: \beta_1=\beta_2=\cdots = \beta_p=0
\]</span> <span class="math display">\[
H_a: \text{at least one } \beta_j \text{ is non-zero.}
\]</span> using <span class="math inline">\(F\)</span>-statistic <span class="math display">\[
F=\frac{\text{SSB/df(B)}}{\text{SSW/df(W)}}=\frac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)}\sim F_{p,n-p-1}
\]</span> If <span class="math inline">\(H_0\)</span> is true, <span class="math inline">\(F\approx 1\)</span>; if <span class="math inline">\(H_a\)</span> is true, <span class="math inline">\(F&gt;&gt;1\)</span>. <span class="math inline">\(F\)</span>-statistic adjust with <span class="math inline">\(p\)</span>. Note that one <strong>cannot conclude</strong> if an individual <span class="math inline">\(t\)</span>-statistic is significant, then there is at least one predictor is related to the response, especially when <span class="math inline">\(p\)</span> is large. This is related to <em>multiple testing</em>. The reason is that when <span class="math inline">\(p\)</span> is large, there is <span class="math inline">\(\alpha\)</span> (eg 5%) chance that a predictor will have a small <span class="math inline">\(p\)</span>-value by chance. When <span class="math inline">\(p&gt;n\)</span>, <span class="math inline">\(F\)</span>-statistic cannot be used.</li>
</ul>
<p>If the goal is to test that a particular subset of <span class="math inline">\(q\)</span> of the coefficients are zero, that is, (for convenience, we put the <span class="math inline">\(q\)</span> variables chosen at the end of the variabale list) <span id="eq-subset-hypothesis"><span class="math display">\[
H_0: \beta_{p-q+1} = \beta_{p-q+2}=\cdots = \beta_p=0
\tag{3.2}\]</span></span> In this case, use <span class="math display">\[
F = \frac{(\text{RSS}_0-\text{RSS})/q}{\text{RSS}/(n-p-1)}\sim F_{q,n-p-1}
\]</span> where, <span class="math inline">\(\text{RSS}_0\)</span> is the residual sum of squares of a second model that uses all variables <em>except</em> those last <span class="math inline">\(q\)</span> variables. When <span class="math inline">\(q=1\)</span>, <span class="math inline">\(F\)</span>-statistic in <a href="#eq-subset-hypothesis" class="quarto-xref">Equation&nbsp;<span>3.2</span></a> is the square of the <span class="math inline">\(t\)</span>-statistic of that variable. The <span class="math inline">\(t\)</span>-statistic reported in a regression model gives the <em>partial effect</em> of adding that variable, while holding other variables fixed.</p>
</section>
<section id="assess-the-accuracy-of-the-future-prediciton" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="assess-the-accuracy-of-the-future-prediciton"><span class="header-section-number">3.2.3</span> Assess the accuracy of the future prediciton</h3>
<ul>
<li><p>confidence interval: Indicate how far away <span class="math inline">\(\hat{Y}=\hat{f}(X)\)</span> is from the population average <span class="math inline">\(f(X)\)</span> because the coefficients <span class="math inline">\(\hat{\beta}_{j}\)</span> are estimated, It quantifies <em>reducible error</em> around the predicted average response <span class="math inline">\(\hat{f}(X)\)</span>, does-not include <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p>prediction interval: Indicate how far away <span class="math inline">\(\hat{Y}=\hat{f}(X)\)</span> is from <span class="math inline">\(Y\)</span>. predict an individual response <span class="math inline">\(Y\approx \hat{f}(X)+\epsilon\)</span>. Prediction interval is always wider than the confidence interval, because it includes <em>irreducible error</em> <span class="math inline">\(\epsilon\)</span>.</p></li>
</ul>
</section>
<section id="assessing-the-overall-accuracy-of-the-model" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="assessing-the-overall-accuracy-of-the-model"><span class="header-section-number">3.2.4</span> Assessing the overall accuracy of the model</h3>
<ul>
<li><p>RSE. To this end, first define the <em>lack of fit</em> measure <strong>Residual Standard Error</strong> <span class="math display">\[
\text{RSE} = \sqrt{\frac{1}{n-p-1}\text{RSS}} = \sqrt{\frac{1}{n-p-1}\sum_{i=1}^n(y_i-\hat{y}_i)^2} \approx \sigma=\sqrt{\text{Var}(\epsilon)}
\]</span> It is the <em>average amount</em> in <span class="math inline">\(\hat{Y}\)</span> that a response deviates from the <em>true regression line</em> (<span class="math inline">\(\beta_0+\beta_1 X\)</span>). Note, RSE can increase with more variables if the decrease of RSS doesnot offset the increase of <span class="math inline">\(p\)</span>.</p></li>
<li><p>Approach 2: Using <em>R-squared</em> (fraction of variance in <span class="math inline">\(Y\)</span> explained by <span class="math inline">\(X\)</span>), which is independent of of the scale of <span class="math inline">\(Y\)</span>, and <span class="math inline">\(0\le R^2 \le 1\)</span>: <span class="math display">\[
R^2 =\frac{\text{TSS}-\text{RSS}}{\text{TSS}} = 1-\frac{\text{RSS}}{\text{TSS}}
\]</span> where, <span class="math inline">\(\text{TSS}=\sum_{i=1}^n(y_i- \bar{y})\)</span>. When <span class="math inline">\(R^2\)</span> is near 0 indicates that 1) either the linear model is wrong 2) or th error variance <span class="math inline">\(\sigma^2\)</span> is high, or both. <span class="math inline">\(R^2\)</span> measures the linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. If computed on the training set, when adding more variables, the RSS always decrease, hence <span class="math inline">\(R^2\)</span> will always increase.</p></li>
</ul>
<p>For simple linear regression, <span class="math inline">\(R^2=r_{xy}^2\)</span>, where the <em>sample correlation</em> measures the linear relationship between variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. See the formula <span class="math inline">\(r_{xy}\)</span> above <a href="#eq-correlation-rxy" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>. For multiple linear regression, <span class="math inline">\(R^2=(\text{Cor}(Y, \hat{Y}))^2\)</span>. The fitted linear model maximizes this correlation among all possible linear models.</p>
</section>
</section>
<section id="model-selectionvariable-selections-balance-training-errors-with-model-size" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="model-selectionvariable-selections-balance-training-errors-with-model-size"><span class="header-section-number">3.3</span> Model Selection/Variable Selections: balance training errors with model size</h2>
<ul>
<li><p><strong>All subsets (best subsets) regression</strong>: compute the least square fit for all <span class="math inline">\(2^p\)</span> possible subsets and then choose among them based on certain criterion that balance training error and model size</p></li>
<li><p><strong>Forward selection</strong>: Start from the <em>null model</em> that only contains <span class="math inline">\(\beta_0\)</span>. Then find the best model containing one predictor that minimizing RSS. Denote the variable by <span class="math inline">\(\beta_1\)</span>. Then continue to find the best model with the lowest RSS by adding one variable from the remaining predictors, and so on. Continue until some stopping rule is met: e.g., when all remaining variables have a <span class="math inline">\(p\)</span>-value greater than some threshold.</p></li>
<li><p><strong>Backward selection</strong>: start with all variables in the model. Remove the variable with the largest <span class="math inline">\(p\)</span>-value (least statistically significant). The new <span class="math inline">\((p-1)\)</span> model is fit, and remove the variable with the largest <span class="math inline">\(p\)</span>-value. Continue until a stopping rule is satisfied, e.g., all remaining variables have <span class="math inline">\(p\)</span>-value less than some threshold.</p></li>
<li><p><strong>Mixed selection</strong>: Start with forward selection. Since the <span class="math inline">\(p\)</span>-value for variables can become larger as new predictors are added, at any point if the <span class="math inline">\(p\)</span>-value of a variable in the model rises above a certain threshold, then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low <span class="math inline">\(p\)</span>-value, and all variables outside the model would have a large <span class="math inline">\(p\)</span>-value if added to the model.</p>
<p>Backward selection cannot be used if <span class="math inline">\(p&gt;n\)</span>. Forward selection can always be used, but might include variables early that later become redundant. Mixed selection can remedy this problem.</p></li>
<li><p><strong>others</strong> (Chapter 6): including Mallow’s <span class="math inline">\(C_p\)</span>, AIC (Akaike Informaton Criterion), BIC, adjusted <span class="math inline">\(R^2\)</span>, Cross-validation, test set performance.</p></li>
<li><p><strong>not valid</strong>: we could look at individual <span class="math inline">\(p\)</span>-values, but when the number of variables <span class="math inline">\(p\)</span> is large, we likely to make a false discoveries.</p></li>
</ul>
</section>
<section id="handle-categorical-variables-factor-variables" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="handle-categorical-variables-factor-variables"><span class="header-section-number">3.4</span> Handle categorical variables (factor variables)</h2>
<p>For a categorical variable <span class="math inline">\(X_i\)</span> with <span class="math inline">\(m\)</span> levels, create one fewer dummy variables (<span class="math inline">\(x_{ij}, 1\le j \le m-1\)</span>)&gt;. The level with no dummy variable is called the <em>baseline</em>. The coefficient corresponding to a dummy variable is the expected difference in change in <span class="math inline">\(Y\)</span> when compared to the baseline, while holding other predictors fixed.</p>
</section>
<section id="adding-non-linearity" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="adding-non-linearity"><span class="header-section-number">3.5</span> Adding non-linearity</h2>
<section id="modeling-interactions-synergy" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="modeling-interactions-synergy"><span class="header-section-number">3.5.1</span> Modeling interactions (synergy)</h3>
<p>When two variables have interaction, then their product <span class="math inline">\(X_iX_j\)</span> can be added into the regression model, and the product maybe considered as a single variable for inference, for example, compute its SE, <span class="math inline">\(t\)</span>-statistics, <span class="math inline">\(p\)</span>-value, Hypothesis test, etc.</p>
<p>If we include an interaction in a model, then the <strong>Hierarchy principle</strong> should be followed: always include the main effects, even if the <span class="math inline">\(p\)</span>-values associated with their coefficients are not significant. This is because without the main effects, the interactions are hard to interpret, as they would also contain the main effect.</p>
</section>
<section id="adding-terms-of-transformed-predictors" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="adding-terms-of-transformed-predictors"><span class="header-section-number">3.5.2</span> Adding terms of transformed predictors</h3>
<ol type="1">
<li><em>Polynomial regression</em>: Add a term involving <span class="math inline">\(X_i^k\)</span> for some <span class="math inline">\(k&gt;1\)</span>.</li>
<li>other forms: Adding root or logarithm terms of the predictors.</li>
</ol>
</section>
</section>
<section id="outliers-unusual-y_i-that-is-far-from-haty_i" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="outliers-unusual-y_i-that-is-far-from-haty_i"><span class="header-section-number">3.6</span> Outliers (Unusual <span class="math inline">\(y_i\)</span> that is far from <span class="math inline">\(\hat{y}_i\)</span>)</h2>
<p>It is typical for an outlier that does not have an unusual predictor value (with low levarage) to have little effect on the least squares fit, but it will increase RSE, hence deteriorate CI, <span class="math inline">\(p\)</span>-value and <span class="math inline">\(R^2\)</span>, thus affecting interpreting the model.</p>
<p>An outlier can be identified by computing the <span class="math display">\[\text{studentized residual}=\frac{e_i}{\text{RSE}_i}\]</span> A studentized residual great than 3 may be considered as an outlier.</p>
</section>
<section id="high-leverage-points-unusual-x_i" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="high-leverage-points-unusual-x_i"><span class="header-section-number">3.7</span> High leverage points (unusual <span class="math inline">\(x_i\)</span>)</h2>
<p>High leverage points tend to have sizeable impact on the regression line. To quantify the observation’s leverage, one needs to compute the <strong>leverage statistic</strong> <span class="math display">\[h_i = \frac{1}{n}+ \frac{(x_i-\bar{x})^2}{\sum_{j=1}^n (x_j-\bar{x})^2}.\]</span> <span class="math inline">\(1/n \le h_i\le 1\)</span> and <span class="math inline">\(\text{Ave}(h_i)=(p+1)/n\)</span>. A large value of this statistic (for example, great than <span class="math inline">\((p+1)/n\)</span>) indicates an observation with high leverage. The leverage <span class="math inline">\(1/n\le h_i\le 1\)</span>, reflects the amount an observation influences its own fit.</p>
</section>
<section id="compared-to-knn-regression" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="compared-to-knn-regression"><span class="header-section-number">3.8</span> Compared to KNN Regression</h2>
<p>KNN regression is a non-parametric method that makes prediction at <span class="math inline">\(x_0\)</span> by taking the average in a <span class="math inline">\(K\)</span>-point neightborhood <span class="math display">\[
\hat{f}(x_0) = \frac{1}{K}\sum_{x_i \in \mathcal{N}_{x_0}}{y_i}
\]</span> A small value of <span class="math inline">\(K\)</span> provides more flexible model with low bias but high variance while a larger value of <span class="math inline">\(K\)</span> provides smoother fit with less variance. An optimal value of <span class="math inline">\(K\)</span> depend on the <em>bias-variance tradeoff</em>. For non-linear data set, KNN may provides better fit than a linear regression model. However, in higher dimension (e.g., <span class="math inline">\(p\ge 4\)</span>), even for nonlinear data set, KNN may perform much inferior to linear regression, because of the <strong>curse of dimensionality</strong>, as the <span class="math inline">\(K\)</span> observations that are nearest to <span class="math inline">\(x_0\)</span> may in fact far away from <span class="math inline">\(x_0\)</span>.</p>
</section>
<section id="homework-indicates-optional" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="homework-indicates-optional"><span class="header-section-number">3.9</span> Homework (* indicates optional):</h2>
<ul>
<li>Conceptual: 1–6</li>
<li>Applied: 8–15. at least one.</li>
</ul>
</section>
<section id="code-gist" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="code-gist"><span class="header-section-number">3.10</span> Code Gist</h2>
<section id="python" class="level3" data-number="3.10.1">
<h3 data-number="3.10.1" class="anchored" data-anchor-id="python"><span class="header-section-number">3.10.1</span> Python</h3>
<pre><code>dir() # provides a list of objects at the top level name space
dir(A) # display addtributes and methods for the object A
' + '.join(X.columns) # form a string by joining the list of column names by "+"</code></pre>
</section>
<section id="numpy" class="level3" data-number="3.10.2">
<h3 data-number="3.10.2" class="anchored" data-anchor-id="numpy"><span class="header-section-number">3.10.2</span> Numpy</h3>
<pre><code>np.argmax(x) # identify the location of the largest element
np.concatenate([x,y],axis=0) # concatenate two arrays x and y. 
</code></pre>
</section>
<section id="pandas" class="level3" data-number="3.10.3">
<h3 data-number="3.10.3" class="anchored" data-anchor-id="pandas"><span class="header-section-number">3.10.3</span> Pandas</h3>
<pre><code>X = pd.DataFrame(data=X, columns=['a','b'])

pd.DataFrame({'intercept': np.ones(Boston.shape[0]),
                  'lstat': Boston['lstat']}) # make a dataframe using a dictionary
Boston.columns.drop('medv','age') # drop the elements 'medv' and 'age' from the list of column names

pd.DataFrame({'vif':vals},
                   index=X.columns[1:]) # form a df by specifying index labels

X.values  # Convert dataframe X to numpy array
X.to_numpy() # recommended to replace the above method
DataFrame.corr(numeric_only=True) # correlations between columns 
x.sort_values(ascending=False)
pd.to_numeric(auto_df['horsepower'], errors='coerce') # if error, denote it by "NaN".
auto_df.dropna(subset= ['horsepower', 'mpg',], inplace=True) # looking for NaN in the columns in `subset`, otherwise, all columns

auto_df.drop('name', axis=1, inplace=True)

left2.join(right2, how="left") #join two databases by index. 
left1.join(right1, on="key") # left-join by left1["key"] and the index of right1. 
pd.concat([s1, s4], axis="columns", join="outer")
</code></pre>
</section>
<section id="graphics" class="level3" data-number="3.10.4">
<h3 data-number="3.10.4" class="anchored" data-anchor-id="graphics"><span class="header-section-number">3.10.4</span> Graphics</h3>
<pre><code>xlim = ax.get_xlim() # get the x_limit values xlim[0], xlim[1]
ax.axline() # add a line to a plot
ax.axhline(0, c='k', ls='--'); # horizontal line
line, = ax.plot(x,y,label="line 1") # "line 1" is the legend
# alternatively the label can be set by 
line.set_label("line 1")
ax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')
ax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)
ax.legend(loc="upper left", fontsize=25) # adding legendes
ax.annotate(i,xy=(fitted[i],residuals[i])) # annote at the xy position with i. 


plt.style.use('seaborn') # pretty matplotlib plots
plt.rcParams.update({'font.size': 16})
plt.rcParams["figure.figsize"] = (8,7)

plt.rc('font', size=10)
plt.rc('figure', titlesize=13)
plt.rc('axes', labelsize=10)
plt.rc('axes', titlesize=13)
plt.rc('legend', fontsize=8) # adjust legend globally
    </code></pre>
</section>
<section id="using-sns" class="level3" data-number="3.10.5">
<h3 data-number="3.10.5" class="anchored" data-anchor-id="using-sns"><span class="header-section-number">3.10.5</span> Using Sns</h3>
<pre><code>sns.set(font_scale=1.25) # set font size 25% larger than default
sns.heatmap(corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})
ax = sns.regplot(x=x, y=y)</code></pre>
</section>
<section id="using-sklearn" class="level3" data-number="3.10.6">
<h3 data-number="3.10.6" class="anchored" data-anchor-id="using-sklearn"><span class="header-section-number">3.10.6</span> Using Sklearn</h3>
<pre><code>from sklearn.linear_model import LinearRegression
## Set the target and predictors
X = auto_df['horsepower']

### To get polynomial features
poly = PolynomialFeatures(interaction_only=True,include_bias = False)
X = poly.fit_transform(X)

y = auto_df['mpg']

## Reshape the columns in the required dimensions for sklearn
length = X.values.shape[0]
X = X.values.reshape(length, 1) #both X and y needs to be 2-D
y = y.values.reshape(length, 1)

## Initiate the linear regressor and fit it to data using sklearn
regr = LinearRegression()
regr.fit(X, y)
regr.intercept_
regr.coef_

pred_y = regr.predict(X)</code></pre>
</section>
<section id="using-statsmodels-and-islp" class="level3" data-number="3.10.7">
<h3 data-number="3.10.7" class="anchored" data-anchor-id="using-statsmodels-and-islp"><span class="header-section-number">3.10.7</span> Using statsmodels and ISLP</h3>
<pre><code>from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
                         summarize,
                         poly)
                         
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence \
     import variance_inflation_factor as VIF
from statsmodels.stats.anova import anova_lm

#Training
Boston = load_data("Boston") 
#hand-craft the design matrix X
X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), #design matrix. intercept column
                  'lstat': Boston['lstat']}) 
#the following is the preferred method to create X
design = MS(['lstat']) # specifying the model variables. Automatically add an intercept, adding "intercept=False" if no intercept. 
design = design.fit(Boston) # do intial computation as specified in the model object design by MS(), such as means or sd. This attached some statistics to the `design` object, and need to be applied to the new data for prediciton

X = design.transform(Boston) # apply the fitted transformation to the data to create X
#alternatiely, 
X = design.fit_transform(Boston) # this combines the .fit() and .transform() two lines

y = Boston['medv']
model = sm.OLS(y, X) # setup the model
model = smf.ols('mpg ~ horsepower', data=auto_df) # alternatively use smf formula, y~x
smf.ols("y ~ x -1" , data=df).fit() # "-1" not inclding the intercept
results = model.fit() # results is a dictionary:.summary(), .params 

results.summary()
results.params # coefficients
results.resid # reisdual array
results.rsquared # R^2
results.pvalues
np.sqrt(results.scale) # RSE
results.fittedvalues # fitted \hat(y)_i at x_i in the traning set


summarize(results) # summzrize() is from ISLP to show the esstial results from model.fit()

# Makding prediciton 
new_df = pd.DataFrame({'lstat':[5, 10, 15]})  # new test-set containing data where to make predicitons
newX = design.transform(new_df) # apply the same transform to the test-set
new_predictions = results.get_prediction(newX);
new_predictions.predicted_mean #predicted values
new_predictions.conf_int(alpha=0.05) #for the predicted values

new_predictions.conf_int(obs=True, alpha=0.05) # prediction intervals by setting obs=True

# Including an interaction term
X = MS(['lstat',
        'age',
        ('lstat', 'age')]).fit_transform(Boston) #interaction term ('lstat', 'age')

# Adding a polynomial term of higher degree
X = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston) # Note poly is from ISLP, # adding deg1 and deg2 terms. by default poly creates ortho. poly. not including an intercept. 
# Given a qualitative variable, `ModelSpec()` generates dummy
variables automatically, to avoid collinearity with an intercept, the first column is dropped in the design matrix generated by 'ModelSpec()` by default.

# Compare nested models using ANOVA
anova_lm(results1, results3) # result1 is the result of linear model, an result3 is the result of a larger model

# Identify high leverage x
infl = results.get_influence() 
# hat_matrix_diag calculate the leverate statistics
np.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage

# Calculate VIF
vals = [VIF(X, i)
        for i in range(1, X.shape[1])] #excluding column 0 because it's all 1's in X.
vif = pd.DataFrame({'vif':vals},
                   index=X.columns[1:])
vif # VIF exceeds 5 or 10 indicates a problematic amount of colinearity
</code></pre>
<p>Useful Code Snippets</p>
<pre><code>def abline(ax, b, m, *args, **kwargs):
    "Add a line with slope m and intercept b to ax"
    xlim = ax.get_xlim()
    ylim = [m * xlim[0] + b, m * xlim[1] + b]
    ax.plot(xlim, ylim, *args, **kwargs)</code></pre>
<pre><code># Plot scatter plot with a regression line
ax = Boston.plot.scatter('lstat', 'medv')
abline(ax,
       results.params[0],
       results.params[1],
       'r--',
       linewidth=3)</code></pre>
<pre><code># Plot residuals vs. fitted values (note, not vs x, therefore works for multiple regression)
ax = subplots(figsize=(8,8))[1]
ax.scatter(results.fittedvalues, results.resid)
ax.set_xlabel('Fitted value')
ax.set_ylabel('Residual')
ax.axhline(0, c='k', ls='--');

# Alternatively
sns.residplot(x=X, y=y, lowess=True, color="g", ax=ax)

# Plot the smoothed residuals~fitted by LOWESS
from statsmodels.nonparametric.smoothers_lowess import lowess
smoothed = lowess(residuals,fitted) # Note the order (y,x)
ax.plot(smoothed[:,0],smoothed[:,1],color = 'r')

# QQ plot for the residuas (obtain studentized residuals for identifying outliers)
import scipy.stats as stats
sorted_student_residuals = pd.Series(smf_model.get_influence().resid_studentized_internal)
sorted_student_residuals.index = smf_model.resid.index
sorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)
df = pd.DataFrame(sorted_student_residuals)
df.columns = ['sorted_student_residuals']

#stats.probplot() #assess whether a dataset follows a specified distribution
df['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0] 
    
x = df['theoretical_quantiles']
y = df['sorted_student_residuals']
ax.scatter(x,y, edgecolor = 'k',facecolor = 'none')
</code></pre>
<pre><code># Plot leverage statistics
infl = results.get_influence()
ax = subplots(figsize=(8,8))[1]
ax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)
ax.set_xlabel('Index')
ax.set_ylabel('Leverage')
np.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ch2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 2: Statistical Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch4.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 4: Classification</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>