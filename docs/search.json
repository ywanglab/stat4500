[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stat4500notes",
    "section": "",
    "text": "Preface\nThis is a Lecture note written for the course STAT 4500: Machine Learning offered at Auburn University at Montgomery. The course uses the textbook James et al. (2023).\nThis is a book wrtieen by Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#on-your-own-computer",
    "href": "intro.html#on-your-own-computer",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.1 on Your own computer",
    "text": "1.1 on Your own computer\n\nyou can either git clone or download a zipped file containing the codes from the site: https://github.com/intro-stat-learning/ISLP_labs/tree/stable. If downloaded a zipped file of the codes, unzipped the file to a folder, for example, named islp. If git clone (preferred, you need to have Git installed on your computer, check this link for how to install Git https://ywanglab.github.io/stat1010/git.html), do git clone https://github.com/intro-stat-learning/ISLP_labs.git\nDownload and install the following software:\n\nAnaconda: Download anaconda and install using default installation options\nVisual Studio Code (VSC): Download VSC and install\nstart VSC and install VSC extensions in VSC: Python, Jupyter, intellicode\n(optional) Quarto for authoring: Download Quarto and install\n\nCreate a virtual environment named islp for Python. Start an anaconda terminal.\n  conda create -n islp python==3.10\n  conda activate islp\n  conda install pip ipykernel\n  pip install -r https://raw.githubusercontent.com/intro-stat-learning/ISLP_labs/v2.1.2/requirements.txt\nYou are ready to run the codes using VSC or jupyter lab.\n\nActivate the venv: conda activate islp\nStart a Anaconda terminal, navigate to the folder using the command cd path/to/islp, where path/to/islp means the file path to the folder islp, such as \\Users\\ywang2\\islp. Start VSC by typing code . in the anaconda terminal.\nopen/create a .ipynb or .py file.\nSelect the kernel islp\nRun a code cell by pressing Shift+Enter or click the triangular play button.\nContinue to run other cells.\nAfter finishing using VSC, close the VSC, and deactivate the virtual environment in a conda terminal: conda deactivate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "intro.html#use-google-colab",
    "href": "intro.html#use-google-colab",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.2 Use Google Colab",
    "text": "1.2 Use Google Colab\nAll you need is a Google account. Sign in your Google account in a browser, and navigate to Google Colab. Google Colab supports both Python and R. Python is the default engine. Change the engine to R in Connect-&gt;change runtime type. Then you are all set. Your file will be saved to your Google Drive or you can choose to send it to your GitHub account (recommended).\n\n1.2.1 How to run a project file from your Google Drive?\nMany times, when you run a python file in Colab, it needs to access other files, such as data files in a subdirectory. In this case, it would be convenient to have the same file structure in the Google Colab user home directory. To do this, you can use Google Drive to store your project folder, and then mount the Google Drive in Colab.\nLet’s assume the project folder name, islp/.Here are the steps:\n\ngit clone the project folder (example: git clone https://github.com/intro-stat-learning/ISLP_labs.git) to your local folder. This step is only needed when you want to clone some remote repo from GitHub.\nUpload the folder (ex: islp) to Google Drive.\nOpen the file using Colab. In Google Drive, double click on the ipynb file, example, ch06.ipynb (or click on the three dots on the right end, and choose open with, then Google Colaborotary), the file will be opened by Google Colab.\nMount the Google Drive. In Google Colab, with the specific file (example, ch06.ipynb) being opened, move your cursor to the first code cell, and then click on the folder icon (this should be the fourth icon) on the upper left border in the Colab browser. This will open the file explorer pane. Typically you would see a folder named sample_data shown. On the top of the pane, click on the Google Drive icon to mount the Google Drive. Google Colab will insert the following code below the cursor in your opened ipynb file:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nRun this code cell by pressing SHIFT+ENTER, and follow the prompts to complete the authentication. Wait for ~10 seconds, your Google Drive will be mounted in Colab, and it will be displayed as a folder named drive in the file explorer pane. You might need to click on the Refresh folder icon to see the folder drive.\nOpen a new code cell below the above code cell, and type the code\n  %cd /content/drive/MyDrive/islp/\nThis is to change the directory to the project directory on the Google Drive. Run this code cell, and you are ready to run the file ch06.ipynb from the folder islp on your personal Google Drive, just like it’s on your local computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "ch2.html#what-is-statistical-learning",
    "href": "ch2.html#what-is-statistical-learning",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.1 What is statistical learning?",
    "text": "2.1 What is statistical learning?\nFor the input variable \\(X\\in \\mathbb{R}^p\\) and response variable \\(Y\\in \\mathbb{R}\\), assume that \\[Y=f(X) + \\epsilon, \\] where \\(\\epsilon\\) is a random variable representing irreducible error. We assume \\(\\epsilon\\) is independent of \\(X\\) and \\(E[\\epsilon]=0\\). \\(\\epsilon\\) may include unmeasured variables or unmeasurable variation.\nStatistical learning is to estimate \\(f\\) using various methods. Denote the estimate by \\(\\hat{f}\\).\n\nregression problem: when \\(Y\\) is a continuous (quantitative) variable . In this case \\(f(x)=E(Y|X=x)\\) is the population regression function, that is, regression finds a conditional expectation of \\(Y\\).\nclassification problem: when \\(Y\\) only takes small number of discrete values, i.e., qualitative (categorical).\n\nLogistic regression is a classification problem, but since it estimates class probability, it may be considered as a regression problem.\n\nsupervised learning: training data \\(\\mathcal{Tr}=\\{(x_i, y_i):i\\in \\mathbb{Z}_n\\}\\): linear regression, logistic regression\nunsupervised learning: when only \\(x_i\\) are available. clustering analysis, PCA\nsemi-supervised learning: some data with labels (\\(y_i\\)), some do not.\nreinforcement learning: learn a state-action policy function for an agent to interacting with an environment to maximize a reward function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#why-estimate-f",
    "href": "ch2.html#why-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.2 Why estimate \\(f\\)?",
    "text": "2.2 Why estimate \\(f\\)?\nWe can use estimated \\(\\hat{f}\\) to\n\nmake predictions for a new \\(X\\), \\[\\hat{Y} =\\hat{f}(X). \\] The prediction error may be quantified as \\[E[(Y-\\hat{Y})^2] = (f(X)-\\hat{f})^2 +\\text{Var}[\\epsilon].\\] The first term of the error is reducible by trying to improve \\(\\hat{f}\\), where we assume \\(f\\), \\(\\hat{f}\\) and \\(X\\) are fixed.\nmake inference, such as\n\nWhich predictors are associated with the response?\nwhat is the relationship between the response and each predictor?\nis the assumed relationship adequate? (linear or more complicated?)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-estimate-f",
    "href": "ch2.html#how-to-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.3 How to estimate \\(f\\)",
    "text": "2.3 How to estimate \\(f\\)\nWe use obtained observations called training data \\(\\{(x_k, y_k): k \\in \\mathbb{Z}_n \\}\\) to train an algorithm to obtain the estimate \\(\\hat{f}\\).\n\nParametric methods: first assume there is a function form (shape) with some parameters. For example, a linear regression model with two parameters. Then use the training data to train or fit the model to determine the values of the parameters.\nAdvantages: simplify the problem of fit an arbitrary function to estimate a set of parameters.\nDisadvantages: may not be flexible unless with large number of parameters and/or complex function shapes.\nExample: linear regression,\nNon-parametric methods: Do not explicitly assume a function form of \\(f\\). They seek to estimate \\(f\\) directly using data points, can be quite flexible and accurate.\n**Disadvantage: need large number of data points\nExample: KNN (but breakdown for higher dimention. Typically only for \\(p\\le 4\\)), spline fit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-assess-model-accuracy",
    "href": "ch2.html#how-to-assess-model-accuracy",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.4 How to assess model accuracy",
    "text": "2.4 How to assess model accuracy\nFor regression problems, the most commonly used measure is the mean squared error (MSE), given by \\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{f}(x_i))^2\n\\] For classification problems, typically the following error rate (classifications error) is calculated: \\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i\\ne \\hat{y}_i)\n\\] The accuracy on a training set can be arbitrarily increased by increasing the model flexibility. However, we are in general interested in the error on the test set rather on the training set, the model accuracy should be assessed on a test set.\nFlexible models tend to overfit the data, which essentially means they follow the error or noise too closely in the training set, therefore cannot be generalized to unseen cases (test set).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#model-selection",
    "href": "ch2.html#model-selection",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.5 Model Selection:",
    "text": "2.5 Model Selection:\nNo free lunch theorem\nThere is no single best method for all data sets, which means some method works better than other methods for a particular dataset. Therefore, one needs to perform model selections. Here are some principles.\n\n2.5.1 Trade-off between Model flexibility and Model Interpretability\nMore flexible models have higher degree of freedom and are less interpretable because it’s difficult to interpret the relationship between a predictor and the response.\nLASSO is less flexible than linear regression. GAM (generalized additive model) allows some non-linearity. Full non-linear models have higher flexibility, such as bagging, boosting, SVM, etc.\nWhen inference is the goal, then there are advantages to using simple and less flexible models for interpretability.\nWhen prediction is the main goal, more flexible model may be a choice. But sometimes, we obtain more accurate prediction using a simpler model because the underlying dataset has a simpler structure. Therefore, it is not necessarily true that a more flexible model has a higher prediction accuracy.\nOccam’s Razor: Among competing hypotheses that perform equally well, the one with the fewest assumptions should be selected.\n\n\n2.5.2 Model Selection: the Bias-Variance Trade-off\nAs the model flexibility increases, the training MSE (or error rate for classificiton) will decrease, but the test MSE (error rate) in general will not and will show a characteristic U-shape. This is because when evaluated at a test point \\(x_0\\), the expected test MSE can be decomposed into \\[\nE\\left[ (y_0-\\hat{f}(x_0))^2 \\right] = \\text{Var}[\\hat{f}(x_0)] + (\\text{Bias}(\\hat{f}(x_0)))^2+\\text{Var}[\\epsilon]\n\\] where the expectation is over different \\(\\hat{f}\\) on a different training set or on a different training step if the training process is stochastic, and \\[\n\\text{Bias}(\\hat{f}(x_0))= E[\\hat{f}(x_0)]-f(x_0)\n\\] To obtain the least test MSE, one must trade off between variance and bias. Less flexible model tendes to have higher bias, and more flexible models tend to have higher variance. An optimal flexibility for the least test MSE varies with different data sets. Non-linear data tends to require higher optimal flexibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#bayes-classifier",
    "href": "ch2.html#bayes-classifier",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.6 Bayes Classifier",
    "text": "2.6 Bayes Classifier\nIt can be shown that Bayes Classifier minimizes the classification test error \\[\n\\text{Ave}(I(y_0\\ne \\hat{y}_0)).\n\\] A Bayes Classifier assigns a test observation with predictor \\(x_0\\) to the class for which \\[\n\\text{Pr}(Y=j|X=x_0)\n\\] is largest. It’s error rate is given by \\[\n1-E[\\max_{j} \\text{Pr}(Y=j|X)]\n\\]\nwhere the expectation is over \\(X\\). The Bayes error is analogous to the irreducible error \\(\\epsilon\\).\nBayes Classifier is not attainable as we do not know \\(\\text{Pr}(Y|X)\\). We only can estimate \\(\\text{Pr}(Y|X)\\). One way to do this is by KNN. KNN estimate the conditional probability simply with a majority vote. The flexibility of KNN increases as \\(1/K\\) increases with \\(K=1\\) being the most flexible KNN. The training error is 0 for \\(K=1\\). A suitable \\(K\\) should be chosen for an appropriate trade off between bias and variance. The KNN classifier will classify the test point \\(x_0\\) based on the probability calculated from the \\(k\\) nearest points. KNN regression on the other hand will assign the test point \\(x_0\\) the average value of the \\(k\\) nearest neighbors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#homework-indicates-optional",
    "href": "ch2.html#homework-indicates-optional",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.7 Homework (* indicates optional):",
    "text": "2.7 Homework (* indicates optional):\n\nConceptual: 1,2,3,4*,5,6,7\nApplied: 8, 9*, 10*",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#code-gist",
    "href": "ch2.html#code-gist",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.8 Code Gist",
    "text": "2.8 Code Gist\n\n2.8.1 OS\nimport os\nos.chdir(path) # change dir\n\n\n2.8.2 Python:\nConcatenation using +\n\"hello\" + \" \" + \"world\"  # 'hello world'\n[3,4,5] + [4,9,7] # [3,4,5, 4,9,7]\n\nString formatting using string.format()\nprint('Total is: {0}'.format(total))\n\nzip to loop over a sequence of tuples\nfor value, weight in zip([2,3,19],\n                         [0.2,0.3,0.5]):\n    total += weight * value\n\n\n\n2.8.3 Numpy\n\n2.8.3.1 Numpy functions:\nnp.sum(x), np.sqrt(x) (entry wise). x**2 (entry wise power), np.corrcoef(x,y) (find the correlation coefficient of array x and array y)\nnp.mean(axis=None): axis could be None (all entries), 0(along row), 1(along column)\nnp.var(x, ddof=0), np.std(x, ddof=0), # Note both np.var and np.std accepts an argument ddof, the divisor is N-ddof.\nnp.linspace(-np.pi, np.pi, 50) # start, end, number of points 50\nnp.multiply.outer(row,col) # calculate the product over the mesh with vectors row and col.\nnp.zeros(shape or int, dtype) #eg: np.zeros(5,bool)\nnp.ones(Boston.shape[0])\nnp.all(x), np.any(x): check if all or any entry of x is true.\nnp.unique(x): find unique values in x. np.isnan(x): return a boolean array of len(x). np.isnan(x).mean(): find the percentage of np.nan values in x.\n\n\n2.8.3.2 Array Slicing and indexing\nnp.arange(start, stop, step) # numpy version of range\nx[slice(3:6)] # equivalent to x[3:6]\nIndexing an array using [row, col] format. If col is missing, then index the entire rows. len(row) must be equal to len(col). Otherwise use iterative indexing or use np.ix_(x_idx, y_idx) function, or use Boolean indexing, see below.\nA[1,2]: index entry at row 1 and col 2 (recall Python index start from 0)\nA[[1,3]] # row 1 and 3. Note the outer [] is considered as the operator, so only row indices are provided. \nA[:,[0,2]] # cols 0 and 2\nA[[1,3], [0,2,3]] # entry A[1,0] and A[3,2]\nA[1:4:2, 0:3:2] # entries in rows 1 and 3, cols 0 and 2\nA[[1,3], [0,2,3]] # syntax error\n# instead one can use the following two methods \nA[[1,3]][:,[0,2]] # iterative subsetting\nA[np.ix_([1,3],[0,2,3])] # use .ix_ function to create an index mesh\nA[keep_rows, keep_cols] # keep_rows, keep_cols are boolean arrays of the same length of rows or cols, respectively\nA[np.ix_([1,3],keep_cols)] # np.ix_()can be applied to mixture of integer array and boolean array\n\n\n2.8.3.3 Random numbers and generators\nnp.random.normal(loc=0.0, scale=1.0,size=None) # size can be an integer or a tuple.\n# \nrng = np.random.default_rng(1303) # set random generator seed\nrng.normal(loc=0, scale=5, size=2) # \nrng.standard_normal(10) # standard normal distribution of size 10\nrng.choice([0, np.nan], p=[0.8,0.2], size=A.shape)\n\n\n2.8.3.4 Numpy array atributes\n.dtype, .ndim, .shape\n\n\n2.8.3.5 Numpy array methods\nx.sum(axis=None) (equivalent to np.sum(x)), x.T (transpose),\nx.reshape((2,3)) # x.reshape() is a reference to x.\nx.min(), x.max()\n\n\n\n2.8.4 Graphics\n\n2.8.4.1 2-D figure\n# Using the subplots + ax methods\nfig, ax = subplots(nrows=2, ncols=3, figsize=(8, 8)) \n# explicitly name each axis in the grid \nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n\nax[0,1].plot(x, y,marker='o', 'r--', linewidth=3); #line plot. `;` suppresses the text output. pick ax[0,1] when there are  multiple axes\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.scatter(x, y, marker='o'); #scatter plot\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.set_xlabel(\"this is the x-axis\")\nax.set_ylabel(\"this is the y-axis\")\nax.set_title(\"Plot of X vs Y\");\naxes[0,1].set_xlim([-1,1]) # set x_lim. similarly `set_ylim()`\n\nfig = ax.figure  # get the figure object from an axes object\nfig.set_size_inches(12,3) # access the fig object to change fig size (width, height)\nfig # re-render the figure\nfig.savefig(\"Figure.pdf\", dpi=200); #save a figure into pdf. Other formats: .jpg, .png, etc\n\n\n2.8.4.2 Contour and image\nfig, ax = subplots(figsize=(8, 8))\nx = np.linspace(-np.pi, np.pi, 50)\ny = x\nf = np.multiply.outer(np.cos(y), 1 / (1 + x**2))\nax.contour(x, y, f, levels=None); # numbre of levels. if None, automatically choose\nax.imshow(f); # heatmap colorcoded by f\n\n\n\n2.8.5 Pandas\n\n2.8.5.1 loading data\npd.read_csv('Auto.csv') # read csv\npd.read_csv('Auto.data', \n            na_values =['?'], #specifying the na_values in the datafile. \n            delim_whitespace=True) # read whitespaced text file\npd.read_csv('College.csv', index_col=0) # use column `0` as the row labels \n\n\n\n2.8.5.2 Pandas Dataframe attributes and methods\nAuto.shape\nAuto.columns # gets the list of column names\nAuto.index #return the index (labels) objects\nAuto['horsepower'].to_numpy() # convert to numpy array\nAuto['horsepower'].sum()\n\nAuto.dropna() # drop the rows containing na values. \ndf.drop('B', axis=1, inplace=True) # drop a column 'B' inplace. \n#equivalent to df.drop(columns=['B'], inplace=True)\ndf.drop(index=['Ohio','Colorado']) #eqivalent to: df.drop(['Ohio','Colorado'], axis=0)\nauto_df.drop(auto_df.index[10:86]) # drop rows with index[10:86] not including 86\n\nAuto.set_index('name')# rename the index using the column 'name'.\n\npd.Series(Auto.cylinders, dtype='category') # convert the column `cylinders` to 'category` dtype\n# the convertison can be done using `astype()` method\nAuto.cylinders.astype('category')\nAuto.describe() # statistics summary of all columns\nAuto['mpg'].describe() # for selected columns\n\ncollege.rename({'Unnamed: 0': 'College'}, axis=1): # change column name, \n# alternavie way\ncollege_df.rename(columns={college_df.columns[0] : \"College\"}, inplace=True) #\n\ncollege['Elite'] = pd.cut(college['Top10perc'],  # binning a column\n                          [0,0.5,1],  #bin edges\n                          labels=['No', 'Yes'],  # bin labels (names)\n                          right=True,# True: right-inclusive (default) for each bin ( ]; False:rigth-exclusive \n                          )   \ncollege['Elite'].value_counts() # frequency counts\nauto.columns.tolist() # equivalent to  auto.columns.format() (rarely used)\n\n\n\n2.8.5.3 Selecting rows and columns\nSelect Rows:\nAuto[:3] # the first 3 rows. \nAuto[Auto['year'] &gt; 80] # select rows with boolean array\nAuto_re.loc[['amc rebel sst', 'ford torino']] #label_based row selection\nAuto_re.iloc[[3,4]] #integer-based row seleciton: rows 3 and 4 (index starting from 0)\nSelect Columns\nAuto['horsepower'] # select the column 'horsepower', resulting a pd.Series.\nAuto[['horsepower']] #obtain a dataframe of the column 'horsepower'. \nAuto_re.iloc[:,[0,2,3]] # intger-based selection\nauto_df.select_dtypes(include=['int16','int32']) # select columns by dtype\nSelect a subset\nAuto_re.iloc[[3,4],[0,2,3]] # integer-based \nAuto_re.loc['ford galaxie 500', ['mpg', 'origin']] #label-based \nAuto_re.loc[Auto_re['year'] &gt; 80, ['weight', 'origin']] # mix bolean indexing with labels\n\nAuto_re.loc[lambda df: (df['year'] &gt; 80) & (df['mpg'] &gt; 30),\n            ['weight', 'origin']\n           ]  # using labmda function with loc[]\n\n\n2.8.5.4 Pandas graphics\nWithout using subplots to get axes and figure objects\nax = Auto.plot.scatter('horsepower', 'mpg') #scatter plot of 'horsepower' vs 'mpg' from the dataframe Auto\nax.set_title('Horsepower vs. MPG');\nfig = ax.figure\nfig.savefig('horsepower_mpg.png');\n\nplt.gcf().subplots_adjust(bottom=0.05, left=0.1, top=0.95, right=0.95) #in percentage of the figure size. \nax1.fig.suptitle('College Scatter Matrix', fontsize=35)\nUsing subplots\nfig, axes = subplots(  ncols=3, figsize=(15, 5))\nAuto.plot.scatter('horsepower', 'mpg', ax=axes[1]);\nAuto.hist('mpg', ax=ax);\nAuto.hist('mpg', color='red', bins=12, ax=ax); # more customized \nBoxplot using subplots\nAuto.cylinders = pd.Series(Auto.cylinders, dtype='category') # needs to convert the `cylinders` column to categorical dtype\nfig, ax = subplots(figsize=(8, 8))\nAuto.boxplot('mpg', by='cylinders', ax=ax);\nScatter matrix\npd.plotting.scatter_matrix(Auto); # all columns\npd.plotting.scatter_matrix(Auto[['mpg',\n                                 'displacement',\n                                 'weight']]);  # selected columns\n                                 \n                                 \n#Alternatively with sns.pairplot\n\n\nSns Graphic\n# Scatter matrix\nax1 = sns.pairplot(college_df[college_df.columns[0:11]])\n\n# Boxplot\nsns.boxplot(ax=ax, x=\"Private\", y=\"Outstate\", data=college_df)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch3.html#simple-linear-regression",
    "href": "ch3.html#simple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.1 Simple Linear Regression",
    "text": "3.1 Simple Linear Regression\nAssumes the population regression line model \\[\nY = \\beta_0 + \\beta_1 X +\\epsilon,\n\\] where, \\(\\beta_0\\) is the expected value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) is the average change in \\(Y\\) with a one-unit increase in \\(X\\). \\(\\epsilon\\) is a “catch all” error term.\nAfter training using the training data, we can obtain the parameter estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The we can obtain the prediction for \\(x\\) given by the least square line: \\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\] The error at a data point \\(x_i\\) is given by \\(e_i = y_i -\\hat{y}_i\\), and the residual sum of squares (RSS) is \\[\n\\text{RSS} =e_1^2+\\cdots +e_n^2.\n\\] One can use the least square approach to minimize RSS to obtain \\[\n\\hat{\\beta}_1 =\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=r_{xy}\\frac{\\sigma_y}{\\sigma_x}\n\\] \\[\n\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\n\\] where, \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\). If we assume the data matrix \\(X\\) is demeaned, then \\(\\hat{beta}_0=\\bar{y}\\). and the correlation \\[\nr_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}=\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\tag{3.1}\\] is the normalized covariance. Note \\(-1\\le r_{xy} \\le 1\\). When there is no intercept, that is \\(\\beta_0=0\\), then \\[\n\\hat{y}_i=x_i \\hat{\\beta}=\\sum_{i=1}^n a_i y_i\n\\] where, \\[\n\\hat{\\beta} =\\frac{\\sum_{i=1}^n x_iy_i}{ \\sum_{i=1}^{n} x_i^2}\n\\] That is, the fitted values are linear combinations of the response values when there is no intercept.\n\n3.1.1 Assessing the accuracy of the coefficients\nLet \\(\\sigma^2=\\text{Var}(\\epsilon)\\), that is, \\(\\sigma^2\\) is the variance of \\(Y\\), (estimated by \\(\\sigma^2\\approx =\\text{RSE} =\\text{RSS}/(n-p-1)\\). ) Assume each observation have common variance (homoscedasticity) and are uncorrelated, then the standard errors under repeated sampling \\[\n(\\text{SE}[\\hat{\\beta}_1])^2 = \\frac{1}{\\sigma^2_x}\\cdot \\frac{\\sigma^2}{n}\n\\] \\[\n(\\text{SE}[\\hat{\\beta}_0])^2 = \\left[1+ \\frac{\\bar{x}^2}{\\sigma^2_x} \\right]\\cdot \\frac{\\sigma^2}{n}\n\\]\n\nwhen \\(x_i\\) are more spread out (with large \\(\\sigma_x^2\\)), then \\(\\text{SE}[\\hat{\\beta}_1]\\) is small. This is because there are more leverage (of \\(x\\) values) to estimate the slope.\nwhen \\(\\bar{x} =0\\) , then \\(\\text{SE}[\\hat{\\beta}_0] = \\text{SE}[\\bar{y}]\\). In this case, \\(\\hat{\\beta}_0 = \\bar{y}\\).\n\nStandard errors are used to construct CI and perform hypothesis test for the estimated \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\). Under the assumption of Gaussian error, One can construct the CI of significance level \\(\\alpha\\) (e.g., \\(\\alpha=0.05\\)) as \\[\n\\hat{\\beta}_j = [\\hat{\\beta}_j- t_{1-\\alpha/2,n-p-1}\\cdot \\text{SE}[\\hat{\\beta}_j], \\hat{\\beta}_j+ t_{1-\\alpha/2,n-p-1} \\cdot \\text{SE}[\\hat{\\beta}_j]  ]\n\\] Where \\(j=0, 1\\). Large interval including zero indicates \\(\\beta_j\\) is not statistically significant from 0. When \\(n\\) is sufficient large, \\(t_{0.975,n-p-1} \\approx 2\\). With the standard errors of the coefficients, one can also perform hypothesis test on the coefficients. For \\(j=0,1\\),\n\\[H_0: \\beta_j=0\\] \\[H_A: \\beta_j\\ne 0\\] The \\(t\\)-statistic of degree \\(n-p-1\\), given by \\[\nt = \\frac{\\hat{\\beta}_j - 0}{\\text{SE}[\\hat{\\beta}_j]}\n\\] shows how far away \\(\\hat{\\beta}_j\\) is away from zero, normalized by its error \\(\\text{SE}[\\hat{\\beta}_j]\\). One can then compute the \\(p\\)-value corresponding to this \\(t\\) and test the hypothesis. Small \\(p\\)-value indicates strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#multiple-linear-regression",
    "href": "ch3.html#multiple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.2 Multiple Linear Regression",
    "text": "3.2 Multiple Linear Regression\n\\[\nY= \\beta_0 + \\beta_1X_1 +\\cdots + \\beta_pX_p + \\epsilon.\n\\] The estimate of the coefficients \\(\\hat{\\beta_j}\\), \\(j\\in \\mathbb{Z}_{p+1}\\) are found by using the same least square method to minimize RSS. we interpret \\(\\beta_j\\) as the expected (average) effect on \\(Y\\) with one unit increase in \\(X_j\\), holding all other predictors fixed. This interpretation is based on the assumptions that the predictors are uncorrelated, so each predictor can be estimated and tested separately. When there are correlations among predictors, the variance of all coefficients tends to increase, sometimes dramatically, and the previous interpretation becomes hazardous because when \\(X_j\\) changes, everything else changes.\n\n3.2.1 Model Assumption\n\nlinearity: \\(Y\\) is linear in \\(X\\). The change in Y associated with one unit of change in \\(X_j\\) is constant, regardless of the value of \\(X_j\\). This can be examined visually by plotting the residual plot (\\(e_i\\) vs. \\(x_i\\) for \\(p=1\\) or \\(e_i\\) vs \\(\\hat{y}_i\\) for multiple regression). If the linear assumption is true, then the residual plot should not exhibit obvious pattern. If there is a nonlinear relationship suggested by by the residual plot, then a simple approach is to include transformed \\(X\\), such as \\(\\log X\\), \\(\\sqrt{X}\\), or \\(X^2\\).\nadditive: The association between \\(X_j\\) and \\(Y\\) is independent of other predictors.\nErrors \\(\\epsilon_i\\) are uncorrelated. This means \\(\\epsilon_i\\) provides no information for \\(\\epsilon_{i+1}\\). Otherwise (for example, frequently observed in a time series, where error terms are positively correlated, and tracking is observed in the residuals, i.e., adjacent error terms take similar values), the estimated standard error will tend to be underestimated, hence leading less confidence in the estimated model.\nHomoscedasticity: \\(\\text{Var}(\\epsilon_i) =\\sigma^2\\). The error terms have constant variance. If not (heteroscedasticity), one may use transformed \\(Y\\), such as \\(\\sqrt{Y}\\), or \\(\\log(Y)\\) to mitigate this; or use weighted least squares if it’s known that for example \\(\\sigma_i^2=\\sigma^2/n_i\\).\nNon-colinearity: two variables are colinear if they are highly correlated with each other. Co-linearity causes a great deal of uncertainty in the coefficient estimates, that is, reducing the accuracy of the coefficient estimates, thus cause the standard error of \\(\\beta_j\\) to grow, and hence smaller \\(t\\)-statistic. As a result, we may fail to reject \\(H_0: \\beta_j=0\\). This in turn means the power of Hypothesis test, the probability of correctly detecting a non-zero coefficient is reduced by colinearity. To detect colinearity,\n\nuse the correlation matrix of predictors. Large value of the matrix in absolute value indicates highly correlated variable pairs. But this approach cannnot detect multicolinearity.\nUse VIF (Variance inflation factor, VIF \\(\\ge 1\\)) to detect multicolinearity. It is possible for colinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This is the multicolinearity situation.\n\nVIF is the ratio of the variance of \\(\\hat{\\beta}_j\\) when fitting the full model divided by the variance of \\(\\hat{\\beta}_j\\) if fit on its own. It can be calculated by \\[\n  \\text{VIF}(\\hat{\\beta}_j) =\\frac{1}{1-R^2_{X_j|X_{-j}}}\n  \\] Where \\(R^2_{X_j|X_{-j}}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors. A VIF value exceeds 5 or 10 (i.e., \\(R^2_{X_j|X_{-j}}\\) close to 1) indicates colinearity.\nTo remedy a colinearity problem:\n\ndrop a redundant variable (variables with colinearity should have similar VIF values. )\nCombine the colinear variables into a single predictor, e.g., taking the average of the standardized versions of those variables.\n\n\nClaims of causality should be avoided for observational data.\n\n\n3.2.2 Assessing existence of linear relationship\n\ntest Hypothesis (test if there is a linear relationship between the response and predictors) \\[\nH_0: \\beta_1=\\beta_2=\\cdots = \\beta_p=0\n\\] \\[\nH_a: \\text{at least one } \\beta_j \\text{ is non-zero.}\n\\] using \\(F\\)-statistic \\[\nF=\\frac{\\text{SSB/df(B)}}{\\text{SSW/df(W)}}=\\frac{(\\text{TSS}-\\text{RSS})/p}{\\text{RSS}/(n-p-1)}\\sim F_{p,n-p-1}\n\\] If \\(H_0\\) is true, \\(F\\approx 1\\); if \\(H_a\\) is true, \\(F&gt;&gt;1\\). \\(F\\)-statistic adjust with \\(p\\). Note that one cannot conclude if an individual \\(t\\)-statistic is significant, then there is at least one predictor is related to the response, especially when \\(p\\) is large. This is related to multiple testing. The reason is that when \\(p\\) is large, there is \\(\\alpha\\) (eg 5%) chance that a predictor will have a small \\(p\\)-value by chance. When \\(p&gt;n\\), \\(F\\)-statistic cannot be used.\n\nIf the goal is to test that a particular subset of \\(q\\) of the coefficients are zero, that is, (for convenience, we put the \\(q\\) variables chosen at the end of the variabale list) \\[\nH_0: \\beta_{p-q+1} = \\beta_{p-q+2}=\\cdots = \\beta_p=0\n\\tag{3.2}\\] In this case, use \\[\nF = \\frac{(\\text{RSS}_0-\\text{RSS})/q}{\\text{RSS}/(n-p-1)}\\sim F_{q,n-p-1}\n\\] where, \\(\\text{RSS}_0\\) is the residual sum of squares of a second model that uses all variables except those last \\(q\\) variables. When \\(q=1\\), \\(F\\)-statistic in Equation 3.2 is the square of the \\(t\\)-statistic of that variable. The \\(t\\)-statistic reported in a regression model gives the partial effect of adding that variable, while holding other variables fixed.\n\n\n3.2.3 Assess the accuracy of the future prediciton\n\nconfidence interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from the population average \\(f(X)\\) because the coefficients \\(\\hat{\\beta}_{j}\\) are estimated, It quantifies reducible error around the predicted average response \\(\\hat{f}(X)\\), does-not include \\(\\epsilon\\).\nprediction interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from \\(Y\\). predict an individual response \\(Y\\approx \\hat{f}(X)+\\epsilon\\). Prediction interval is always wider than the confidence interval, because it includes irreducible error \\(\\epsilon\\).\n\n\n\n3.2.4 Assessing the overall accuracy of the model\n\nRSE. To this end, first define the lack of fit measure Residual Standard Error \\[\n\\text{RSE} = \\sqrt{\\frac{1}{n-p-1}\\text{RSS}} = \\sqrt{\\frac{1}{n-p-1}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2} \\approx \\sigma=\\sqrt{\\text{Var}(\\epsilon)}\n\\] It is the average amount in \\(\\hat{Y}\\) that a response deviates from the true regression line (\\(\\beta_0+\\beta_1 X\\)). Note, RSE can increase with more variables if the decrease of RSS doesnot offset the increase of \\(p\\).\nApproach 2: Using R-squared (fraction of variance in \\(Y\\) explained by \\(X\\)), which is independent of of the scale of \\(Y\\), and \\(0\\le R^2 \\le 1\\): \\[\nR^2 =\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}} = 1-\\frac{\\text{RSS}}{\\text{TSS}}\n\\] where, \\(\\text{TSS}=\\sum_{i=1}^n(y_i- \\bar{y})\\). When \\(R^2\\) is near 0 indicates that 1) either the linear model is wrong 2) or th error variance \\(\\sigma^2\\) is high, or both. \\(R^2\\) measures the linear relationship between \\(X\\) and \\(Y\\). If computed on the training set, when adding more variables, the RSS always decrease, hence \\(R^2\\) will always increase.\n\nFor simple linear regression, \\(R^2=r_{xy}^2\\), where the sample correlation measures the linear relationship between variables \\(X\\) and \\(Y\\). See the formula \\(r_{xy}\\) above Equation 3.1. For multiple linear regression, \\(R^2=(\\text{Cor}(Y, \\hat{Y}))^2\\). The fitted linear model maximizes this correlation among all possible linear models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "href": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.3 Model Selection/Variable Selections: balance training errors with model size",
    "text": "3.3 Model Selection/Variable Selections: balance training errors with model size\n\nAll subsets (best subsets) regression: compute the least square fit for all \\(2^p\\) possible subsets and then choose among them based on certain criterion that balance training error and model size\nForward selection: Start from the null model that only contains \\(\\beta_0\\). Then find the best model containing one predictor that minimizing RSS. Denote the variable by \\(\\beta_1\\). Then continue to find the best model with the lowest RSS by adding one variable from the remaining predictors, and so on. Continue until some stopping rule is met: e.g., when all remaining variables have a \\(p\\)-value greater than some threshold.\nBackward selection: start with all variables in the model. Remove the variable with the largest \\(p\\)-value (least statistically significant). The new \\((p-1)\\) model is fit, and remove the variable with the largest \\(p\\)-value. Continue until a stopping rule is satisfied, e.g., all remaining variables have \\(p\\)-value less than some threshold.\nMixed selection: Start with forward selection. Since the \\(p\\)-value for variables can become larger as new predictors are added, at any point if the \\(p\\)-value of a variable in the model rises above a certain threshold, then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low \\(p\\)-value, and all variables outside the model would have a large \\(p\\)-value if added to the model.\nBackward selection cannot be used if \\(p&gt;n\\). Forward selection can always be used, but might include variables early that later become redundant. Mixed selection can remedy this problem.\nothers (Chapter 6): including Mallow’s \\(C_p\\), AIC (Akaike Informaton Criterion), BIC, adjusted \\(R^2\\), Cross-validation, test set performance.\nnot valid: we could look at individual \\(p\\)-values, but when the number of variables \\(p\\) is large, we likely to make a false discoveries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#handle-categorical-variables-factor-variables",
    "href": "ch3.html#handle-categorical-variables-factor-variables",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.4 Handle categorical variables (factor variables)",
    "text": "3.4 Handle categorical variables (factor variables)\nFor a categorical variable \\(X_i\\) with \\(m\\) levels, create one fewer dummy variables (\\(x_{ij}, 1\\le j \\le m-1\\))&gt;. The level with no dummy variable is called the baseline. The coefficient corresponding to a dummy variable is the expected difference in change in \\(Y\\) when compared to the baseline, while holding other predictors fixed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#adding-non-linearity",
    "href": "ch3.html#adding-non-linearity",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.5 Adding non-linearity",
    "text": "3.5 Adding non-linearity\n\n3.5.1 Modeling interactions (synergy)\nWhen two variables have interaction, then their product \\(X_iX_j\\) can be added into the regression model, and the product maybe considered as a single variable for inference, for example, compute its SE, \\(t\\)-statistics, \\(p\\)-value, Hypothesis test, etc.\nIf we include an interaction in a model, then the Hierarchy principle should be followed: always include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant. This is because without the main effects, the interactions are hard to interpret, as they would also contain the main effect.\n\n\n3.5.2 Adding terms of transformed predictors\n\nPolynomial regression: Add a term involving \\(X_i^k\\) for some \\(k&gt;1\\).\nother forms: Adding root or logarithm terms of the predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "href": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))",
    "text": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))\nIt is typical for an outlier that does not have an unusual predictor value (with low levarage) to have little effect on the least squares fit, but it will increase RSE, hence deteriorate CI, \\(p\\)-value and \\(R^2\\), thus affecting interpreting the model.\nAn outlier can be identified by computing the \\[\\text{studentized residual}=\\frac{e_i}{\\text{RSE}_i}\\] A studentized residual great than 3 may be considered as an outlier.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#high-leverage-points-unusual-x_i",
    "href": "ch3.html#high-leverage-points-unusual-x_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.7 High leverage points (unusual \\(x_i\\))",
    "text": "3.7 High leverage points (unusual \\(x_i\\))\nHigh leverage points tend to have sizeable impact on the regression line. To quantify the observation’s leverage, one needs to compute the leverage statistic \\[h_i = \\frac{1}{n}+ \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n (x_j-\\bar{x})^2}.\\] \\(1/n \\le h_i\\le 1\\) and \\(\\text{Ave}(h_i)=(p+1)/n\\). A large value of this statistic (for example, great than \\((p+1)/n\\)) indicates an observation with high leverage. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#compared-to-knn-regression",
    "href": "ch3.html#compared-to-knn-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.8 Compared to KNN Regression",
    "text": "3.8 Compared to KNN Regression\nKNN regression is a non-parametric method that makes prediction at \\(x_0\\) by taking the average in a \\(K\\)-point neightborhood \\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_{x_0}}{y_i}\n\\] A small value of \\(K\\) provides more flexible model with low bias but high variance while a larger value of \\(K\\) provides smoother fit with less variance. An optimal value of \\(K\\) depend on the bias-variance tradeoff. For non-linear data set, KNN may provides better fit than a linear regression model. However, in higher dimension (e.g., \\(p\\ge 4\\)), even for nonlinear data set, KNN may perform much inferior to linear regression, because of the curse of dimensionality, as the \\(K\\) observations that are nearest to \\(x_0\\) may in fact far away from \\(x_0\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#homework-indicates-optional",
    "href": "ch3.html#homework-indicates-optional",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.9 Homework (* indicates optional):",
    "text": "3.9 Homework (* indicates optional):\n\nConceptual: 1–6\nApplied: 8–15. at least one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#code-gist",
    "href": "ch3.html#code-gist",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.10 Code Gist",
    "text": "3.10 Code Gist\n\n3.10.1 Python\ndir() # provides a list of objects at the top level name space\ndir(A) # display addtributes and methods for the object A\n' + '.join(X.columns) # form a string by joining the list of column names by \"+\"\n\n\n3.10.2 Numpy\nnp.argmax(x) # identify the location of the largest element\nnp.concatenate([x,y],axis=0) # concatenate two arrays x and y. \n\n\n\n3.10.3 Pandas\nX = pd.DataFrame(data=X, columns=['a','b'])\n\npd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']}) # make a dataframe using a dictionary\nBoston.columns.drop('medv','age') # drop the elements 'medv' and 'age' from the list of column names\n\npd.DataFrame({'vif':vals},\n                   index=X.columns[1:]) # form a df by specifying index labels\n\nX.values  # Convert dataframe X to numpy array\nX.to_numpy() # recommended to replace the above method\nDataFrame.corr(numeric_only=True) # correlations between columns \nx.sort_values(ascending=False)\npd.to_numeric(auto_df['horsepower'], errors='coerce') # if error, denote it by \"NaN\".\nauto_df.dropna(subset= ['horsepower', 'mpg',], inplace=True) # looking for NaN in the columns in `subset`, otherwise, all columns\n\nauto_df.drop('name', axis=1, inplace=True)\n\nleft2.join(right2, how=\"left\") #join two databases by index. \nleft1.join(right1, on=\"key\") # left-join by left1[\"key\"] and the index of right1. \npd.concat([s1, s4], axis=\"columns\", join=\"outer\")\n\n\n\n3.10.4 Graphics\nxlim = ax.get_xlim() # get the x_limit values xlim[0], xlim[1]\nax.axline() # add a line to a plot\nax.axhline(0, c='k', ls='--'); # horizontal line\nline, = ax.plot(x,y,label=\"line 1\") # \"line 1\" is the legend\n# alternatively the label can be set by \nline.set_label(\"line 1\")\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.legend(loc=\"upper left\", fontsize=25) # adding legendes\nax.annotate(i,xy=(fitted[i],residuals[i])) # annote at the xy position with i. \n\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rcParams.update({'font.size': 16})\nplt.rcParams[\"figure.figsize\"] = (8,7)\n\nplt.rc('font', size=10)\nplt.rc('figure', titlesize=13)\nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=13)\nplt.rc('legend', fontsize=8) # adjust legend globally\n    \n\n\n3.10.5 Using Sns\nsns.set(font_scale=1.25) # set font size 25% larger than default\nsns.heatmap(corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})\nax = sns.regplot(x=x, y=y)\n\n\n3.10.6 Using Sklearn\nfrom sklearn.linear_model import LinearRegression\n## Set the target and predictors\nX = auto_df['horsepower']\n\n### To get polynomial features\npoly = PolynomialFeatures(interaction_only=True,include_bias = False)\nX = poly.fit_transform(X)\n\ny = auto_df['mpg']\n\n## Reshape the columns in the required dimensions for sklearn\nlength = X.values.shape[0]\nX = X.values.reshape(length, 1) #both X and y needs to be 2-D\ny = y.values.reshape(length, 1)\n\n## Initiate the linear regressor and fit it to data using sklearn\nregr = LinearRegression()\nregr.fit(X, y)\nregr.intercept_\nregr.coef_\n\npred_y = regr.predict(X)\n\n\n3.10.7 Using statsmodels and ISLP\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n                         \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\n#Training\nBoston = load_data(\"Boston\") \n#hand-craft the design matrix X\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), #design matrix. intercept column\n                  'lstat': Boston['lstat']}) \n#the following is the preferred method to create X\ndesign = MS(['lstat']) # specifying the model variables. Automatically add an intercept, adding \"intercept=False\" if no intercept. \ndesign = design.fit(Boston) # do intial computation as specified in the model object design by MS(), such as means or sd. This attached some statistics to the `design` object, and need to be applied to the new data for prediciton\n\nX = design.transform(Boston) # apply the fitted transformation to the data to create X\n#alternatiely, \nX = design.fit_transform(Boston) # this combines the .fit() and .transform() two lines\n\ny = Boston['medv']\nmodel = sm.OLS(y, X) # setup the model\nmodel = smf.ols('mpg ~ horsepower', data=auto_df) # alternatively use smf formula, y~x\nsmf.ols(\"y ~ x -1\" , data=df).fit() # \"-1\" not inclding the intercept\nresults = model.fit() # results is a dictionary:.summary(), .params \n\nresults.summary()\nresults.params # coefficients\nresults.resid # reisdual array\nresults.rsquared # R^2\nresults.pvalues\nnp.sqrt(results.scale) # RSE\nresults.fittedvalues # fitted \\hat(y)_i at x_i in the traning set\n\n\nsummarize(results) # summzrize() is from ISLP to show the esstial results from model.fit()\n\n# Makding prediciton \nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})  # new test-set containing data where to make predicitons\nnewX = design.transform(new_df) # apply the same transform to the test-set\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean #predicted values\nnew_predictions.conf_int(alpha=0.05) #for the predicted values\n\nnew_predictions.conf_int(obs=True, alpha=0.05) # prediction intervals by setting obs=True\n\n# Including an interaction term\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston) #interaction term ('lstat', 'age')\n\n# Adding a polynomial term of higher degree\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston) # Note poly is from ISLP, # adding deg1 and deg2 terms. by default poly creates ortho. poly. not including an intercept. \n# Given a qualitative variable, `ModelSpec()` generates dummy\nvariables automatically, to avoid collinearity with an intercept, the first column is dropped in the design matrix generated by 'ModelSpec()` by default.\n\n# Compare nested models using ANOVA\nanova_lm(results1, results3) # result1 is the result of linear model, an result3 is the result of a larger model\n\n# Identify high leverage x\ninfl = results.get_influence() \n# hat_matrix_diag calculate the leverate statistics\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage\n\n# Calculate VIF\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])] #excluding column 0 because it's all 1's in X.\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif # VIF exceeds 5 or 10 indicates a problematic amount of colinearity\n\nUseful Code Snippets\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n# Plot scatter plot with a regression line\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n# Plot residuals vs. fitted values (note, not vs x, therefore works for multiple regression)\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n# Alternatively\nsns.residplot(x=X, y=y, lowess=True, color=\"g\", ax=ax)\n\n# Plot the smoothed residuals~fitted by LOWESS\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nsmoothed = lowess(residuals,fitted) # Note the order (y,x)\nax.plot(smoothed[:,0],smoothed[:,1],color = 'r')\n\n# QQ plot for the residuas (obtain studentized residuals for identifying outliers)\nimport scipy.stats as stats\nsorted_student_residuals = pd.Series(smf_model.get_influence().resid_studentized_internal)\nsorted_student_residuals.index = smf_model.resid.index\nsorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)\ndf = pd.DataFrame(sorted_student_residuals)\ndf.columns = ['sorted_student_residuals']\n\n#stats.probplot() #assess whether a dataset follows a specified distribution\ndf['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0] \n    \nx = df['theoretical_quantiles']\ny = df['sorted_student_residuals']\nax.scatter(x,y, edgecolor = 'k',facecolor = 'none')\n\n# Plot leverage statistics\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch4.html#linear-regression-and-classification",
    "href": "ch4.html#linear-regression-and-classification",
    "title": "4  Chapter 4: Classification",
    "section": "4.1 Linear regression and Classification",
    "text": "4.1 Linear regression and Classification\n\nFor a binary classification, one can use linear regression and does a good job. In this case, the linear regression classifier is equivalent to LDA, because \\[\nP(Y=1|X=x)= E[Y|X=x]\n\\] However, linear regression may not represent a probability as it may give a value outside the interval \\([0,1]\\).\nWhen there are more than two classes, linear regression is not appropriate, because any chosen coding of the \\(Y\\) variable imposes an ordering and fixed differences among categories, which may not be implied by the data set. If the coding changes, a dramatic function will be fitted, which is not reasonable. One should turn to multiclass logistic regression or Discriminant Analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#logistic-regression",
    "href": "ch4.html#logistic-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nLogistic regression is a discriminative learning, because it directly calculates the conditional probability \\(P(Y|X)\\) to make classification.\n\n4.2.1 Binary classification\nwith a single variable Logistic regression simply convert the linear regression to probability by \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\beta_0+\\beta_1 X}}{1+ e^{\\beta_0+\\beta_1X}}.\n\\] Note the logit or log odds is linear \\[\n\\log\\left( \\frac{p(X)}{1-p(X)}  \\right) =\\beta_0 +\\beta_1 X.\n\\] Increasing \\(X\\) by one unit, changes the log odds by \\(\\beta_1\\). Equivalently, it multiplied the odds by \\(e^{\\beta_1}\\). The rate of change of \\(p(X)\\) is no longer a constant, but depends on the current value of \\(X\\). Positive \\(\\beta_1\\) implies increasing \\(p(X)\\), and vice vesa.\nThe parameters are estimated by maximizing the liklihood \\[\n\\ell(\\beta_0, \\beta_1) =\\prod_{i: y_i=1}p(x_i) \\prod_{i:y_i=0}(1-p(x_i))\n\\] With the estimated parameters \\(\\hat{\\beta_j}, j=0,1\\), one can calculate the probability \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1} X}}{1+ e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}\n\\]\n\n\n4.2.2 with multiple variables\nIn this case, simply let the logit be a linear function of \\(p\\) variables.\nNote when there are multiple variables, it’s possible to have variables confounding (especially when two variables are correlated): the coefficient of a variable may changes significantly or may change sign, this is because the coefficient represents the rate of change in \\(Y\\) of that variable when holding other variable constants. The coefficient reflects the effect when other variables are hold constant, how the variable affects \\(Y\\), and this effect may be different than when only this variable is used in the model.\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the logit model, similar to a linear regression that includes a non-linear term.\n\n\n\n\n4.2.3 Multi-class logistic regression (multinomial regression) with more than two classes\nin this case, we use the softmax function to model \\[\n\\text{Pr} (Y=k|X) =\\frac{e^{\\beta_{0k}+\\beta_{1k}X_1+ \\cdots + \\beta_{pk}X_p}}{\\sum_{\\ell=1}^K e^{\\beta_{0\\ell}+\\beta_{1\\ell}X_1+ \\cdots + \\beta_{p\\ell}X_p}} =a_k\n\\] for each class \\(k\\). Note \\(\\Sigma_k a_k=1\\) and the cross-entropy loss function is given by \\(-\\log \\ell(\\beta)= -\\Sigma_k \\mathbb{1}_k \\log a_k\\), where \\(\\beta\\) represents all the parameters.\nThe log odds between \\(k\\)th and \\(k'\\)th classes equals \\[\n\\log(\\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=k'|X=x)})=(\\beta_{k0}-\\beta_{k'0}) + (\\beta_{k1}-\\beta_{k'1}) + \\cdots + (\\beta_{kp}-\\beta_{k'p})\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "href": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier",
    "text": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier\nApply the Bayes Theorem, the model \\[\n\\text{Pr}(Y=k|X=x)=\\frac{\\text{Pr}(X=x|Y=k)\\cdot \\text{Pr}(Y=k)}{\\text{Pr}(X=x)}=\\frac{\\pi_k f_k(x)}{\\sum_{\\ell =}^ K \\pi_{\\ell}f_\\ell(x)}\n\\] where \\(\\pi_k=\\text{Pr(Y=k)}\\) is the marginal or prior probability for class \\(k\\), and \\(f_k(x)=\\text{Pr}(X=x|Y=k\\)) is the density for \\(X\\) in class \\(k\\). Note the denominator is a normalizing constant. So when making decisions, effectively we compare \\(\\pi_kf_k(x)\\), and assign \\(x\\) to a class \\(k\\) with the largest \\(\\pi_kf_k(x)\\).\nDiscriminant uses the full liklihood \\(P(X,Y)\\) to calculate \\(P(Y|X)\\) to make a classification, so it’s known as generative learning.\n\nwhen \\(f_k\\) is chosen as a normal distribution with constant variance (\\(\\sigma^2\\)) for \\(p=1\\) or correlation matrix \\(\\Sigma\\) for \\(p&gt;1\\), this leads to the LDA. For \\(p=1\\), the discriminant score is given by \\[\n\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log (\\pi_k)\n\\] when \\(K=2\\) and \\(\\pi_1=\\pi_2=0.5\\)m then the decision boundary is given by \\[\nx=\\frac{\\mu_1+\\mu_2}{2}.\n\\] When \\(p\\ge 2\\), assume that \\(X=(X_1, X_2, \\cdots, X_p)\\) is drawn from a multivariate Gaussian distribution \\(X \\sim N(\\mu_k, \\Sigma)\\), with a class-specific mean vector and a a common variance matrix. \\[\n\\delta_k(x) =x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k +\\log \\pi_k=c_{k0}+c_{k1}x_1+\\cdots +c_{kp}x_p.\n\\] The score function (posterior probability) is linear in \\(x\\). With \\(\\hat{\\delta}_k(x)\\) for each \\(k\\), it can be converted to the class probability by the softmax function \\[\n\\hat{\\text{Pr}}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{\\ell=1}^K e^{\\hat{\\delta}_{\\ell}(x)}}\n\\] The \\(\\pi_k\\), \\(\\mu_k\\) and \\(\\sigma\\) are estimate the follwing way: \\[\n\\hat{\\pi}_k =\\frac{n_k}{n}\n\\] \\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i:y_i=k} x_i\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2=\\sum_{k=1}^{K}\\frac{n_k-1}{n-K}\\hat{\\sigma}^2_k\n\\] where \\(\\hat{\\sigma}_k^2=\\frac{1}{n_k-1} \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\) is the estimated variance for the \\(k\\)-th class.\n\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the LDA model, similar to a linear regression that includes a non-linear term.\n\n\n\nwhen each class chooses a different \\(\\Sigma_k\\), then it’s QDA. It assumes an observation from the \\(k\\)-th class is \\(X\\sim N(\\mu_k, \\Sigma_k)\\).The score function has a quadratic term \\[\n\\delta_k(x)=-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+\\log \\pi_k -\\frac{1}{2}\\log |\\Sigma_k|\n\\] QDA has much more parameters \\(Kp(p+1)/2\\) to estimate compared to LDA (\\(Kp\\)), hence has higher flexibility and may lead to higher variance. When there are few training examples, LDA tend to perform better and reducing variance is crucial. When there is a large traning set, QDA is recommended as variance is not a major concern. LDA is a special case of QDA.\nwhen the features are modeled independently, i.e., there is no association between the \\(p\\) predictors, \\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\), the method is naive Bayes, and \\(\\Sigma_k\\) are diagonal. Any classifier with a linear decision boundary is a special case of NB. So LDA is a special case of NB. To estimate \\(f_kj\\), one can\n\nassume that \\(X_j|Y=k \\sim N(\\mu_{jk,\\sigma^2_{jk}})\\), that is, a class specific covariance but is diagonal. QDA’s \\(\\Sigma_k\\) is not diagonal. If we model \\(f_{kj}(x_j)\\sim N(\\mu_{kj}+\\sigma_j^2)\\) (Note \\(\\sigma_j^2\\) is shared among clases), In this case NB is a special case of LDA that has a diagonal \\(\\Sigma\\) and\nuse a non-parametric estimate such as histogram (or a smooth kernel density estimator) for the observations of the jth Predictor within each class.\nIf \\(X_j\\) is qualitative, then one can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\nCan applied to mixed feature vectors (qualitative and quantitative). NB does not assume normally distributed predictors.\nDespite strong assumptions, performs well, especially when \\(n\\) is not large enough relative to \\(p\\), when estimating the joint distribution is difficult. It introduces some biases but reduces variance, leading to a classifier that works quite well as a result of bias-variance trade-off.\nUseful when \\(p\\) is very large.\nNB is a generalized additive model.\nNeigher NB nor QDA is a special case of the other. Because QDA contains interaction term \\(x_ix_j\\), while NB is purely additive, in the sense that a function of \\(x_i\\) is added to a function of \\(x_j\\). Therefore, QDA potentially is a better fit when the interactions among predictors are important.\n\n\n\n4.3.1 Why discriminant analysis\n\nWhen the classes are well-separated, the parameter estimation of logistic regression is unstable, while LDA does not suffer from this problem.\nif the data size \\(n\\) is small and the distribution of \\(X\\) is approximately normal in each of the classes, then LDA is more stable than logistic regression. Also used when \\(K&gt;2\\).\nwhen there are more than two classes, LDA provides low-dimensional views of the data hence popular. Specifically, when there are \\(K\\) classes, LDA can be viewed exactly in \\(K-1\\) dimensional plot. This is because it essentially classifies to the closest centroid, and they span a \\(K-1\\) dimensional plane.\nFor a two-class problem, the logit of \\(p(Y=1|X=x\\)) by LDA (generative learning) is a linear function in \\(X\\), the same as a logistic regression (discriminative learning). The difference lies in how the parameters are estimated. But in practice, they are similar.\nLDA assumes the predictors follow a multivariable normal distribution with a shared \\(\\Sigma\\) among classes. So when this assumption holds, we expect LDA performs better; and Logistic regress performs better when this asuumption does not hold.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#knn",
    "href": "ch4.html#knn",
    "title": "4  Chapter 4: Classification",
    "section": "4.4 KNN",
    "text": "4.4 KNN\nKNN is a non-parametric method and doesnot assume a shape for the decision boundary. KNN assign the class of popularity to \\(X=x\\) in a \\(K\\)-neighborhood.\n\nKNN dominates LDA and Logistic Regression when the decision boundary is highly non-linear, provided \\(n\\) is large and \\(p\\) is small. As KNN breaks down when \\(p\\) is large.\nKNN requires large \\(n&gt;&gt;p\\), this is because KNN is non-parametric and tends to reduce bias but increase variance.\nWhen the decision boundary is non-linear but \\(n\\) is only modest and \\(p\\) is not very small, QDA may outperform KNN. This is because QDA provides a non-linear boundary while taking advantage of a parametric form, which means that if requires smaller size for accurate classification.\nUnlike logistic regression, KNN does not tell which predictors are more importnat: We dont get a table of coefficients.\nWhen the decision boundary is linear, LDA or logistic regression may perform better, when the boundary is moderately non-linear, QDA or NB may perform better; For a much more complicated decision boundary, KNN may perform better.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#poisson-regression",
    "href": "ch4.html#poisson-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.5 Poisson Regression",
    "text": "4.5 Poisson Regression\nWhen \\(Y\\) is discrete and non-negative, a linear regression model is not satisfactory, even with the transformation of \\(\\log (Y)\\), because \\(\\log\\) does not allow \\(Y=0\\).\n\nPoisson Regression: typically used to model counts, \\[\n\\text{Pr}(Y=k)= \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\qquad k=0,1,2, \\cdots,\n\\] where, \\(\\lambda = E(Y)= \\text{Var}(Y)\\). This means that if \\(Y\\) follows a Poissson distribution, the larger the mean of \\(Y\\), the larger its variance. Posisson regression can handle this when variance changes with mean, but linear regression cannot, because it assumes constant variance.\n\nAssume \\[\n\\log(\\lambda(X_1, X_2, \\cdots, X_p))=\\beta_0+\\beta_1X_1+\\cdots +\\beta_pX_p\n\\] Then one can use maximum likelihood \\[\n\\ell(\\beta_0, \\beta_1, \\cdots, \\beta_p)=\\prod_{i=1}^n \\frac{e^{-\\lambda(x_i)}\\lambda(x_i)^{y_i}}{y_i!}\n\\] to estimate the parameters.\n\nInterpretation: An increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y)=\\lambda\\) by a factor (percentage) of \\(\\exp(\\beta_j)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#generalized-linear-models-glm",
    "href": "ch4.html#generalized-linear-models-glm",
    "title": "4  Chapter 4: Classification",
    "section": "4.6 Generalized Linear Models (GLM)",
    "text": "4.6 Generalized Linear Models (GLM)\nPerform a regression by modeling \\(Y\\) from a particular member of the exponential family (Gaussian, Bernoulli, Poisson, Gamma, negative binomial), and then transform the mean of \\(Y\\) to a linear function.\n\nUse predictors \\(X_1, \\cdots, X_p\\) to predict \\(Y\\). Assume \\(Y\\) conditional on \\(X\\) follow some distribution: For linear regression, assume \\(Y\\) follows a normal distribution; for logistic regression, assume \\(Y\\) follows a Bernoulli (multinomial distribution for multi-class logistic regression) distribution; For poisson distribution, assume \\(Y\\) follows a poisson distribution.\nEach approach models the mean of \\(Y\\) as a function of \\(X\\) using a linking function \\(\\eta\\) to transform \\(E[Y|X]\\) to a linear function.\n\nfor linear regression \\[\nE(Y|X)= \\beta_0+\\beta_1 X_1+\\cdots +\\beta_p X_p\n\\] \\(\\eta(\\mu) =\\mu\\)\nfor logistic regression \\[\nE(Y|X)=P(Y=1|X)=\\frac{e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}\n\\] \\(\\eta(\\mu) = \\log (\\mu/(1-\\mu))\\)\nfor Poisson regression \\[\nE(Y|X) = \\lambda(X) = e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}\n\\] \\(\\eta(\\mu) = \\log(\\mu)\\).\nGamma regression and negative binomial regression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#assessment-of-a-classifier",
    "href": "ch4.html#assessment-of-a-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.7 Assessment of a classifier",
    "text": "4.7 Assessment of a classifier\n\nConfusion matrix\nOverall error rate: equals to \\[\n\\frac{FP+FN}{N+P}\n\\]\nClass-specific performance: One can adjust the decision boundary (posterior probability threshold) to improve class specific performance at the expense of lowered overall performance.\n\npercentage of TP detected among all positives\n\\[\\text{sensitivity (recall, power)} = TPR = \\frac{TP}{TP+FN}=\\frac{TP}{P}= 1-\\text{Type II error}=1-\\beta\\] this is equal to \\(1- FNR\\), where, FNR is The fraction of positive examples that are classified as negatives \\[\nFNR = \\frac{FN}{FN+TP}=\\frac{FN}{P}\n\\]\npercentage of TN detected among all negatives \\[\\text{specificity}= TNR = \\frac{TN}{TN+FP}=\\frac{TN}{N}\\] This is equal to \\(1-FPR\\), where, False positive rate (FPR): the fraction of negative examples (N) that are classified as positive: \\[\nFPR=\\frac{FP}{FP+TN}=\\frac{FP}{N} = \\text{Type I error} (\\alpha)\n\\]\nROC (receiver operating characteristic curve): plot true positive rate (TPR=1-Type II error) ~ false positive rate (FPR= 1- specificity=Type I error) as a threshold for the posterior probability of positive class changes from 0 to 1. The point on the ROC curve closest to the point (0,1) corresponds to the best classifier.\nAUC (area under the ROC): Overall performance of a classifier summarized over all thresholds. AUC measures the probability a random positive example is ranked higher than a random negative example. A larger AUC indicates a better classifier. More specifically, define the score function for th e \\(i\\)-th observation by \\(Pr(Y=1|X=x_i)\\). Consider all pairs consisting of one observation in Class 1 and one observation in Class 0, then the AUC is the fraction (probability) of pairs for which the score for the observation in Class 1 exceeds the score for the observations in Class 0.\n\nclass-specific prediction performance\n\n\\[\\text{precision} = \\frac{TP}{TP+FP}=\\frac{TP}{\\text{predicted postives}}=1-\\text{false discovery proportion}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#homework",
    "href": "ch4.html#homework",
    "title": "4  Chapter 4: Classification",
    "section": "4.8 Homework:",
    "text": "4.8 Homework:\n\nConceptual: 1,2,3,4, 5,6,7,8, 9, 10, 12\nApplied: 13, 14*,15*,16*",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#code-gist",
    "href": "ch4.html#code-gist",
    "title": "4  Chapter 4: Classification",
    "section": "4.9 Code Gist",
    "text": "4.9 Code Gist\n\n4.9.1 Python\n\n\n4.9.2 Numpy\nnp.where(lda_prob[:,1] &gt;= 0.5, 'Up','Down')\nnp.argmax(lda_prob, 1) #argmax along axis=1 (col)\nnp.asarray(feature_std) # convert to np array\nnp.allclose(M_lm.fittedvalues, M2_lm.fittedvalues) \n#check if corresponding elts are equal within rtol=1e-5 and atol=-1e08\n\n\n4.9.3 Pandas\nSmarket.corr(numeric_only=True)\ntrain = (Smarket.Year &lt; 2005)\nSmarket_train = Smarket.loc[train] # equivalent to Smarket[train]\nPurchase.value_counts() # frequency table\nfeature_std.std() #calculate column std\nS2.index.str.contains('mnth')\nBike['mnth'].dtype.categories # get the categories of the categorical data\nobj2 = obj.reindex([\"a\", \"b\", \"c\", \"d\", \"e\"])# rearrange the entries in obj according to the new index, introducing missing values if any index values were not already present. \n\n\n4.9.4 Graphics\nax_month.set_xticks(x_month) # set_xticks at the place given by x_month\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize=20)\nax.axline([0,0], c='black', linewidth=3,  \n          linestyle='--', slope=1);#axline method draw a line passing a given point with a given slope. \n\n\n4.9.5 ISLP and Statsmodels\nfrom ISLP import confusion_table\nfrom ISLP.models import contrast\n\n# Logistic Regression using sm.GLM() syntax similar to sm.OLS()\ndesign = MS(allvars)\nX = design.fit_transform(Smarket)\ny = Smarket.Direction == 'Up'\nglm = sm.GLM(y,\n             X,\n             family=sm.families.Binomial())\nresults = glm.fit()\nsummarize(results)\nresults.pvalues\nprobs = results.predict() #without data set, calculate predictions on the training set. \nresults.predict(exog=X_test) # on test set\n# Prediction on a new dataset\nnewdata = pd.DataFrame({'Lag1':[1.2, 1.5],\n                        'Lag2':[1.1, -0.8]});\nnewX = model.transform(newdata)\nresults.predict(newX)\nconfusion_table(labels, Smarket.Direction) #(predicted_labels, true_labels)\nnp.mean(labels == Smarket.Direction) # calculate the accuracy\n\nhr_encode = contrast('hr', 'sum') #coding scheme for categorical data: the unreported coefficient for the missing level equals to the negative ofthe sum of the coefficients of all other variables. In this a coefficient for a level may be interpreted as the differnece from the mean level of response. \n\n#Poisson Regression \nM_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()\n#`family=sm.families.Gamma()` fits a Gamma regression\nmodel.\n\n\n\n4.9.6 sklearn\nfrom sklearn.discriminant_analysis import \\\n     (LinearDiscriminantAnalysis as LDA,\n      QuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n#LDA\nlda = LDA(store_covariance=True) #store the covariance of each class\nX_train, X_test = [M.drop(columns=['intercept']) # drop the intercept column\n                   for M in [X_train, X_test]]\nlda.fit(X_train, L_train) # LDA() model will automatically add a intercept term\n\nlda.means_ # mu_k (n_classes, n_features)\nlda.classes_\nlda.priors_ # prior probability of each class\n#Linear discrimnant vectors\nlda.scalings_ #Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.\n\nlda_pred = lda.predict(X_test) #predict class labels\nlda_prob = lda.predict_proba(X_test) #ndarray of shape (n_samples, n_classes)\n\n#QDA\nqda = QDA(store_covariance=True)\nqda.fit(X_train, L_train)\nqda.covariance_[0] #estimated covariance for the first class\n\n# Naive Bayes\nNB = GaussianNB()\nNB.fit(X_train, L_train)\nNB.class_prior_\nNB.theta_ #means for (#classes, #features)\nNB.var_ #variances (#classes, #features)\nNB.predict_proba(X_test)[:5]\n\n# KNN\nknn1 = KNeighborsClassifier(n_neighbors=1)\nX_train, X_test = [np.asarray(X) for X in [X_train, X_test]]\nknn1.fit(X_train, L_train)\nknn1_pred = knn1.predict(X_test)\n\n# When using KNN one should standarize each varaibles\nscaler = StandardScaler(with_mean=True,\n                        with_std=True,\n                        copy=True) # do calculaton on a copy of the dataset\nscaler.fit(feature_df)\n\n#train test split\nX_std = scaler.transform(feature_df)\n(X_train,\n X_test,\n y_train,\n y_test) = train_test_split(np.asarray(feature_std),\n                            Purchase,\n                            test_size=1000,\n                            random_state=0)\n\n# Logistic Regression\nlogit = LogisticRegression(C=1e10, solver='liblinear') #use solver='liblinear'to avoid warning that the alg doesnot converge.  \nlogit.fit(X_train, y_train)\nlogit_pred = logit.predict_proba(X_test)\n\n\n\n\n4.9.7 Useful code snippet\n# Tuning KNN\nfor K in range(1,6):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn_pred = knn.fit(X_train, y_train).predict(X_test)\n    C = confusion_table(knn_pred, y_test)\n    templ = ('K={0:d}: # predicted to rent: {1:&gt;2},' +  # &gt; for right alighment\n            '  # who did rent {2:d}, accuracy {3:.1%}')\n    pred = C.loc['Yes'].sum()\n    did_rent = C.loc['Yes','Yes']\n    print(templ.format(\n          K,\n          pred,\n          did_rent,\n          did_rent / pred))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch5.html#how-to-estimate-test-error",
    "href": "ch5.html#how-to-estimate-test-error",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.1 how to estimate test error",
    "text": "5.1 how to estimate test error\n\nuse a large designated test set, but often not available.\nmake adjustment to the training error to estimate the test error, e.g., Cp statistic, AIC and BIC.\nvalidation set approach: estimate the test error by holding out a subset of the training set, also called a validation set.\n\nthe estimate of the test error can be highly variable, depending on the random train-validation split.\nOnly a subset of the training set is used to fit the model. Since statistical methods tend to perform worse when trained on a smaler data set, which suggests the validation error tends to overestimate the test error compared to the model that uses the entire training set.\n\nK-fold Cross-Validation: randomly divide the data into \\(K\\) equal-sized parts \\(C_1, C_2, \\cdots, C_K\\). For each \\(k\\), leave out part \\(k\\), fit the model on the remaining \\(K-1\\) parts (combined, \\((K-1)/K\\) of the original traning set), and then evaluate the model on the part \\(k\\). Then repeat this for each \\(k\\), and weighted average of the errors is computed: \\[\nCV_{(K)} = \\sum_{k=1}^K \\frac{n_k}{n}\\text{MSE}_k\n\\] where \\(\\text{MSE}_k=\\sum_{i\\in C_k}(y_i\\ne \\hat{y}_i)/n_k\\).\n\nFor classification problem, simply replace \\(\\text{MSE}_k\\) with the misclassificaiton rate \\(\\text{Err}_k =\\sum_{i\\in C_k}I(y_i\\ne \\hat{y}_i)/n_k\\).\nThe estimated standard error of \\(CV_k\\) can be calculated by \\[\n\\hat{\\text{SE}}(CV_k)=\\sqrt{\\frac{1}{K}\\sum_{k=1}^K\\frac{(\\text{Err}_k-\\overline{\\text{Err}_k})^2}{K-1}}\n\\]\nThe estimated error tends bias upward because it uses only \\((K-1)/K\\) of the training set. This bias is minimized with \\(K=n\\) (LOOCV), but LOOCV estimate has high variance due to the high correlation between folds.\n\nLOOCV: it’s a special case of K-fold CV with \\(K=n\\). For least squares linear or polynomial regression, the LOOCV error can be computed by \\[\n\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{y_i-\\hat{y}_i}{1-h_i} \\right)^2\n\\] Where \\(h_i\\) is the leverage statistic of \\(x_i\\). There is no randomness in the error. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit. The above formula doesn’t hold in genearl, in which case the model has to refit \\(n\\) times to estimate the test error.\nfor LOOCV, the estimate from each fold are highly correlated, hence their average can have high variance.\nbetter choice is \\(K=5\\) or \\(K=10\\) for bias-variance trade-off, because large \\(k\\) leads to low bias but high variance due to the increased correlation between models. Despite the estimated test error sometimes underestimate the true test error, they then to be close to identify the correct flexibility where the test error is minimum.\nBootstrap: Primarily used to estimate the standard error, or a CI (called bootstrap percentile) of an estimate . Repeatedly sampling the training set with replacement and obtain a bootstrap set of the the same size as the original training set. One can fit a model and estimate a parameter with each bootstrap data set, and then estimate the standard error using the estimated parameters by the bootstrap model, assuming there are \\(B\\) bootstrap data sets: \\[\nSE_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{r=1}^B (\\hat{\\alpha}^{*r}-\\bar{\\hat{\\alpha}}^*)^2   }\n\\]\n\nNote sometimes sampling with replacement must take caution, for example, one can’t simply sample a time series with replacement because the data are sequential.\nEstimate prediction error: Each bootstrap sample has significant overlap with the original data, in fact, about 2/3 of the original data points appear in each bootstrap sample. If we use the original data set as the validation set, This will cause the bootstrap to seriously underestimate the true prediction error. To fix this, one can only use predictions on those samples that do not occur (by chance) in a bootstrap sample.\nBootstrap vs. Permutation test: permutation methods sample from an estimated null distribution for the data, and use this to estimate \\(p\\)-values and False Discovery Rates for hypothesis tests.\nThe bootstrap can be used to test a null hypothesis in simple situation. Eg. If \\(H_0: \\theta=0\\), we can check whether the confidence interval for \\(\\theta\\) contains zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#homework",
    "href": "ch5.html#homework",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.2 Homework",
    "text": "5.2 Homework\n\nConceptual: 1,2,3,4\nApplied: 5–9, at least one.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#code-gist",
    "href": "ch5.html#code-gist",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.3 Code Gist",
    "text": "5.3 Code Gist\n\n5.3.1 Python\nnp.empty(1000) #create an array without initializing\nquartiles = np.percentile(arr, [25, 50, 75])\n\n\n5.3.2 Numpy\nc = np.power.outer(row, col) # mesh of row[i]^col[j] power. \n# random choice \nrng = np.random.default_rng(0)\nalpha_func(Portfolio,\n           rng.choice(100, # random numbers are selected from arange(100)\n                      100, #size\n                      replace=True))\n\n    \n\n\n5.3.3 Pandas\nnp.cov(D[['X','Y']].loc[idx], rowvar=False) #cov compute corr of variables. rowvar-False: cols are vars.\n\n\n5.3.4 Graphics\n\n\n5.3.5 ISLP and statsmodels\n# function that evalues MSE for training a model\ndef evalMSE(terms,    #predictor variables\n            response, #response variable\n            train,\n            test):\n\n   mm = MS(terms)\n   X_train = mm.fit_transform(train)\n   y_train = train[response]\n\n   X_test = mm.transform(test)\n   y_test = test[response]\n\n   results = sm.OLS(y_train, X_train).fit()\n   test_pred = results.predict(X_test)\n\n   return np.mean((y_test - test_pred)**2)\n\n# Compare polynomial models of different degrees\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)],\n                       'mpg',\n                       Auto_train,\n                       Auto_valid)\nMSE\n\n# Estimating the accuracy of a LR model using bootstrap\n\n# Compute the SE of the boostraped values computed by func                      \ndef boot_SE(func,\n            D,\n            n=None,\n            B=1000,\n            seed=0):\n    rng = np.random.default_rng(seed)\n    first_, second_ = 0, 0\n    n = n or D.shape[0]\n    for _ in range(B):\n        idx = rng.choice(D.index,\n                         n,\n                         replace=True)\n        value = func(D, idx)\n        first_ += value\n        second_ += value**2\n    return np.sqrt(second_ / B - (first_ / B)**2) #compute var. \ndef boot_OLS(model_matrix, response, D, idx):\n    D_ = D.loc[idx]\n    Y_ = D_[response]\n    X_ = clone(model_matrix).fit_transform(D_) #clone create a deep copy. \n    return sm.OLS(Y_, X_).fit().params\n    \nquad_model = MS([poly('horsepower', 2, raw=True)]) #raw=True: not normalize the feature\nquad_func = partial(boot_OLS,\n                    quad_model,\n                    'mpg')\nboot_SE(quad_func, Auto, B=1000)\n\n\n\n5.3.6 sklearn\nfrom functools import partial\nfrom sklearn.model_selection import \\\n     (cross_validate,\n      KFold,\n      ShuffleSplit)\nfrom sklearn.base import clone\nfrom ISLP.models import sklearn_sm #wrapper to feed a sm model to sklearn\n\n#Cross Validation\nhp_model = sklearn_sm(sm.OLS,\n                      MS(['horsepower']))\nX, Y = Auto.drop(columns=['mpg']), Auto['mpg']\ncv_results = cross_validate(hp_model,\n                            X,\n                            Y,\n                            cv=Auto.shape[0]) #cv=K.loocv. Can use cv=KFold()object\ncv_err = np.mean(cv_results['test_score']) # test_score: MSE\ncv_err\n\n# Use KFold to partition instead of using an integer. \ncv_error = np.zeros(5)\ncv = KFold(n_splits=10,\n           shuffle=True,#shuffle before splitting\n           random_state=0) # use same splits for each degree\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M,\n                          X,\n                          Y,\n                          cv=cv)\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\n\n# using ShuffleSplit() method \nvalidation = ShuffleSplit(n_splits=10,\n                          test_size=196,\n                          random_state=0)\nresults = cross_validate(hp_model,\n                         Auto.drop(['mpg'], axis=1),\n                         Auto['mpg'],\n                         cv=validation)\nresults['test_score'].mean(), results['test_score'].std()\n\n\n#View skleanrn fitting results using model.results_\nhp_model.fit(Auto, Auto['mpg']) # hp_model is a sklearn model sk_model.fit(X, Y) for trainning\nmodel_se = summarize(hp_model.results_)['std err'] #summarize is an ISLP function\nmodel_se\n\n\n\n5.3.7 Useful code snippet",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch6.html#best-subset-selecttion",
    "href": "ch6.html#best-subset-selecttion",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.1 Best Subset Selecttion",
    "text": "6.1 Best Subset Selecttion\nAlgorithm\n\nFit the data with the null model \\(\\mathcal{M}_0\\), which contains no predictors. This model simply set \\(Y=\\text{mean}(y_i)\\).\nfor \\(k=1, 2, \\cdots, p\\): fit \\(p \\choose k\\) models containing exactly \\(k\\) predictors. Pick the best one that having the smallest RSS or largest \\(R^2\\) (or deviance for classification problem, i.e., \\(-2\\max \\log (\\text{likelihood})\\) on the training set, called \\(\\mathcal{M}_k\\). Note for each categorical variable with \\(L\\)-level, there are \\(L-1\\) dummy variables.\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\). If cross-validation is used, then Step 2 is repeated on each training fold, and the validation errors are averaged to select the best \\(k\\). The the model \\(\\mathcal{M}_k\\) fit on the full training set is delivered for chosen \\(k\\).\n\nBest subset selection suffers - high computation: needs to compute \\(2^p\\) models - overfitting due to the large search space of models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#stepwise-selection",
    "href": "ch6.html#stepwise-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.2 Stepwise selection",
    "text": "6.2 Stepwise selection\nBoth Forward and Backward selection are stepwise selection. They are used when \\(p\\) is large. They searches over \\(1+p(p+1)/2\\) models and are greedy algorithm and is not guaranteed to find the best possible model out of all \\(2^p\\) models.\n\nBackward selection requires \\(n&gt;p\\) (so that the full model can be fit);\nForward selection can be used when \\(n&lt;p\\) but only fits up to models with \\(n\\) variables.\nOne can combine forward and backward selection to a hybrid selection.\n\n\n6.2.1 Forward Stepwise Selection\nAdding one variable at at time that offers the greatest additional improvement.\nAlgorithm\n\nFit the data with the null model \\(\\mathcal{M}_0\\), which contains no predictors. This model simply set \\(Y=\\text{mean}(y_i)\\).\nfor \\(k=1, 2, \\cdots, p-1\\):\n\n\nfit all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nPick the best one that having the smallest RSS or largest \\(R^2\\) on the training set, called \\(\\mathcal{M}_{k+1}\\).\n\n\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\).\n\n\n\n6.2.2 Backward Stepwise Selection\nIt begins with the full model with all variables, and iteratively removing one variable at at time.\nAlgorithm\n\nFit the data with the full model \\(\\mathcal{M}_p\\), which contains all predictors.\nfor \\(k=p, p-1, \\cdots, 1\\):\n\n\nfit all \\(k\\) models that contains all but one of the predictors in \\(\\mathcal{M}_k\\).\nPick the best one that having the smallest RSS or largest \\(R^2\\) on the training set, called \\(\\mathcal{M}_{k-1}\\).\n\n\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#model-selection",
    "href": "ch6.html#model-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.3 Model selection",
    "text": "6.3 Model selection\nModels with all predictors always have the smallest \\(RSS\\) or largest \\(R^2\\) on the training set. Therefore they are not suitable to choose the best one among models with different number of predictors.\nWe ought to estimate the test error on a test set. This may be done - indirectly by adjusting the training error to account for the bias due to overfitting: \\(C_p\\) (equivalently, AIC in case of linear model with Gaussian errors), BIC and adjusted \\(R^2\\).\n\nMallow’s \\(C_p\\): \\[\n  C_p =\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n  \\] where, \\(d\\) us the number of parameters and \\(\\hat{\\sigma}^2 \\approx Var{\\epsilon}\\), typically estimated by using the full model containing all variables. \\(C_p\\) is an unbiased estimate of test MSE.\nAIC is defined for models fit by maximmum likelihood. \\[\n  AIC = -2\\log L + 2d\n  \\] where, \\(L\\) is the maximum likelihood function for the estimated model. For linear regression with Gaussian error, \\(AIC\\propto C_p\\).\nBIC \\[\n  BIC = \\frac{1}{n}( RSS + \\log (n)d \\hat{\\sigma}^2 )\n  \\] Since \\(\\log n&gt;\\) for \\(n&gt;7\\), the BIC places a higher penalty on models with many variables, and hence select smaller models than \\(C_p\\).\nAdjusted \\(R^2\\) (larger value is better)\n\\[\n\\text{Adjusted }R^2=1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\] \\(RSS/(n-d-1)\\) may increase or decrease depends on \\(d\\). Unlike \\(R^2\\), adjusted \\(R^2\\) pays a price for the inclusion of unnecessary variables in a model.\n\\(C_p\\), AIC, BIC, adjusted \\(R^2\\),are not appropriate in high-dimentional setting, as the estimated \\(\\hat{\\sigma}^2 \\approx 0\\) (when \\(p\\ge n\\)).\ndirectly by cross-validation (or validation). It does not require estimate \\(\\sigma^2\\). It has a wide range of usage, as it may difficult to estimate \\(d\\) or \\(\\sigma^2\\). One can choose the model that has the smallest test error or using the one-standard-error rule to select the model that has a smaller size:\n\ncalculate the SE of the estimated test MSE for each model size.\nidentify the lowest test MSE\nchoose the smallest model for which its test MSE is within one SE of the lowest point.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#shrinkage-methods-for-variable-selection",
    "href": "ch6.html#shrinkage-methods-for-variable-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.4 Shrinkage methods for Variable selection",
    "text": "6.4 Shrinkage methods for Variable selection\nThe shrinkage offers an alternative to selecting variables by adjusting a hyperparameter that trades-off RSS and the model parameter magnitudes. Shrinkage methods will produce a different set of coefficients for a different \\(\\lambda\\). Cross-validation may be used to select the best \\(\\lambda\\). After the \\(\\lambda\\) is selected, one can fit a final model using the entire training data set.\nThe reason shrinkage methods may perform better than OLS is rooted in bias-variance trade-off: as \\(\\lambda\\) increases, the flexibility of the model decreases b ecause of shrunk coefficients, leading to decreased variance but increased bias.\n\n6.4.1 Ridge regression: minimize the following objective\n\\[\nRSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] The ridge regression is equivalent to \\[\n\\text{minimize } RSS \\qquad \\text{subject to } \\sum_{j=1}^p \\beta_j^2 \\le s\n\\] for some \\(s\\ge 0\\).\n\nIt encourages the model parameters to shrink toward zero and find a balance between RSS and model parameter magnitudes. Cross-validation is used to find the best tuning parameter \\(\\lambda\\). When \\(\\lambda\\) is large, \\(\\beta_j\\to 0\\). Note, Ridge shrinks all coefficients and include all \\(p\\) variables.\nThe OLS coefficients estimates are scale equivariant: regardless of how \\(X_j\\) is scaled, \\(X_j\\hat{\\beta}_j\\) remain the same: if \\(X_j\\) is multiplied by \\(c\\), this will simply leads to \\(\\hat{\\beta}_j\\) be scaled by a factor of \\(1/c\\).\nIn contrast, when multiplying \\(X_j\\) by a factor, this may significantly change the ridge coefficients. Ridge coefficients depends on \\(\\lambda\\) and the scale of \\(X_j\\), and may even on the scaling of other predictors. Therefore, it is best practice to standardize the predictors before fitting a ridge model: \\[\n\\tilde{x}_{ij} =\\frac{x_{ij}}{\\frac{1}{n} \\sum_{i=1}^n(x_{ij} - \\bar{x}_j)}\n\\]\nRidge regression works best in situations where the OLS estimates have high variance, especially when \\(p\\) is large.\nRidge will include all \\(p\\) variables in the final model.\n\n\n\n6.4.2 The Lasso (Least Absolute Shrinkage and Selection Operator)\n\nThe Lasso replaces the \\(\\ell^2\\) error with \\(\\ell^1\\) penalty. Lasso can force some coefficients to become exactly zero when \\(\\lambda\\) is large enough. Thus it can actually performs variable selection hence better interpretation. Again, cross-validation is employed to select \\(\\lambda\\).\nThe reason Lasso can perform variable selection is because the objective function is equivalent to\n\n\\[\\text {minimizing RSS}, \\text{subject to } \\sum_{j=1}^p |\\beta_j| \\le s\n  \\] for some \\(s\\). The contour of RSS in general only touch the \\(\\ell_1\\) ball at its vertex, at which a minimum is obtained with some variables vanishes. In contrast, in the ridge situation, the \\(\\ell_2\\) ball is round, and in general, the contour of the RSS function only touches the sphere at a surface point where a minimum is obtained with no variable vanishes.\n\nNeither ridge nor the lasso will universally dominate the other. When the response depends on a small number of predictors, one may expect lasso performs better; but in practice, this is never known in advance.\nCombining ridge and lasso leads to elastic net method.\nit is well known that ridge tends to give similar values coefficient values to correlated variables, while lasso may give quite different coefficient values to correlated variables.\nridge regression shrinks all coefficients by the same proportion. While lasso perform soft-thresholding, shrink all coefficients by similar amount, and sufficient small coefficients are shrunk all the way to zero.\nboth ridge and lasso can be considered as computationally feasible approximation to the best subset selection which can be equivalently formulated as: \\[\n\\text{minimize } RSS \\qquad \\text{subject to } \\sum_{j=1}^p I(\\beta_j\\ne 0) \\le s.\n\\]\nBayesian formulation: Both ridge and lasso can be interpreted as maximize the posterior probability (MAP) \\[\np(\\beta|X,Y)\\propto f(Y|X,\\beta) p(\\beta|X)=f(Y|X,\\beta)p(\\beta)\n\\] where \\(p(\\beta)= \\prod_{j=1}^p g(\\beta_j)\\) with some density function \\(g\\) is the believed prior on \\(\\beta\\).\n\nif \\(g\\) is Gaussian with mean zero and standard deviation a function of \\(\\lambda\\), then it follows the solution \\(\\beta\\) given by the ridge is the same as maximizing the posterior \\(p(\\beta|X,Y)\\), that is, \\(\\beta\\) is the posterior mode. In fact, \\(\\beta\\) is also the posterior mean. Since the Gaussian prior is flat at near zero, ridge assumes the coefficients are randomly distributed about zero.\nif \\(g\\) is double-exponential (Laplace) with mean zero and scale parameter a function of \\(\\lambda\\), then it follows the solution \\(\\beta\\) given by the lasso is the same as maximizing the posterior \\(p(\\beta|X,Y)\\), that is, \\(\\beta\\) is the posterior mode. In this case \\(\\beta\\) is not the posterior mean. Since the Laplacian prior is steeply peaked at zero, lasso expects a priori that many coefficients are (exactly) zero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#dimension-reduction-methods-transforming-x_j.",
    "href": "ch6.html#dimension-reduction-methods-transforming-x_j.",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.5 Dimension reduction methods: transforming \\(X_j\\).",
    "text": "6.5 Dimension reduction methods: transforming \\(X_j\\).\nThere are two types of dimension reduction methods for regression: a) PCA regression, b) Partial list squares PLS. Both are designed to handle when the OLS breaks down due to that there are large number of correlated variables.\n\n6.5.1 PCA regression: first use PCA to obtain \\(M\\)- PCA as linear combinations (directions) of the original \\(p\\) predictors:\n\\[\n  Z_m =\\sum_{j=1}^p \\phi_{mj} X_j, \\qquad 1\\le m \\le M,  \n   \\tag{6.1}\\] where, the \\(\\phi_{mj}\\) are called PCA loadings, and subject to the norm \\(\\sum_{j=1}^p\\phi_{mj}^2=1\\) for each \\(m\\). Note \\(Z_m\\) is a vector of length equal to the length of \\(X_j\\), which is the number of data points \\(n\\). The component of \\(Z_m\\): \\(z_{im}\\), \\(1\\le i \\le n\\) are called PCA scores. \\(z_{im}\\) is a single number summary of the \\(p\\) predictors with the \\(m\\)-th PCA for the \\(i\\)-th observation. PCA is not a feature selection method.\nThe first PCA defines the direction that contains the largest variance in \\(X\\), and minimize the sum of squared perpendicular distances to each point (the projection error on the PCA), that is it defines the line that is as close as possible to the data; (In fact, the first PCA is given by the eigenvector of the largest eigenvalue of the covariance matrix \\(\\frac{1}{n-1}X^TX\\)). The second PCA is orthogonal to the first PCA and has the second largest variance and is uncorrelated with the first, and so on. These directions are obtained in an unsupervised way, as \\(Y\\) is not used to obtain these components. Consequently, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\nPCA is typically conducted after standardizing the data \\(X\\), as without scaling, the high variance variables will tend to have higher influence on the obtained PCAs.\nWe then use OLS to fit a linear regression model \\[\ny_i =\\theta_0 +\\sum_{m=1}^M \\theta_m z_{im}+\\epsilon_i, \\qquad i=1,2,\\cdots, n\n\\tag{6.2}\\]\nAfter substitute Equation 6.1 into equation Equation 6.2, one can find that \\[\n\\sum_{m=1}^M \\theta_mz_{im} =\\sum_{j=1}^p \\beta_jx_{ij}\n\\] with \\[\n\\beta_j = \\sum_{m=1}^M \\theta_m\\phi_{mj}.\n\\tag{6.3}\\] Eq. Equation 6.3 has the potential to bias the coefficient estimates, but selecting \\(M&lt;&lt; p\\) can significantly reduce the variance. So model Equation 6.2 is a special case of linear regression subject to the constants Equation 6.3.\nPCR and ridge are closely related and one can think of ridge regression as a continuous version of PCR.\n\n\n6.5.2 Partial Least Squares\nSimilar to PCAR, PLS also first identifies a new set of features \\(Z_1, Z_2, \\cdots, Z_m\\), each of which is a linear combinations of the original features, and then fits a linear model via OLS with these new \\(M\\) features.\nBut PLS identifies these new features in a supervised way, that is, PLS uses \\(Y\\) in order to identify the new features that not only approximate the old features well, but also are related to the response, i.e., these new features explain both the response and the predictors.\nFirst PLS standardizes the \\(p\\) predictors. PLS identifies the first component \\(Z_1 = \\sum_{j=1}^p \\phi_{1j}X_j\\) by choosing \\(\\phi_{1j}=&lt;X_j, Y&gt;\\), the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\). Since this coefficient is equal to the correlation between \\(Y\\) and \\(X_j\\), PLS places the highest weight on the variables that are most strongly related to \\(Y\\). The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response.\nNext, PLS orthogonality each \\(X_j\\) with respect to \\(Z_1\\), that is, replace each \\(X_j\\) with the residual by regressing \\(X_j\\) on \\(Z_1\\), and then repeat the same process.\nWhen \\(p\\) is large, especially \\(p&gt;n\\), the forward selection method, shrinkage methods (lasso or ridge), PCR, PLR fit a less flexible model, hence particularly useful in performing regression in high-dimensional settings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#homework",
    "href": "ch6.html#homework",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.6 Homework:",
    "text": "6.6 Homework:\n\nConceptual: 1–4\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#code-snippet",
    "href": "ch6.html#code-snippet",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.7 Code Snippet",
    "text": "6.7 Code Snippet\n\n6.7.1 Python\nnp.isnan(Hitters['Salary']).sum()\n\n\n\n6.7.2 Numpy\nnp.linalg.norm(beta_hat) #L2 norm. ord=1: L1  ord='inf': max norm.\n\n\n6.7.3 Pandas\nHitters.dropna();\nsoln_path = pd.DataFrame(soln_array.T,\n                         columns=D.columns,\n                         index=-np.log(lambdas))\nsoln_path.index.name = 'negative log(lambda)'\n\n\n6.7.4 Graphics\nax.errorbar(np.arange(n_steps), \n            cv_mse.mean(1), #mean of each row (model)\n            cv_mse.std(1) / np.sqrt(K), #estimate standard error of the mean\n            label='Cross-validated',\n            c='r') # color red\n            \nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--') # plot a verticalline\n\n\n6.7.5 ISLP and statsmodels\n#Estimate Var(epsilon)\n\ndesign = MS(Hitters.columns.drop('Salary')).fit(Hitters)\ndesign.terms # to see the variable names in the design matrix\nY = np.array(Hitters['Salary'])\nX = design.transform(Hitters)\nsigma2 = OLS(Y,X).fit().scale  #.scale: RSE: residual standard error estimating \n\n# Forward Selection using ISLP.models and a scoring function\nfrom ISLP.models import \\\n     (Stepwise,\n      sklearn_selected,\n      sklearn_selection_path)\nstrategy = Stepwise.first_peak(design,\n                               direction='forward',\n                               max_terms=len(design.terms))\nhitters_Cp = sklearn_selected(OLS,\n                               strategy,\n                               scoring=neg_Cp)\n                               #default scoring MSE, will choose all variables\nhitters_Cp.fit(Hitters, Y) # the same as hitters_Cp.fit(Hitters.drop('Salary', axis=1), Y)\nhitters_Cp.selected_state_\n\n#Forward selection using cross-validation\nstrategy = Stepwise.fixed_steps(design,\n                                len(design.terms),\n                                direction='forward')\nfull_path = sklearn_selection_path(OLS, strategy) #using default scoring MSE\nfull_path.fit(Hitters, Y) # there are , 19 variables, 20 models\nYhat_in = full_path.predict(Hitters)\n\n#calculate in-sample mse\n\nmse_fig, ax = subplots(figsize=(8,8))\ninsample_mse = ((Yhat_in - Y[:,None])**2).mean(0) #Y[:,None]: add a second axis, create a column vector\n                        #[yw] mean(0): calculate mean along row, i.e., for each col. mean(1): calculate mean for each row\n\n#Cross-validation\nK = 5\nkfold = skm.KFold(K,\n                  random_state=0,\n                  shuffle=True)\nYhat_cv = skm.cross_val_predict(full_path,\n                                Hitters,\n                                Y,\n                                cv=kfold)\n# Cross-validation mse\ncv_mse = []\nfor train_idx, test_idx in kfold.split(Y):\n    errors = (Yhat_cv[test_idx] - Y[test_idx,None])**2\n    cv_mse.append(errors.mean(0)) # column means\ncv_mse = np.array(cv_mse).T\n\n#validation approach using ShuffleSplit\nvalidation = skm.ShuffleSplit(n_splits=1, # only split one time. \n                              test_size=0.2,\n                              random_state=0)\nfor train_idx, test_idx in validation.split(Y):\n    full_path.fit(Hitters.iloc[train_idx], #note needing to use iloc\n                  Y[train_idx])\n    Yhat_val = full_path.predict(Hitters.iloc[test_idx])\n    errors = (Yhat_val - Y[test_idx,None])**2\n    validation_mse = errors.mean(0)\n\n\n\n6.7.6 sklearn\nrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\n\n#Best subset selection using 10bnb\n\nD = design.fit_transform(Hitters)\nD = D.drop('intercept', axis=1) #needs to drop intercept\nX = np.asarray(D)\npath = fit_path(X, \n                Y,\n                max_nonzeros=X.shape[1]) #fit_path: a funciton from l0nb. use all variables\n                # max_nonzeros: max nonzero coefficients in the fitted model.\n\n# Ridge Regression\nsoln_array = skl.ElasticNet.path(Xs, # standardized, no intercept\n                                 Y,\n                                 l1_ratio=0., #ridge\n                                 alphas=lambdas)\n# Using pipline\nridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)\nscaler = StandardScaler(with_mean=True,  with_std=True)\npipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\npipe.fit(X, Y)\nridge.coef_\n\n# Validation\n\nvalidation = skm.ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0) # validation is a generator\nridge.alpha = 0.01\nresults = skm.cross_validate(ridge,\n                             X,\n                             Y,\n                             scoring='neg_mean_squared_error',\n                             cv=validation) # using the strategy defined in validation\n-results['test_score']\n\n# GridSearchCV()\nparam_grid = {'ridge__alpha': lambdas}\ngrid = skm.GridSearchCV(pipe,\n                        param_grid,\n                        cv=validation, # or use cv=kfold (5-fold CV defined separately)\n                        scoring='neg_mean_squared_error') #default scoring=R^2\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\ngrid.cv_results_['mean_test_score']\ngrid.cv_results_['std_test_score']\n\n\n#Plot CV MSE\nridge_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);\n\n# Use ElasticNetCV()\nridgeCV = skl.ElasticNetCV(alphas=lambdas, # ElasticNetCV accepts a sequence of alphas\n                           l1_ratio=0,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler), # scaling is done once. \n                         ('ridge', ridgeCV)])\npipeCV.fit(X, Y)\ntuned_ridge = pipeCV.named_steps['ridge']\ntuned_ridge.mse_path_\ntuned_ridge.alpha_ # best alpha\ntuned_ridge.coef_\n\n# Evaluating test Error of Cross-validated Ridge \n\nouter_valid = skm.ShuffleSplit(n_splits=1, \n                               test_size=0.25,\n                               random_state=1)\ninner_cv = skm.KFold(n_splits=5,\n                     shuffle=True,\n                     random_state=2)\nridgeCV = skl.ElasticNetCV(alphas=lambdas, # a sequence of lambdas\n                           l1_ratio=0,\n                           cv=inner_cv) # K-fold validation\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('ridge', ridgeCV)]);\n                         \n                         \nresults = skm.cross_validate(pipeCV, \n                             X,\n                             Y,\n                             cv=outer_valid,\n                             scoring='neg_mean_squared_error')\n-results['test_score']\n\n# Lasso regression\nlassoCV = skl.ElasticNetCV(n_alphas=100, #test 100 alpha values\n                           l1_ratio=1,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('lasso', lassoCV)])\npipeCV.fit(X, Y)\ntuned_lasso = pipeCV.named_steps['lasso']\ntuned_lasso.alpha_\ntuned_lasso.coef_\nnp.min(tuned_lasso.mse_path_.mean(1)) # miminum avg mse\n\n#to get the soln path\nlambdas, soln_array = skl.Lasso.path(Xs, # standarsized, no -intercept\n                                    Y,\n                                    l1_ratio=1,\n                                    n_alphas=100)[:2]\n\n#PCA and PCR\npca = PCA(n_components=2)\nlinreg = skl.LinearRegression()\npipe = Pipeline([('scaler', scaler), \n                 ('pca', pca),\n                 ('linreg', linreg)])\npipe.fit(X, Y)\npipe.named_steps['linreg'].coef_\npipe.named_steps['pca'].explained_variance_ratio_\n\n# perform Grid search\nparam_grid = {'pca__n_components': range(1, 20)} #PCA needs n_components &gt;0\ngrid = skm.GridSearchCV(pipe,\n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, Y)\n\n# cross-validation a null model\ncv_null = skm.cross_validate(linreg,\n                             Xn,\n                             Y,\n                             cv=kfold,\n                             scoring='neg_mean_squared_error')\n-cv_null['test_score'].mean()\n\n#PLS\npls = PLSRegression(n_components=2, \n                    scale=True) # standarsize the data \npls.fit(X, Y) # X has no-intercept \n\n# Cross-validation\nparam_grid = {'n_components':range(1, 20)}\ngrid = skm.GridSearchCV(pls,\n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, Y)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch7.html#polynomials",
    "href": "ch7.html#polynomials",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.1 Polynomials",
    "text": "7.1 Polynomials\nThe basis functions are simply the polynomial functions of different degrees. Polynomial terms of higher powers for \\(X_j\\) or interaction terms \\(X_iX_j\\) are used, but the model is still a linear model in the coefficients \\(\\beta_j\\). The optimum degree \\(d\\) can be chosen by cross-validation. polynomial terms can be included in either a linear regression model or a logistic regression model.\nIn practice hardly a degree greater than 3 or 4 is used because a higher degree polynomial exhibits high degree of oscillation, especially near the boundary (Runge’s phenomenon). This is because a polynomial imposes a global structure.\nThe standard error at a point \\(x_0\\)is calculated by \\[\nSE[\\hat{f}(x_0)] = \\ell_0^T \\hat{\\mathbf{C}} {\\ell}_0\n\\] where, \\(\\ell_0^T =(1, x_0, x_0^2, \\cdots, x_0^d)\\), and \\(\\hat{\\mathbf{C}}\\) is the covariance matrix of the estimated coefficients \\(\\beta_j\\), \\(j=0, 1, \\cdots, d\\) obtained from the OLS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#step-functions",
    "href": "ch7.html#step-functions",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.2 Step functions",
    "text": "7.2 Step functions\nA step function is a piece-wise constant function. Cut a \\(X\\) variable into \\(K+1\\) regions using \\(K\\) cut points and then either use one-hot coding with \\(K+1\\) dummy variables (and no intercept, in this case, each coefficient can be interpreted as the average value in that region) or create \\(K\\) dummy variables with an intercept to represent all those regions (in this case, the average value in that region equals to the intercept plus the coefficient). Choice of cut-points (knots) can be problematic. Binning the \\(X\\) variable amounts to convert a continuous variable into an ordered categorical variable.\nThe basis functions are simply indicator functions on each region: \\[\nb_j(x_i)= I(c_j\\le x_i&lt; c_{j+1}).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#piececwise-polynomials",
    "href": "ch7.html#piececwise-polynomials",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.3 Piececwise polynomials",
    "text": "7.3 Piececwise polynomials\nIt overcomes the disadvantage of polynomial basis which imposes a global structure. It fits separate low-degree polynomials over different regions of \\(X\\) separated by knots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#splines",
    "href": "ch7.html#splines",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.4 Splines",
    "text": "7.4 Splines\nSplines are piece-wise polynomials of degree \\(d\\) that are continuous up to \\(d-1\\) derivatives at each knot. E.g. a cubic spline with \\(K\\) knots has continuity up to second derivative at each knot, and has degree of freedom of \\(K+4\\).\n\nlinear spline: with knots \\(\\xi_k\\), \\(k=1, \\cdots, K\\) is a piece-wise linear polynomial that is continuous at each knot. \\[\ny= \\beta_0+\\beta_1 b_1(x)+\\cdots + \\beta_{K+1}b_{K+1}(x)+\\epsilon,\n\\] where, \\(b_k\\) are basis functions defined by \\[\\begin{align}\nb_1(x) & = x \\\\\nb_{k+1}(x) & = (x- \\xi_k)_{+}, \\qquad k=1, \\cdots K\n\\end{align}\\] Here the positive part is defined by \\[x_{+}=\\begin{cases}\nx, &  \\text{ if } x&gt;0 \\\\\n0 &  \\text{ otherwise}\n\\end{cases}\n\\]\ncubic splines: with knots \\(\\xi_k\\), \\(k=1, \\cdots, K\\) is a piecewise cubic polynomials with continuous derivatives up to order 2 at each knot.\n\n\\[\ny= \\beta_0+\\beta_1 b_1(x)+\\cdots + \\beta_{K+3}b_{K+3}(x)+\\epsilon,\n\\] knot placement: General principle is that placing more knots in places where the function might vary most rapidly. In practice, it is common to place them at uniform quantiles of the observed \\(X\\). This can be done by specifying a dof, and then let the algorithm to calculate the knots at uniform quantiles. Note a natural spline have more internal knots than a regression spline for the same dof.\nFor a regular cubic spline, the basis functions are defined by \\[\\begin{align}\nb_1(x) & = x \\\\\nb_2(x) & = x^2 \\\\\nb_3(x) & = x^3 \\\\\nb_{k+3}(x) & = (x- \\xi_k)^3_{+}, \\qquad k=1, \\cdots K\n\\end{align}\\]\ndof: \\(K+4\\) number of parameters (including the intercept). A regression spline can have high variance at the outer range of the predictors. To remedy this, one can use a natural cubic spline.\na natural cubic spline extrapolates linearly ( as a linear function) beyond the internal knots. This adds \\(2\\times 2\\) extra constrains. A natural cubic spline allows to put more internal knots for the same degree of freedom as a regular cubic spline.\ndof: \\(K+2\\) (\\(K\\) only counts the internal knots; including the intercept).\nFor a smoothing spline: it is the solution \\(g\\) to the following problem: \\[\n  \\text{minimize}_{g\\in S}\\sum_{i=1}^n(y_i-g(x_i))^2 +\\lambda \\int g''(t)dt\n  \\]\nThe first term is the loss RSS and encourages \\(g(x_i)\\) matches \\(y_i\\). The second term is the penalty that penalize the variability in \\(g\\) (measured by \\(g''(t)\\)) by a tuning parameter \\(\\lambda \\ge 0\\). If \\(\\lambda=0\\) (no constraints on \\(g\\)), then the solution is just an interpolating polynomial. If \\(\\lambda\\to \\infty\\), then \\(g\\) is a linear function (because its second derivative is zero). \\(\\lambda\\) controls the bias-variance trade-off.\nThe smoothing spline is in fact a natural spline with knots at unique values of \\(x_i\\). But it is different than the natural spline. It is a shrunk version of a natural cubic spline, otherwise it would have too large (nominal) dof (number of parameters) because it has knots at unique values of \\(x_i\\). It avoids the knot selection issue and leaving a single \\(\\lambda\\) to tune. An effective degrees of freedom can be calculated for a smoothing spline as \\[\ndf_\\lambda =\\sum_{i=1}^n {\\{\\mathbf{S}_\\lambda}\\}_{ii},\n\\] where \\(\\mathbf{S}_\\lambda\\) is a \\(n\\times n\\) matrix determined by \\(\\lambda\\) and \\(x_i\\) such that the vector of \\(n\\) fitted values can be written as \\[\n\\hat{\\mathbf{g}}_\\lambda =\\mathbf{S}_\\lambda \\mathbf{y}.\n\\] \\(df_\\lambda\\) decreases from \\(n\\) to 2 as \\(\\lambda\\) increases from 0 to \\(\\infty\\).\nThe LOO cross-validation error can be efficiently computed by \\[\n\\text{RSS}_{cv}(\\lambda) =\\sum_{i=1}^n (y_i-\\hat{g}_\\lambda ^{(-i)}(x_i))^2=\\sum_{i=1}^n \\left[ \\frac{y_i-\\hat{g}_\\lambda (x_i)}{1-\\{ {\\mathbf S}_\\lambda\\}_{ii}} \\right]^2\n\\] - Local Regression: a non-parametric method. It is similar to spline, but allowing regions overlap. With a sliding weight function of span \\(s\\), fit separate (constant, linear, quadratic, for instance) fits over the range of \\(X\\) by weighted least squares.\nThe span \\(s\\) plays the similar role as \\(\\lambda\\) in a smoothing spline, it controls the flexibility of the local regression. The smaller \\(s\\) is, the more local and wiggle will be the fit.\nLocal regression is a memory based procedure, because like KNN, all training data are needed each time when making a prediction.\nLocal regression can be generalized to varying coefficient models that fits a multiple linear regression model that is global in some variables but local in another, such as time.\nLocal regression can be naturally extends to \\(p\\)-dimension using a \\(p\\)-dimensional neighborhood, but really used when \\(p\\) is larger than 3 or 4 because there will be generally very few training examples near \\(x_0\\) (curse of dimensionality)\n\nGAM (Generalized Additive Models): can be considered as an extension of multiple linear regression, replacing each feature \\(\\beta_jX_j\\) with an nonlinear function \\(f_j(X_j)\\). \\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) +\\cdots + f_p(x_{ip})  + \\epsilon\n\\] GAM can mix different \\(f_j\\), for example, a spline, or a linear term or even include low order interactive terms. The coefficients are hard to interpret, but the fitted values are of interest.\n\nGAM can be used in fitting a logistic regression model, that is \\[\n\\log \\frac{p(X)}{1-p(X)} =\\beta_0+f_1(X_1)+f_2(X_2)+\\cdots +f_p(X_p)\n\\]\nWhen fitting a GAM, and if OLS can not be used (such as when a smoothing spline is used), then the back fitting iterative method can be used: randomly initialize all variable coefficients; repeatedly hold all but one variable fixed, and perform a simple linear regression on that single variable, and update the corresponding coefficients until convergence. Convergence is typically very fast.\nPros and Cons of GAM\n\nflexible to model \\(f_j\\), eliminating the need to try different transformations on each variable\npotentially more accurate prediction\nbecause the model is additive, can easily examine the effects of \\(X_j\\) on \\(Y\\) by holding all of the other variables fixed.\nThe smoothness of \\(f_j\\) can be summarized by the effective dof.\ninteraction terms \\(X_jX_k\\) can be added.\nlow dimensional interaction functions of the form \\(f_{jk}(X_j, X_k)\\) can be added. Such term can be fit using a two-dimensional smoothers such as local regression or two dimensional splines.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#homework",
    "href": "ch7.html#homework",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.5 Homework:",
    "text": "7.5 Homework:\n\nConceptual: 1–5\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#code-snippet",
    "href": "ch7.html#code-snippet",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.6 Code Snippet",
    "text": "7.6 Code Snippet\n\n7.6.1 Python\n\n\n\n\n7.6.2 Numpy\nWage['education'].cat.categories # .cat is the categorical method accessor\nWage['education'].cat.codes\npd.crosstab(Wage['high_earn'], Wage['education'])\n\nnp.column_stack([Wage_['age'],\n                         Wage_['year'],\n                         Wage_['education'].cat.codes-1])\n\nXs = [ns_age.transform(age),\n      ns_year.transform(Wage['year']),\n      pd.get_dummies(Wage['education']).values] # -&gt; 5 education levels: 1-hot coding\nX_bh = np.hstack(Xs)\n\n\n\n7.6.3 Pandas\ncut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints. pd.cut is similar\n\n\n7.6.4 Graphics\nax.legend(title='$\\lambda$');\n\n\n\n7.6.5 ISLP and statsmodels\n\n\n\n\n7.6.6 sklearn\n\n\n7.6.7 Useful code snippets\n\n7.6.7.1 plot a model fit with confidence interval\ndef plot_wage_fit(age_df, \n                  basis, # ISL model object\n                  title):\n\n    X = basis.transform(Wage)\n    Xnew = basis.transform(age_df)\n    M = sm.OLS(y, X).fit()\n    preds = M.get_prediction(Xnew)\n    bands = preds.conf_int(alpha=0.05)\n    fig, ax = subplots(figsize=(8,8))\n    ax.scatter(age,\n               y,\n               facecolor='gray',\n               alpha=0.5)\n    for val, ls in zip([preds.predicted_mean,\n                      bands[:,0],\n                      bands[:,1]],\n                     ['b','r--','r--']):\n        ax.plot(age_df.values, val, ls, linewidth=3)\n    ax.set_title(title, fontsize=20)\n    ax.set_xlabel('Age', fontsize=20)\n    ax.set_ylabel('Wage', fontsize=20);\n    return ax\n\n\n7.6.7.2 Fitting with a step function\ncut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints\n# note pd.get_dummies(cut_age) is the X matrix\nsummarize(sm.OLS(y, pd.get_dummies(cut_age)).fit()) \n\n\n\n7.6.7.3 Fitting a spline\n#specifying internal knots\n\nbs_age = MS([bs('age',\n                internal_knots=[25,40,60],\n                name='bs(age)')]) #rename the variable names \nXbs = bs_age.fit_transform(Wage) # Xbs == bs_age above\nM = sm.OLS(y, Xbs).fit()\nsummarize(M)\n\n# specifying df \nbs_age0 = MS([bs('age',\n                 df=3, # df count does not include intercept. df=degree+ #knots\n                 degree=0)]).fit(Wage)\nXbs0 = bs_age0.transform(Wage)\nsummarize(sm.OLS(y, Xbs0).fit())\n\nBSpline(df=3, degree=0).fit(age).internal_knots_\n\n# Fit a natural spline\nns_age = MS([ns('age', df=5)]).fit(Wage) #df=degree+ #knots -2\nM_ns = sm.OLS(y, ns_age.transform(Wage)).fit()\nsummarize(M_ns)\n\n# fit a smoothing spline\nX_age = np.asarray(age).reshape((-1,1))\ngam = LinearGAM(s_gam(0, lam=0.6)) #gam is the smoothing spline model with a given lambda\ngam.fit(X_age, y)\n\n#Fiting a smoothing spline with an optimized lambda\ngam_opt = gam.gridsearch(X_age, y)\n\n\n# Fitting a smoothin spline by specifying a df (not including intercept)\nfig, ax = subplots(figsize=(8,8))\nax.scatter(X_age,\n           y,\n           facecolor='gray',\n           alpha=0.3)\nfor df in [1,3,4,8,15]:\n    lam = approx_lam(X_age, age_term, df+1) # find the lambda corresponding to a df. \n    age_term.lam = lam # update lambda\n    gam.fit(X_age, y)\n    ax.plot(age_grid,\n            gam.predict(age_grid),\n            label='{:d}'.format(df),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='Degrees of freedom');\n\n\n\n\n\n\n\n7.6.8 GAM\n### manually contruct basis \nns_age = NaturalSpline(df=4).fit(age) #df counts do not include intercepts. -&gt; 4 columns\nns_year = NaturalSpline(df=5).fit(Wage['year']) # -&gt; 5 cols\nXs = [ns_age.transform(age),\n      ns_year.transform(Wage['year']),\n      pd.get_dummies(Wage['education']).values] # -&gt; 5 education levels: 1-hot coding\nX_bh = np.hstack(Xs)\ngam_bh = sm.OLS(y, X_bh).fit()\n\n### Examinge partial effect\n\nage_grid = np.linspace(age.min(),\n                       age.max(),\n                       100)\nX_age_bh = X_bh.copy()[:100] # Take the first 100 rows of X_bh\n# calculate the row mean and make it a row vector in the shape of 1Xp, then broadcast\nX_age_bh[:] = X_bh[:].mean(0)[None,:] \nX_age_bh[:,:4] = ns_age.transform(age_grid)# replace the first 4 cols with basis functions evalued at age_grid\npreds = gam_bh.get_prediction(X_age_bh) #gam_bh is the GAM model with all 14 basis\nbounds_age = preds.conf_int(alpha=0.05)\npartial_age = preds.predicted_mean\ncenter = partial_age.mean() # center of the prediction \npartial_age -= center # center the prediction for better viz\nbounds_age -= center\nfig, ax = subplots(figsize=(8,8))\nax.plot(age_grid, partial_age, 'b', linewidth=3)\nax.plot(age_grid, bounds_age[:,0], 'r--', linewidth=3)\nax.plot(age_grid, bounds_age[:,1], 'r--', linewidth=3)\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage', fontsize=20);\n\n\n### Using a smoothing spline and pygam package\n#### Specifying lambda\n#### default \\lambda = 0.6 is used.\ngam_full = LinearGAM(s_gam(0) + # spline smoothing applies to the first col of the feature matrix\n                     s_gam(1, n_splines=7) + # smoothing applied to the 2nd col \n                     f_gam(2, lam=0)) # smothing applied to the 3rd col: a factor col\nXgam = np.column_stack([age,  #stack as columns\n                        Wage['year'],\n                        Wage['education'].cat.codes]) \ngam_full = gam_full.fit(Xgam, y)\n\ngam_full.summary() # verbose summary\n\n#### Plot partial effect using a plot_gam from ISLP.pygam\nfig, ax = subplots(figsize=(8,8))\nplot_gam(gam_full, 0, ax=ax) # 0: partial plot of the first component: age\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage - default lam=0.6', fontsize=20);\n\n### Specifying df\nage_term = gam_full.terms[0]\nage_term.lam = approx_lam(Xgam, age_term, df=4+1)\nyear_term = gam_full.terms[1]\nyear_term.lam = approx_lam(Xgam, year_term, df=4+1)\ngam_full = gam_full.fit(Xgam, y)\n\n#### Plot partial effect\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_full, 2)\nax.set_xlabel('Education')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of wage on education',\n             fontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n\n\n7.6.8.1 Anova for GAM\ngam_0 = LinearGAM(age_term + f_gam(2, lam=0)) # note age_term is a s_gam with df=4 defined above \ngam_0.fit(Xgam, y)\ngam_linear = LinearGAM(age_term +\n                       l_gam(1, lam=0) +\n                       f_gam(2, lam=0))\ngam_linear.fit(Xgam, y)\nanova_gam(gam_0, gam_linear, gam_full)\n\n\n\n7.6.8.2 Logistic GAM\ngam_logit = LogisticGAM(age_term + \n                        l_gam(1, lam=0) +\n                        f_gam(2, lam=0))\ngam_logit.fit(Xgam, high_earn)\n\n\n\n7.6.8.3 LOESS\nlowess = sm.nonparametric.lowess\nfig, ax = subplots(figsize=(8,8))\nax.scatter(age, y, facecolor='gray', alpha=0.5)\nfor span in [0.2, 0.5]:\n    fitted = lowess(y,\n                    age,\n                    frac=span,\n                    xvals=age_grid)\n    ax.plot(age_grid,\n            fitted,\n            label='{:.1f}'.format(span),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='span', fontsize=15);",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch8.html#regression-tree",
    "href": "ch8.html#regression-tree",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.1 Regression tree",
    "text": "8.1 Regression tree\nFor a Regression tree, The value at a leaf node equals to the average of the \\(Y\\) values of all examples in the leaf node. The objective is to minimize the RSS \\[\nRSS =\\sum_{j=1}^J \\sum_{i\\in R_j}(y_i-\\hat{y}_{R_j})^2\n\\tag{8.1}\\] where, \\(\\hat{y}_{R_j}\\) is the mean response for the training observations in the \\(j\\)-th box \\(R_j\\) that corresponds to the \\(j\\)-th leaf node.\nThe first node (root) is the most important predictor, and so on.\nIt is computationally intractable to consider every possible partition of the feature space into \\(J\\) boxes in objective Equation 8.1. The solution is to use a top-down and greedy recursive binary splitting. It is greedy (myopic) because at each step of the tree-building process, the best split is decided at that particular step by choosing a predictor \\(X_j\\) (consider all predictors) and a cut-point \\(s\\) (consider all values of that predictor) that leads to the greatest reduction in RSS, rather than looking ahead and picking a split that will lead to a better tree in some future step. The greedy splitting amounts to at each step, only the current region is split by a feature. This process is repeated within each of the resulting regions, until a stopping criterion.\nStopping criterion: - max number of observations in each leaf - max number of depth - RSS decrease smaller than a threshold.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#classificaiton-tree",
    "href": "ch8.html#classificaiton-tree",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.2 Classificaiton tree",
    "text": "8.2 Classificaiton tree\nFor a Classification tree: An example is classified as the class which is the mode of the examples in the leaf node. The training objective is similar to Equation 8.1, but with RSS replaced with\n\nclassification error rate \\(E=1- \\max_k(\\hat{p}_{mk})\\), where \\(\\hat{p}_{mk}\\) is the fraction of training examples in the \\(m\\)th region that are in the \\(k\\)-th class. But this measure is not sufficiently sensitive for tree-growing (node-splitting).\nGini index that measures node purity (total variance): \\(G=\\sum_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\). The Gini index takes on small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one, indicating that a node contains predominantly observations from a single class.\nCross-entropy: very similar to Gini index numerically, \\(D=-\\sum_{k=1}^K\\hat{p}_{mk}\\log \\hat{p}_{mk}\\). Cross-entropy is always non-negative. Cross entropy is the expectation of the information contained in a probability information.\n\nGini index and Cross-entropy are preferred when splitting a node, while Classification error rate is preferred when pruning a tree if the prediction accuracy is the final goal.\nTwo leaf nodes might have the same predicted value resulting from a split, this is because the two leaf nodes have different node purity, which amounts to the certainty of a predicted value. In this case, an observation falls into the leaf node with higher purity renders higher certainty.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#prunning-a-tree",
    "href": "ch8.html#prunning-a-tree",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.3 Prunning a tree",
    "text": "8.3 Prunning a tree\nUsing stopping criteria directly to obtain a small tree may be near-sighted: A seemingly worthless spit early on might be followed by a very good split with large RSS reduction. A better approach is to grow a very large tree \\(T_0\\) such that each leaf only has some minimum number of observations, and then prune it back in order to obtain a subtree with the least test error via cross-validation or validation approach. Cost complexity pruning (weakest link pruning) is used to do this by minimizing the following with a tuning \\(\\alpha\\): \\[\n\\sum_{m=1}^{|T_\\alpha|}\\sum_{i:x_i\\in R_m}(y_i-\\hat{y}_{R_m})^2+\\alpha |T_\\alpha|\n\\] where, \\(|T_alpha|\\) is the number of leaf nodes in \\(T_\\alpha\\), which is the best subtree that minimize the above objective. As we increase \\(\\alpha\\) from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the sequence of \\(T_\\alpha\\) is easy. The formulation is similar to lasso. And then An optimal \\(\\hat{\\alpha}\\) is chosen by cross-validation, and the corresponding \\(T_{\\hat{\\alpha}}\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#bagging",
    "href": "ch8.html#bagging",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.4 Bagging",
    "text": "8.4 Bagging\nBootstrapping aggregation, or bagging is a general purpose procedure for reducing variance of a statistical learning method because of the Law of Large Numbers: given a set of \\(n\\) independent observations \\(Z_1, \\cdots, Z_n\\) with variance \\(\\sigma^2\\), the variance of the mean \\(\\bar{Z}\\) is \\(\\sigma^2/n\\). In other words, averaging a set of observations reduces variance.\nBut in practice, we typically do not have access to multiple training sets. Instead, we bootstrap the training set to obtain \\(B\\) (usually hundreds or even thousands) bootstrapped training sets, and then fit a separate tree (usually deep and not pruned, hence with low bias) independently for the \\(b\\)-th bootstrapped training set to get the prediction \\(\\hat{f}^{*b}(x)\\) at \\(x\\) for each \\(b\\), and then finally combine all the trees by averaging all the predictions to obtain \\[\n\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x).\n\\] The above formula works for regression. For classification, the average is replaced by majority vote. Using large \\(B\\) in bagging (including RF) typically does not lead to overfitting. But small \\(B\\) may underfit. Bagging often leads to correlated (similar) trees, and can get caught in local optima and thus fail to explore the model space, and thus averaging may not lead to large reduction in variance. One way to remedy this is by RF.\n\n8.4.1 Out-of-Bag Error Estimate\nOn average, each bagged tree makes use of 2/3 of the total observations. The remaining 1/3 of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. For each observation, it is an OOB observation in around \\(B/3\\) trees, and hence the average of the predictions of those \\(B/3\\) trees for the \\(i\\)-the observation can be used as a cross-validation error for observation \\(i\\). The overall OOB error can be calculated this way for all \\(n\\) observations.\nWhen \\(B\\) is large, such as \\(B=3n\\), then this is essentially the LOO cross-validation error for bagging. This is cheap way to evaluate test error without the need of cross-validation (which may be onerous) or validation approach.\n\n\n8.4.2 Random Forests\nBagging results in correlated trees and thus the variance may not be reduced by the average. Random forests still grows independent trees using bootstrapped data sets of the original data set, but RF decorrelates the trees by randomly selecting \\(m\\) predictors (\\(m&lt;p\\)) each time a split in a tree is considered. Typically \\(m\\approx \\sqrt{p}\\). Thereby leading to a more thorough exploration of model space. Bagging is the case when \\(m=p\\). On average, \\((p-m)/p\\) of the splits will not consider a specific predictor. Using a small value of \\(m\\) in building RF will typically helpful when there are a large number of correlated predictors.\nLarge \\(B\\) will not lead to RF to overfit, but small \\(B\\) may underfit.\n\n\n8.4.3 Variable Importance Measure (VI)\nFor bagged/RF regression trees, VI is the total amount of RSS decreased due to splits over a given predictor, averaged over all \\(B\\) trees. A large VI value indicates an important predictor. For bagged/RF classification trees, replacing RSS with Gini index or cross-entorpy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#boosting",
    "href": "ch8.html#boosting",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.5 Boosting",
    "text": "8.5 Boosting\nLike Bagging, boosting is a general approach that can be applied to many statistical learning methods for regression and classification. Boosting does not involve bootstrap sampling; Boosting grows trees sequentially by slow learning: each new tree is grown by fitting a new tree to the residuals (modified version of the original data set) left over from the previous trees, and then a shrunken version of the new tree is added to the model. Each new tree tries to capture signal that is not yet accounted for by the current set of trees.\n\n8.5.1 Boosting Algorithm for regression trees\n\nSet \\(\\hat{f}(x)=0\\), and \\(r_i=y_i\\) for all \\(i\\) in the training set.\nFor \\(b=1, \\cdots, B\\), repeat 2.1. Fit a tree \\(\\hat{F}^b\\) with \\(d\\) splits (\\(d+1\\) terminal nodes, can involve at most \\(d\\) variables) to the training data \\((X,r)\\). 2.2. Update \\(\\hat(f)\\) by adding a shrunken version of the new tree: \\[\n\\hat{f}(x) \\leftarrow \\hat{f}(x) +\\lambda \\hat{f}^b(x).\n\\] 2.3. Update the residuals \\[\nr_i \\leftarrow r_i -\\lambda \\hat{f}^b(x_i).\n\\]\nOutput the boosted model \\[\n  \\hat{f}(x)=\\sum_{b=1}^B \\lambda \\hat{f}^b (x).\n\\] Each new tree can be rather small (\\(d=1\\) or 2, hence with low variance) controlled by the parameter \\(d\\). By fitting a small tree to the residual, we slowly improve \\(\\hat{f}\\) in areas where it does not perform well. The shrinkage \\(\\lambda\\) slows the learning, allowing more and different shaped trees to attack the residuals.\n\n\n\n8.5.2 Tuning parameters for Boosting\n\nThe number of trees \\(B\\): unlike bagging and random forests, boosting can overfit if \\(B\\) is too large, although overfitting tends to occur slowly. \\(B\\) is selected with cross-validation.\nThe shrinkage parameter \\(\\lambda\\): Typical values are 0.01 or 0.001. Very small \\(\\lambda\\) may require very large \\(B\\).\nThe number of splits \\(d\\): \\(d\\) controls the complexity of the boosted ensemble. Often \\(d=1\\) works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only one variable, hence easy to interpret. \\(d\\) is the interaction depth, controls the interaction order of the boosted model, since \\(d\\) splits can involve at most \\(d\\) variables.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#bayesian-additive-regression-trees-bart",
    "href": "ch8.html#bayesian-additive-regression-trees-bart",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.6 Bayesian Additive Regression Trees (BART)",
    "text": "8.6 Bayesian Additive Regression Trees (BART)\nBART is related to the approaches used by both bagging and boosting. We only make use of the original data (not using bootstrap) and their modified version (residuals from other trees) , and grow trees sequentially.\n\neach tree tries to capture the signal not yet accounted by for by the current model, as in boosting.\neach tree is constructed in a random manner as in bagging and RF\n\nThe main novelty of BART is the way in which new trees are generated. Assume there are \\(K\\) trees and \\(B\\) iterations. Let \\(\\hat{f}^b_k(x)\\) be the prediction at \\(x\\) for the \\(k\\)th tree used in the \\(b\\)th iteration.\nInitially, BART initializes all trees to be a single root node, with \\(\\hat{f}^1_k(x)=\\frac{1}{nK}\\sum_{i=1}^n y_i\\). Thus \\(\\hat{f}^1(x)= \\sum_{k=1}^K\\hat{f}^1_k(x)=\\frac{1}{n}\\sum_{i=1}^n y_i\\).\nIn subsequent iteration, BART updates each of the \\(K\\) trees, one at a time. In the \\(b\\)-th iteration, to update the \\(k\\)th tree, obtain a partial residual by subtracting from each response \\(y_i\\) the predictions from all but the \\(k\\)-th tree, \\[\nr_i = y_i -\\sum_{k'&lt;k}\\hat{f}^b_{k'}(x_i) - \\sum_{k'&gt;k}\\hat{f}^{b-1}_{k'}(x_i)\n\\] for each observation \\(i=1, \\cdots, n\\). Note when \\(k'&lt;k\\), the trees are updated already in the \\(b\\)-th iteration, and for \\(k'&gt;k\\), the trees are from the previous iteration \\(b-1\\). Rather than fitting a fresh tree to this partial residual \\(r_i\\), BART obtain a new tree \\(\\hat{f}^b_k\\) by randomly perturb the tree \\(\\hat{f}^{b-1}_k\\) from the \\((b-1)\\)-th iteration via the following operations:\n\nchange the structure of \\(\\hat{f}^{b-1}_k\\) by adding or pruning branches\nkeep the same structure of \\(\\hat{f}^{b-1}_k\\) but perturb the prediction values.\n\nPerturbations that improve the fit are favored. The perturbation only modifies the previous tree slightly hence guard against overfitting. In addition, the size of each tree is limited to avoid overfitting. The perturbation can be interpreted as drawing a new tree from a posterior distribution via Markov chain Monte Carlo sampling. The perturbation avoids local minima and achieve a more thorough exploration of the model space.\nAt the end of each iteration, the \\(K\\) trees from that iteration will be summed, i.e. \\(\\hat{f}^b(x)=\\sum_{k=1}^K \\hat{f}^b_k(x)\\) for \\(b=1, \\cdots, B\\).\nFinally, computer the mean (or other quantities such as percentile) after \\(L\\) burn-in samples: \\[\n\\hat{f}(x) = \\frac{1}{B-L} \\sum^B_{b=L+1} \\hat{f}^b(x).\n\\] During the burn-in period- the first \\(L\\) iterations, \\(\\hat{f}^{\\ell}\\), \\(\\ell &lt;=L\\) tends not to provide good results, hence are discarded.\nBART has very impressive out-of-box performance: perform well (not overfitting) with minimal tuning.\nParameters:\n\nnumber of trees, \\(K\\): e.g., \\(K=200\\)\nnumber of iterations: \\(B\\): e.g., \\(B=1000\\)\nburn-in iterations \\(L\\): e.g., \\(L=100\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#homework",
    "href": "ch8.html#homework",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.7 Homework:",
    "text": "8.7 Homework:\n\nConceptual: 1–6\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch8.html#code-snippet",
    "href": "ch8.html#code-snippet",
    "title": "8  Chapter 8: Tree-Based Methods",
    "section": "8.8 Code Snippet",
    "text": "8.8 Code Snippet\n\n8.8.1 Python\n\n\n\n\n8.8.2 Numpy\nnp.asarray(D)\n\n\n\n\n8.8.3 Pandas\nfeature_imp.sort_values(by='importance', ascending=False)\n\n\n8.8.4 Graphics\n\n\n\n\n8.8.5 ISLP and statsmodels\n\n\n\n\n8.8.6 sklearn\n\n\n8.8.7 Useful code snippets\n\n8.8.7.1 Classification Decision Tree\nfrom sklearn.metrics import (accuracy_score,\n                             log_loss)\n                             \nclf = DTC(criterion='entropy',\n          max_depth=3,\n          random_state=0)        \nclf.fit(X, High)\naccuracy_score(High, clf.predict(X))\nresid_dev = log_loss(High, clf.predict_proba(X))\nax = subplots(figsize=(12,12))[1]\nplot_tree(clf,\n          feature_names=feature_names,\n          ax=ax);\nprint(export_text(clf,\n                  feature_names=feature_names,\n                  show_weights=True))\n                  \n# Using validaiton approach to train and test the model\nvalidation = skm.ShuffleSplit(n_splits=1,\n                              test_size=200,\n                              random_state=0)\nresults = skm.cross_validate(clf,\n                             D,\n                             High,\n                             cv=validation)\nresults['test_score']\n\n\n\n8.8.7.2 Pruning a classifcaiton Decision tree\n(X_train,\n X_test,\n High_train,\n High_test) = skm.train_test_split(X,\n                                   High,\n                                   test_size=0.5,\n                                   random_state=0)\nclf = DTC(criterion='entropy', random_state=0)\nclf.fit(X_train, High_train)\naccuracy_score(High_test, clf.predict(X_test))\nccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\nkfold = skm.KFold(10,\n                  random_state=1,\n                  shuffle=True)\ngrid = skm.GridSearchCV(clf,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True, # Refit the best estimator with the entire dataset\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X_train, High_train)\ngrid.best_score_\nbest_ = grid.best_estimator_\nbest_.tree_.n_leaves\nprint(accuracy_score(High_test,\n                     best_.predict(X_test)))\nconfusion = confusion_table(best_.predict(X_test),\n                            High_test)\n\n\n\n8.8.7.3 Fitting a regression tree\nreg = DTR(max_depth=3)\nreg.fit(X_train, y_train)\nax = subplots(figsize=(12,12))[1]\nplot_tree(reg,\n          feature_names=feature_names,\n          ax=ax);\n\n\n\n8.8.7.4 Pruning a regression tree\nccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\nkfold = skm.KFold(5,\n                  shuffle=True,\n                  random_state=10)\ngrid = skm.GridSearchCV(reg,\n                        {'ccp_alpha': ccp_path.ccp_alphas},\n                        refit=True,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\nG = grid.fit(X_train, y_train)\nbest_ = grid.best_estimator_\nnp.mean((y_test - best_.predict(X_test))**2)\n\n\n\n8.8.7.5 Bagging and RG\nbag_boston = RF(max_features=X_train.shape[1], random_state=0, n_estimators=500)\nbag_boston.fit(X_train, y_train)\ny_hat_bag = bag_boston.predict(X_test)\n\n\n#RF\nRF_boston = RF(max_features=6,\n               random_state=0).fit(X_train, y_train)\ny_hat_RF = RF_boston.predict(X_test)\nnp.mean((y_test - y_hat_RF)**2)\n\n#VI\nfeature_imp = pd.DataFrame(\n    {'importance':RF_boston.feature_importances_},\n    index=feature_names)\nfeature_imp.sort_values(by='importance', ascending=False)\n\n\n\n8.8.7.6 Gradient Boosting\nboost_boston = GBR(n_estimators=5000,\n                   learning_rate=0.001,\n                   max_depth=3,\n                   random_state=0)\nboost_boston.fit(X_train, y_train)\ntest_error = np.zeros_like(boost_boston.train_score_)\nfor idx, y_ in enumerate(boost_boston.staged_predict(X_test)):\n   test_error[idx] = np.mean((y_test - y_)**2)\n\nplot_idx = np.arange(boost_boston.train_score_.shape[0])\nax = subplots(figsize=(8,8))[1]\nax.plot(plot_idx,\n        boost_boston.train_score_,\n        'b',\n        label='Training')\nax.plot(plot_idx,\n        test_error,\n        'r',\n        label='Test')\nax.legend();\n\n\n\n8.8.7.7 BART\nbart_boston = BART(random_state=0, burnin=5, ndraw=15) #num_trees=200, max_states=100\n# ndraw: number of iterations or samples to draw from the posterior distribution after the burn-in \nbart_boston.fit(X_train, y_train)\nyhat_test = bart_boston.predict(X_test.astype(np.float32))\nnp.mean((y_test - yhat_test)**2)\n\n# Variable Inclusion\nvar_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),\n                               index=D.columns)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 8: Tree-Based Methods</span>"
    ]
  },
  {
    "objectID": "ch9.html#what-is-a-hyperplane",
    "href": "ch9.html#what-is-a-hyperplane",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.1 What is a hyperplane?",
    "text": "9.1 What is a hyperplane?\nA hyperplane is defined by the following linear equation \\[\n\\beta_0+ \\beta_1X_1 +\\cdots +\\beta_pX_p=0.\n\\] It is a \\(p-1\\) dimension flat affine subspace (affines means not necessarily pass the origin). - When \\(p=2\\), it is a line. - When \\(\\beta_0\\), it passes through the origin, becomes a subspace. - The normal vector \\(\\beta=(\\beta_1, \\beta_2, \\cdots, \\beta_p)\\) is perpendicular to the hyperplane. - Let \\(f(X)= \\beta_0+ \\beta_1X_1 +\\cdots +\\beta_pX_p\\), then \\(f(X)=0\\) defines the hyperplane which separte the space into two halves, and for points on one side of the hyperplane, \\(f(X)&gt;0\\), and vice versa. - If we code \\(Y_i=1\\) for \\(f(X_i)&gt;0\\), and \\(Y_i=-1\\) for \\(f(X_i)&lt;0\\), then we always have \\[\nY_i\\cdot f(X_i)&gt;0 \\qquad \\text{for all } i.\n\\] If \\(f(x^*)\\) is far from zero, then we are more confident that the test point \\(x^*\\) belongs to a class.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#maximal-margin-classifier-optimal-seperating-hyperplane",
    "href": "ch9.html#maximal-margin-classifier-optimal-seperating-hyperplane",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.2 Maximal Margin Classifier (Optimal Seperating Hyperplane)",
    "text": "9.2 Maximal Margin Classifier (Optimal Seperating Hyperplane)\nWhen the data can be perfectly separated using a hyperplane, among all infinitely many separating hyperplanes, the maximal margin classifier makes the biggest gap or margin between two classes. It is the solution of the following convex quadratic program \\[\n\\text{maximize}_{\\beta_0, \\beta_1, \\cdots, \\beta_p}M\n\\] \\[\n\\text{subject to } \\sum_{j=1}^p {\\beta_j}^2 =1\\qquad \\text{and}\\qquad y_i(\\beta_0+\\beta_1x_{xi}+\\cdots + \\beta_px_{ip})\\ge M \\text{ for all }i=1, \\cdots, n\n\\] The constraints guarantees that each observation will be on the correct side of the hyperplane. The normalizing constraint allows to interpret \\[\ny_i(\\beta_0+\\beta_1x_{xi}+\\cdots + \\beta_px_{ip})\n\\] to be the perpendicular distance from observation \\(i\\) to the hyperplane. Hence \\(M\\) represents the margin of our hyperplane. In a sense, the maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes. Maximal margin classifier may lead to overfitting when \\(p\\) is large.\nThe observations that lie along the margin are called support vectors. They affect the optimal separating hyperplane. Other observations that outside the separating margin do not affect the optimal separating plane, provided their movement do not cause them to cross the boundary set by the margin.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#support-vector-classifier-soft-margin-classifier",
    "href": "ch9.html#support-vector-classifier-soft-margin-classifier",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.3 Support Vector Classifier (soft margin classifier)",
    "text": "9.3 Support Vector Classifier (soft margin classifier)\nOften time: - the data is not separable by a linear plane, hence there is no maximal margin classifier. - or the data is noisy, and a poor maximal margin separating plane is obtained. leading to a classifier that is sensitive to a single observation (overfitting).\nA support Vector Classifier maximizes a soft margin to almost separates the classes: \\[\n\\text{maximize}_{\\beta_0, \\beta_1, \\cdots, \\beta_p}M, \\text{ subject to } \\sum_{j=1}^p {\\beta_j}^2 =1\n\\] and \\[\ny_i(\\beta_0+\\beta_1x_{xi}+\\cdots + \\beta_px_{ip})\\ge M(1-\\epsilon_i), \\text{ where } \\epsilon_i\\ge 0, \\sum_{i=1}^n \\epsilon_i \\le C.\n\\tag{9.1}\\] The soft margin may be violated by some observations. \\(\\epsilon_i\\) are slack varaibles that individual observations to be on the wrong side of the margin or the hyperplane. - If \\(\\epsilon_i=0\\), then the \\(i\\)-th observation is on the correct side of the margin. - If \\(1&gt;\\epsilon&gt;0\\), then the \\(i\\)-th observation is on the wrong side of the margin. - If \\(\\epsilon &gt;1\\), then it is on the wrong side of the hyperplane.\n\\(C\\) is a regulation parameter that can be tuned with cross-validation, bounding the sum of \\(\\epsilon_i\\)’s, i.e., it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. So \\(C\\) is a budget for such violations. - If \\(C=0\\), then all \\(\\epsilon_i=0\\) for each \\(i\\), and the support vector classifier becomes the maximal margin hyperplane. - If \\(C&gt;0\\), then no more than \\(C\\) observation can be on the wrong side of the hyperplane then \\(\\epsilon_i&gt;1\\). As \\(C\\) increases, more tolerant to the violations leading to wider margin, and more support vectors.\nSo \\(C\\) controls the bias-variance trade-off - When \\(C\\) is small, less tolerance and smaller margin, the classifier may highly fit the data, hence small bias but high variance.\nSimilar to the maximal margin classfier, the support vector classfier is only affected by the support vector points on the margin or that violate the margin, robust to the points that are far away from the hyperplane. This is in contrast to some other classifiers such as LDA which needs a class mean of all within class observations, and a within-class covariance computed using all observations.\nOn the other hand, support vector classifer is very similar to Logistic regression, which is also not sensitive to observations far from the decision boundary.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#feature-basis-expansion-for-nonlinear-decision-boundary",
    "href": "ch9.html#feature-basis-expansion-for-nonlinear-decision-boundary",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.4 Feature (Basis) Expansion for nonlinear decision boundary",
    "text": "9.4 Feature (Basis) Expansion for nonlinear decision boundary\nSometimes, a linear boundary can fail no matter what \\(C\\) takes on. One way to fix this is to enlarge the feature space by including transformations such as \\(X_1^2, X_1^3, X_1X_2\\), \\(X_1X_2^2, \\cdots\\). Hence go from a \\(p\\) dimension space to a higher dimension space. This results in non-linear decision boundaries in the original space. For example, the adding of \\(X_1, X_2, X_1^2, X_2^2 X_1X2\\) would have decision boundary of the form \\[\n\\beta_0+\\beta_1X_1+\\beta_2X_2 +\\beta_3X_1^2+\\beta_4X_2^2 + \\beta_5X_1X_2=0.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#kennel-trick-and-support-vector-machines",
    "href": "ch9.html#kennel-trick-and-support-vector-machines",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.5 Kennel trick and Support Vector Machines",
    "text": "9.5 Kennel trick and Support Vector Machines\nThe kernel trick is simply an efficient computational approach that enacting enlarging the feature space. The linear support vector classifier can be represented by \\[\nf(x)=\\beta_0+ \\sum_{i=1}^n \\alpha_i \\langle x, x_i \\rangle\n\\] The parameters \\(\\alpha_i\\) can be estimated on a training set by computing the \\(n \\choose 2\\) inner products between pairwise training examples \\(\\langle x_i, x'_i \\rangle\\). It turns out most \\(\\hat{\\alpha}_i\\) are zeros, and \\[\nf(x)=\\beta_0+ \\sum_{i\\in S}^n \\hat{\\alpha}_i \\langle x, x_i \\rangle\n\\] Where \\(S\\) is the support set of indices \\(i\\) such that \\(\\hat{\\alpha}_i&gt;0\\). Note all \\(\\hat{\\alpha}_i\\ge 0\\). The inner product \\(\\langle x, x_i \\rangle\\) can be rewritten as a linear kernel (linear on the feature \\(x\\))\n\\[\nK(x,x_i)=\\langle x,x_i  \\rangle\n\\] thus the linear support vector classifier becomes \\[\nf(x) = \\beta_0 + \\sum_{i\\in S} \\hat{\\alpha}_i K(x, x_i).\n\\] The linear kernal function essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. Generalize this idea, one could use other form of kernels to measure the similarity.\nFor a nonlinear boundary determined by a polynomial of degree \\(d\\) feature space with \\(p\\) variables, the kernel function is given by \\[\nK(x_i, x_{i'}) = \\left( 1+ \\sum_{j=1}^p x_{ij}x_{i'j}  \\right)^d\n\\] When the support vector classifier is combined with a non-linear kernel, the resulting classifier is a support vector machine (SVM). It essentially fits a support vector classifier in a higher dimensional space involving polynomials of degree \\(d\\).\nThe kernel function will allow easy computation for the inner product of the \\(p+d \\choose d\\) monomial basis functions without explicitly working in the enlarged feature space. This is important because in many applications, the enlarged feature space is large so that the computations are intractable. For some other kernel such as radial kernel, the feature space is implicit and infinite-dimensional.\nA quick proof of the fact that there are \\(p+d \\choose d\\) monomial basis funtions for the space of polynomials of degree \\(d\\) in \\(p\\) variables.\n\nUse the stars and bars method, it is easy to see that for a fixed degree \\(\\delta\\), there are \\[{p+\\delta-1 \\choose p-1} = {p +\\delta -1 \\choose \\delta}\\] monomials. This can be understood as distributing \\(p-1\\) bars separating \\(\\delta\\) starts (each representing one degree) into \\(p\\) bins, each bin representing a variable.\nAdding the number of monomials for \\(\\delta=0, 1, \\cdots, d\\), \\[\n{p+0-1 \\choose 0 } + {p+1-1 \\choose 1 }+\\cdots +{p+d-1 \\choose d }\n\\] Rewrite \\({p+0-1 \\choose 0 }\\) as \\({p+1-1 \\choose 0 }\\) and apply the Pascal formula \\[\n{n \\choose k}= {n-1 \\choose k-1} + {n \\choose k-1}\n\\] repeatedly, to obtain the sum is \\({p+d \\choose d}\\).\n\nA final note is that SVM can also be used for regression, known as support vector regression, in which SVR seeks coefficents (\\(\\beta_j\\)) that minimize the a loss where only residuals larger in absolute value than some positive constant contributes to the loss.\nOther Common Used Kernels - Radial Kernel \\[\nK(x_i,x_i')=\\exp(-\\gamma \\sum_{j=1}^p (x_{ij}-x_{i'j})^2)\n\\] When a test point \\(x^*\\) is far from a training example \\(x_i\\), then the kernel function value is small and plays little role in \\(f(x^*)\\). So radial kernel has a local behavior: in the sense that only nearby points have an effect on the class label of a test observation.\nAs \\(\\gamma\\) increases (fewer local training points are included in a decision), the fit becomes more non-linear (local) and the training error decreases.\nHow to apply SVM to multiple class classification\n\nOVA (one-vs-rest): One vs All: Fit \\(K\\) different 2-class SVM classifiers \\(\\hat{f}_k(x)\\), \\(k=1, \\cdots, K\\). Classify \\(x^*\\) to the class for which \\(\\hat{f}_k(x^*)\\) is the largest, as this amounts to a high level of confidence that the test observation belogs to.\nOVO (all-pairs): One vs. One: Fit all \\(K \\choose 2\\) pairwise classifiers \\(\\hat{f}_{k\\ell}(x)\\). Classify \\(x^*\\) to the class that wins the most pairwise competitions.\n\nIf \\(K\\) is not too large, use OVO.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#svm-vs.-logistic-regression",
    "href": "ch9.html#svm-vs.-logistic-regression",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.6 SVM vs. Logistic Regression",
    "text": "9.6 SVM vs. Logistic Regression\nSVM can be viewed as minimizing the following hinge loss \\[\n\\text{minimize}_{\\beta_0, \\beta_1, \\cdots, \\beta_p} \\left\\{\\sum_{i=1}^n \\max[0, 1-y_if(x_i)] +\\lambda\\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\] When \\(\\lambda\\) is large, then \\(\\beta_j\\)’s are small, more violations to the margin are tolerated, and a low-variance and high-bias classier will result. A small value of \\(\\lambda\\) amounts to a small value of \\(C\\) in Equation 9.1.\nThe hinge loss function is very similar to the negative log-likelihood loss for the logistic regression. The loss function \\(\\text{minimize}_{\\beta_0, \\beta_1, \\cdots, \\beta_p} \\sum_{i=1}^n \\max[0, 1-y_if(x_i)]\\) is zero when \\(y_i(\\beta_0+\\beta_1x_{i1}+\\cdots +\\beta_px_{ip})\\ge 1\\); Theses corresponds to when an observation in on the correct side of the margin. In contrast, the loss function for logistic equation is not exactly zero anywhere, but it is very small for observations that are far from the decision boundary.\n\nWhen classes are nearly separable, SVM does better than LR, so does LDA. When not, LR is preferred.\nwhen not, LR with ridge penalty and SVM are very similar.\nIf wish to estimate probability, then LR is the choice.\nFor nonlinear boundary, kernel SVMs are popular. Can use kernels with LR and LDA as well, but computations are more expensive.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#homework",
    "href": "ch9.html#homework",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.7 Homework:",
    "text": "9.7 Homework:\n\nConceptual: 1–3\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch9.html#code-snippet",
    "href": "ch9.html#code-snippet",
    "title": "9  Chapter 9: Support Vector Machine",
    "section": "9.8 Code Snippet",
    "text": "9.8 Code Snippet\n\n9.8.1 Python\nroc_curve = RocCurveDisplay.from_estimator # shorthand for the function from_estimator()\n\n\n\n9.8.2 Numpy\n\n\n\n\n\n9.8.3 Pandas\n\n\n\n9.8.4 Graphics\nfrom matplotlib.pyplot import subplots, cm #cm for colormap\nax.scatter(X[:,0],\n           X[:,1],\n           c=y,\n           cmap=cm.coolwarm);\n\n\n\n9.8.5 ISLP and statsmodels\n\n\n\n\n9.8.6 sklearn\n\n\n9.8.7 Useful code snippets\n\n9.8.7.1 SVM\nsvm_linear_small = SVC(C=0.1, kernel='linear')\nsvm_linear_small.fit(X, y)\nfig, ax = subplots(figsize=(8,8))\nplot_svm(X,\n         y,\n         svm_linear_small,\n         ax=ax)\nsvm_linear.coef_\nsvm_linear.intercept_\n\n### Tuning a parameter\nkfold = skm.KFold(5, \n                  random_state=0,\n                  shuffle=True)\ngrid = skm.GridSearchCV(svm_linear,\n                        {'C':[0.001,0.01,0.1,1,5,10,100]}, # 7 values\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy')\ngrid.fit(X, y)\ngrid.best_params_\ngrid.cv_results_\ngrid.cv_results_[('mean_test_score')] #[yw] the () can be omitted\n\n###prediciton and test error\nbest_ = grid.best_estimator_\ny_test_hat = best_.predict(X_test)\nconfusion_table(y_test_hat, y_test)\nconfusion_table(y_test, y_test_hat)\n\n#### Radial Basis kernel\nsvm_rbf = SVC(kernel=\"rbf\", gamma=1, C=1)\nsvm_rbf.fit(X_train, y_train)\n\nkfold = skm.KFold(5, \n                  random_state=0,\n                  shuffle=True)\ngrid = skm.GridSearchCV(svm_rbf,\n                        {'C':[0.1,1,10,100,1000],\n                         'gamma':[0.5,1,2,3,4]},\n                        refit=True,\n                        cv=kfold,\n                        scoring='accuracy');\ngrid.fit(X_train, y_train)\ngrid.best_params_\n\n\n\n\n\n9.8.7.2 ROC curve\nfig, ax = subplots(figsize=(8,8))\nroc_curve(best_svm,\n          X_train,\n          y_train,\n          name='Training',\n          color='r',\n          ax=ax);\n\n\n9.8.7.3 SVM with multiple classes\nsvm_rbf_3 = SVC(kernel=\"rbf\",\n                C=10,\n                gamma=1,\n                decision_function_shape='ovo');\nsvm_rbf_3.fit(X, y)\nfig, ax = subplots(figsize=(8,8))\nplot_svm(X,\n         y,\n         svm_rbf_3,\n         scatter_cmap=cm.tab10,\n         ax=ax)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 9: Support Vector Machine</span>"
    ]
  },
  {
    "objectID": "ch10.html#single-layer-neural-network",
    "href": "ch10.html#single-layer-neural-network",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.1 Single Layer Neural Network",
    "text": "10.1 Single Layer Neural Network\nThe name neural network originally derived from thinking of the hidden units as analogous to neurons in the brain. Consider the NN consisting of input layer, one hidden layer and a single output regression unit. Then the output \\[\\begin{align}\nY=f(X)   = & \\beta_0 +\\sum_{k=1}^K\\beta_k h_k(X) \\\\\n  = & \\beta_0+ \\sum_{k=1}^K \\beta_k g(w_{k0}+\\sum_{j=1}^p w_{kj}X_j)\n\\end{align}\\] where,\n\n\\(A_k =h_k(X)= g(w_{k0}+\\sum_{j=1}^p w_{kj}X_j)\\) are the activations in the hidden layer: they are simply nonlinear transformation via \\(g\\) of an affine transformation of the input features. Each \\(A_k\\) may be understood as a basis function. The success of NN lies in that \\(A_k\\) are not prescribed, rather are learned from data by learning the coefficients \\(w_{kj}\\).\n\\(g\\) is a activate function. E.g.: sigmoid (for binary output unit), softmax (for multi-class output), linear (for regression) or ReLU (for hidden layers). The activation functions typically in the hidden layers are nonlinear, allowing to model complex nonlinearities and interactions. otherwise the model collapses to a linear model.\nFor regression, the model is fit by minimizing the RSS loss \\(\\sum_{i=1}^n (y_i-f(x_i))^2\\). non-convex\nFor classification , if there are \\(M\\) classes, and the logit output for class \\(m\\) is \\[\nZ_m = \\beta_{m0}+ \\sum_{\\ell=1}^K\\beta_{m\\ell}A_\\ell\n\\] where \\(K\\) is the number of activation nodes in the previous layer. The output activation function encodes the softmax function \\[\nf_m(X)= Pr(Y=m|X)=\\frac{e^{Z_m}}{\\sum_{\\ell=0}^M e^{Z_\\ell}}\n\\]\n\nThe model is then fit by minimizing the negative log-liklihood (or cross-entropy) \\[\n-\\sum_{i=1}^n \\sum_{m=1}^M y_{im}\\log(f_m(x_i))\n\\] where \\(y_{im}\\) is one-hot coded.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#multi-layer-nn",
    "href": "ch10.html#multi-layer-nn",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.2 Multi-layer NN",
    "text": "10.2 Multi-layer NN\nIn theory, a single hidden layer with a large number of units has the ability to approximate most functions (universal approximator). However, with multi-layers each of smaller size, the computation is reduced and better solution is obtained.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#cnn-for-image-classificaiton",
    "href": "ch10.html#cnn-for-image-classificaiton",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.3 CNN for Image classificaiton",
    "text": "10.3 CNN for Image classificaiton\nClinches its success in CV such as classifying images. The CNN builds up an image in a hierarchical fashion. Edges and shapes are recognized in lower layers and piece together to form more complex shapes, eventually assembling the target image. The hierarchical construction is achieved by convolution to discover spatial structure. Convolution allows for parameter sharing and finding common small patterns that occur in different parts of the image (feature translation invariance), typically followed by ReLU, sometimes separately called a detector layer) and pooling (to summarize, for down-sampling to select a prominent subset and allowing location invariance).\nCNN convolves a small filter (image, typically small, e.g., \\(3\\times 3\\)) representing a small shape, edge, etc. with an input image by sliding the filer around the input image, scoring the match by dot-product. the more match, the higher the score is. Each filter has the same number of channels as that of the input layer. The filter is typically learned by the network via a learning algorithm. The result of the convolution is a new feature map. The convolved image highlights regions of the original image that resemble the convolution filter.\nArchitecture of a CNN - many convolve-then-pool layers. Sometimes, we repeat several convolve layers before a pool layer. This effectively increases the dimension of the filter. - each filter creates a new channel in the convolution layer. - As pooling reduces the size, the number of filters/channel is typically increased. - network can be very deep. - As pooling has reduced each channel feature map down to a few pixels in each dimension, at the point, the 3D feature maps are flattened, and fed into one or more FC layers before reaching to the output layey.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#rnn-and-lstm",
    "href": "ch10.html#rnn-and-lstm",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.4 RNN and LSTM",
    "text": "10.4 RNN and LSTM\nThere are many sequence data such as sentence, time series, speech, music etc. RNN build models that take into account the sequential nature of the data and build a memory of the past.\n\nthe feature for each observation is a sequence of vectors \\(X=\\{X_1, X_2, \\cdots, X_L \\}\\)\nthe target \\(Y\\): a single variable (e.g. binary variable), one-hot vector (multiclass). Can also be a sequence (seq2seq), e.g., translation in a different language.\nThe hidden layer is a sequence of vectors \\(A_\\ell\\) receiving \\(X_\\ell\\) and \\(A_{\\ell-1}\\) as inputs and output \\(O_\\ell\\). The weight matrices are shared at different time step, hence the name recurrent. \\(A_\\ell\\) accumulates a history of what has been seen and represents an evolving model that is updated when \\(X_\\ell\\) is processed.\nSuppose \\(X_\\ell=(X_{\\ell 1},\\cdots, X_{\\ell p} )\\), and \\(A_\\ell=(A_{\\ell 1}), \\cdots, A_{\\ell K}\\), then \\(A_{\\ell k}\\) and \\(O_\\ell\\) are computed by \\[\nA_{\\ell k} = g \\left(w_{k0} + \\sum_{j=1}^p w_{kj}X_{\\ell j} + \\sum_{s=1}^K u_{ks}A_{\\ell-1 s}   \\right)\n\\] \\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\] If we are only interested in the predicting \\(O_L\\) at the last unit, then for squared error loss, and \\(n\\) sequence/response pairs (examples), we minimize \\[\n\\sum_{i=1}^n (y_i-O_{iL})^2=\\sum_{i=1}^n \\left( y_i- ( \\beta_0+ \\sum_{k=1}^K \\beta_k g(w_{k0} + \\sum_{j=1}^p w_{kj}x^{[i]}_{\\ell j} + \\sum_{s=1}^K u_{ks}a^{[i]}_{\\ell-1 s} )) \\right)^2\n\\]\nDeep RNN: having more than one hidden layers in an RNN. The sequence \\(A_\\ell\\) is treated as an input sequence to the next hidden layers.\nin LSTM, two tracks of hidden layer activation are maintained; each \\(A_\\ell\\) receive the short memory \\(A_{\\ell -1}\\), as well as from a long memory that reaches further back in time.\nbi-drectional RNN",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#applications",
    "href": "ch10.html#applications",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.5 Applications",
    "text": "10.5 Applications\n\n10.5.1 Language models\nApplication: Sentiment Analysis (document classification)\n\n10.5.1.1 Bag-of-words\nHow to create features for a document contains a sequence of \\(L\\) words?\n\nForm a dictionary, e.g. most frequently used 10K words e.g., occuring in the training documents\ncreate a binary vector of length \\(p=10K\\) for each document, and score 1 in every position that the corresponding word occurred. (Bag-of-words)\nWith \\(n\\) documents, this will create a \\(n \\times p\\) sparse feature matrix.\nBag-of-words are unigrams, we can also use bigrams (occurrences of adjacent word pairs), and in general \\(m\\)-grams$, to take into account the context.\none could also record the relative frequency of words.\n\n\n\n10.5.1.2 Word embeddings\n\nEach document is represented as a sequence of words \\(\\{{\\mathcal W}_\\ell \\}_{\\ell=1}^L\\). Typically we truncate/pad the documents to the same number of \\(L\\) words (e.g. L= 512)\nEach word is represented as a one-hot encoded binary vector of \\(X_\\ell\\) of length \\(10K\\), extremely sparse, would not work well.\nUse an embedding layer (either pre-trained (trained on large corpus by such as PCA, such as word2vec, or GloVe) or learned specifically as part of the optimization) to obtain a lower-dimensional word embedding matrix \\({\\mathbf E}\\) (\\(m\\times 10K\\)) to convert each word’s binary feature vector of length 10K to a real feature vector of dimension of length \\(m\\) (e.g. 128, 256, 512, 1024. )\n\n\n\n\n10.5.2 Transfering leraning\nBy freezing the weights of one or a few top layers of a pretrained NN, one can train a new model by only training the last few layers with much less training data, yet obtain a good new model. This is because the feature maps (knowledge) learned in the hidden layer may be transferred to a similar task.\n\n\n10.5.3 Time Series\n\nautocorrelation at lag \\(\\ell\\): is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) time interval apart.\norder-L autoregression model (\\(AR(L)\\)): \\[\n\\hat{v}_t= \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\cdots +  \\hat{\\beta}_L v_{t-L}.\n\\] The model can be fit by OLS.\nUse RNN to model a time series by exacting many short mini-series of the form \\(X=\\{ X_1, X_2, \\cdots, X_L\\}\\), and a corresponding target \\(Y\\).\ntime series can also be modelled using 1-D CNN.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#when-to-use-deep-learning",
    "href": "ch10.html#when-to-use-deep-learning",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.6 When to use Deep Learning",
    "text": "10.6 When to use Deep Learning\n\nCNN big success in CV: e.g. image recolonization\nRNN success in sequence data: e.g.: language translation\nwhen dataset is large, overfitting is not a problem\nOccam’s razor: among algorithms performing equally well, the simpler is preferred as it is easier to interpret.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#fitting-a-nn-gradient-descent",
    "href": "ch10.html#fitting-a-nn-gradient-descent",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.7 Fitting a NN: Gradient Descent",
    "text": "10.7 Fitting a NN: Gradient Descent\nLet the loss be \\(R(\\theta)\\), where \\(\\theta\\) is the parameter to be optimized such that the loss is minimized. The loss \\(R\\) is typically a non-convex function of the parameters, hence there might be multiple solutions and many local minima. The gradient method updates the parameter by \\[\n\\theta^{t+1} = \\theta_t - \\rho \\nabla R(\\theta^t)\n\\] where \\(\\rho\\) is a learning rate, a hyper-parameter, e.g. \\(\\rho=0.01\\); and \\(\\nabla R(\\theta^t) =\\frac{\\partial R(\\theta)}{\\partial \\theta}|_{\\theta = \\theta^t}\\) is the gradient of \\(R\\). The gradient can be found by the backproparation using the chain rule. The backpropation distributes a fraction of residual \\(y_i-f_\\theta(x_i)\\) at each observation \\(i\\) to each parameter via the hidden units. Modern software such as Tensorflow or PyTorch can easily compute the gradient of a function.\nWhen overfitting is detected, training stops. Since \\(R\\) is non-convex, in general we can hope to end up at a good local minimum.\n\nthe learning rate \\(\\rho\\) must be carefully chosen, typically cannot be too large. Early stopping ( a kind of regularization) may help.\nminibatch: rather than using all data each step to update the parameter, draw a random minibatch sample at each step to update the parameter via gradient descent. Such a method is called SGD. Minibatch size is a hyperparameter, e.g. 128. It balances bias and variance. It turns out SGD imposes a regularization similar to ridge.\nepoch: One epoch sweeps through the entire training data set with the number of minibatch subsets that is determined by\n\\[ \\text{number of minibatches in one epoch} = \\frac{n}{\\text{minibatch size}} \\]\nregularization: lasso, ridge; the hyperparameter \\(\\lambda\\) may vary for different layers.\ndropout: at each SGD update, randomly remove units (by setting their activations zero) with probability \\(\\phi\\) (reduce number of variables hence variance \\(\\phi\\) may vary for different layers), and scale up those retained by \\(1/(1-\\phi)\\) to compensate. Dropout has similar effect to ridge.\ndata augmentation: make many copies of \\((x_i, y_i)\\), distort each copy by\n\nadding a small amount of noise (e.g. Gaussian) to the \\(x_i\\),\nzooming, horizontal and vertical shifting, shearing, small rotation, flipping.\n\nbut leave \\(y_i\\) alone. This effectively has increased the training set. This make the model robust to small perturbation in \\(x_i\\), equivalent to ridge. especially effective with SGD in CV with minibatch where augmented images are added on-the-fly without the need to store them.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#interpolation-and-double-descent",
    "href": "ch10.html#interpolation-and-double-descent",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.8 Interpolation and Double Descent",
    "text": "10.8 Interpolation and Double Descent\n\nFor a OLS model, When the degree of freedom of a model \\(d&lt;=n\\), the number of examples, we see usual bias-variance trade-off. When \\(d=n\\), it’s an interpolating polynomial, very wiggly.\nwhen \\(d&gt;n\\), the training error is zero, and there are no unique solutions. Among the zero-residual solutions, if pick the minimum-norm (hence the smoothest) solution, with the increased dof, it’s easy for the model not only fit the training data, but also decreased \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) (there is no need to have large \\(\\beta_j\\) to fit the training data), leads to solutions actually generalize well with small variance (on test data). An interpolating model may perform better than a slightly less complex model that does not interpolate the data. This phenomenon is called double descent.\n\nSuch a minimum norm solution may be obtained by SGD with a small learning rate. In this case, the SGD solution path is similar to ridge path. - By analogy, deep and wide NN fit by SGD down to zero training erro often give good solutions that generalize well. - In particular cases with high signal-to-noise-ratio (SNR = \\(\\frac{Var[f(x)]}{\\sigma^2}\\)), where \\(f\\) is the signal and \\(\\sigma^2\\) is the noise variance (irreducible error). e.g., image recognition, the NN is less prone to overfitting.\n\nDouble descent doesn’t contradict the bias-variance trade-off. rather it reveas that the number of basis functions does not properly capture the true model “complexity”. In other words, a minimum norm solution with \\(d\\) dof has lower flexibility than the model with \\(d\\) dof.\n\nMost statistical learning method with regularization do not exhibit double descent, as they do not interpolate data, but still achieve good result.\n\nMaximal margin classifier and SVM that have zero training error often achieve very good test error, this is because they seek smooth minimum norm solutions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#homework",
    "href": "ch10.html#homework",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.9 Homework:",
    "text": "10.9 Homework:\n\nConceptual: 1–5\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch10.html#code-snippet",
    "href": "ch10.html#code-snippet",
    "title": "10  Chapter 10: Deep Learning",
    "section": "10.10 Code Snippet",
    "text": "10.10 Code Snippet\n\n10.10.1 Python\n#### in Windows the code below is true. \n'logs\\\\hitters\\\\version_0\\\\metrics.csv' == r'logs\\hitters\\version_0\\metrics.csv'\ndel Hitters  # delete the object\n\n[f for f in glob('book_images/*')] # get a list of file names from the dir: book_images\n\n' '.join(lookup[i] for i in sample_review)\n\n\n\n10.10.2 Numpy\nX_test.astype(np.float32)\ncoefs = np.squeeze(coefs)\n\n\n\n10.10.3 Pandas\nY = Hitters['Salary'].to_numpy()\n\nlabs = json.load(open('imagenet_class_index.json'))\nclass_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n                           labs.items()],\n                           columns=['idx', 'label'])\nclass_labels = class_labels.set_index('idx')\nclass_labels = class_labels.sort_index() # sort the rows of a pandas dataframe by index\n\nimg_df = img_df.sort_values(by='prob', ascending=False)[:3]\nimg_df.reset_index().drop(columns=['idx'])\n\npd.merge(X, \n                 pd.get_dummies(NYSE['day_of_week']),\n                 on='date')\n                 \nX = X.reindex(columns=ordered_cols)                 \n\n\n\n10.10.4 Graphics\n\n\n\n\n10.10.5 ISLP and statsmodels\n\n\n\n\n10.10.6 sklearn\n\n10.10.6.1 Linear Regression\nhit_lm = LinearRegression().fit(X_train, Y_train)\nYhat_test = hit_lm.predict(X_test)\nnp.abs(Yhat_test - Y_test).mean()\n\nM.score(X[~train], Y[~train])  # M is a lm, .score for R^2. \n\n\n10.10.6.2 Lasso\nscaler = StandardScaler(with_mean=True, with_std=True)\nlasso = Lasso(warm_start=True, max_iter=30000)\nstandard_lasso = Pipeline(steps=[('scaler', scaler),\n                                 ('lasso', lasso)])\n\n### Calculate the lambda values\nX_s = scaler.fit_transform(X_train)\nn = X_s.shape[0]\nlam_max = np.fabs(X_s.T.dot(Y_train - Y_train.mean())).max() / n \nparam_grid = {'alpha': np.exp(np.linspace(0, np.log(0.01), 100))\n             * lam_max}\n             \n\ncv = KFold(10,\n           shuffle=True,\n           random_state=1)\ngrid = GridSearchCV(lasso,\n                    param_grid,\n                    cv=cv,\n                    scoring='neg_mean_absolute_error')\ngrid.fit(X_train, Y_train);\n\ntrained_lasso = grid.best_estimator_\nYhat_test = trained_lasso.predict(X_test)\nnp.fabs(Yhat_test - Y_test).mean()\n\n\n10.10.6.3 torch for non-linear regression\nclass HittersModel(nn.Module):\n\n    def __init__(self, input_size): #input_size = feature_dim\n        super(HittersModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_size, 50),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(50, 1))\n\n    def forward(self, x):\n        x = self.flatten(x)\n        return torch.flatten(self.sequential(x))\n\nhit_model = HittersModel(X.shape[1])\n\nsummary(hit_model, \n        input_size=X_train.shape,\n        col_names=['input_size',\n                   'output_size',\n                   'num_params']) # indicate the columns included in the summary\n#### Form dataset\nX_train_t = torch.tensor(X_train.astype(np.float32))\nY_train_t = torch.tensor(Y_train.astype(np.float32))\nhit_train = TensorDataset(X_train_t, Y_train_t)\nX_test_t = torch.tensor(X_test.astype(np.float32))\nY_test_t = torch.tensor(Y_test.astype(np.float32))\nhit_test = TensorDataset(X_test_t, Y_test_t)\n\nmax_num_workers = rec_num_workers()\n\n#### Form data module\nhit_dm = SimpleDataModule(hit_train,\n                          hit_test, #test dataset\n                          batch_size=32,\n                          num_workers=min(4, max_num_workers),\n                          validation=hit_test)\n### setup optimizer, loss function and additional error metrics\nhit_module = SimpleModule.regression(hit_model, # using default square loss for training\n                           metrics={'mae':MeanAbsoluteError()}) # additional metric\n#### set up traning logger\nhit_logger = CSVLogger('logs', name='hitters')\n\n#### Training the model\nhit_trainer = Trainer(deterministic=False, # deterministic=True is not working when using GPU\n                      max_epochs=50,\n                      log_every_n_steps=5,\n                      logger=hit_logger,\n                      callbacks=[ErrorTracker()])\nhit_trainer.fit(hit_module, datamodule=hit_dm)\n\n#### Evaluate the test error \nhit_trainer.test(hit_module, datamodule=hit_dm)\n\n### Make prediction\nhit_model.eval() \npreds = hit_module(X_test_t)\ntorch.abs(Y_test_t - preds).mean()\n\n\n\n\n10.10.6.4 Torch for nonlinear classificaiton\n### Data Module\nmnist_dm = SimpleDataModule(mnist_train,\n                            mnist_test,\n                            validation=0.2,\n                            num_workers=max_num_workers,\n                            batch_size=256)\nclass MNISTModel(nn.Module):\n    def __init__(self):\n        super(MNISTModel, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28*28, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4))\n        self.layer2 = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3))\n        self._forward = nn.Sequential(\n            self.layer1,\n            self.layer2,\n            nn.Linear(128, 10))\n    def forward(self, x):\n        return self._forward(x)\n\nmnist_model = MNISTModel()\nsummary(mnist_model,\n        input_data=X_, #also ok X_.shape or [256, 1, 28, 28]\n        col_names=['input_size',\n                   'output_size',\n                   'num_params'])\n### Setup loss, optimizer, additional metrics\nmnist_module = SimpleModule.classification(mnist_model,\n                                           num_classes=10)\nmnist_logger = CSVLogger('logs', name='MNIST')\n\n### Model training\nmnist_trainer = Trainer(deterministic=False,\n                        max_epochs=30,\n                        logger=mnist_logger,\n                        callbacks=[ErrorTracker()])\nmnist_trainer.fit(mnist_module,\n                  datamodule=mnist_dm)\n\n### Evaluating test error\nmnist_trainer.test(mnist_module,\n                   datamodule=mnist_dm)\n                   \n\n\n\n10.10.6.5 Using Torch for multi-class Logistic Regression\nclass MNIST_MLR(nn.Module):\n    def __init__(self):\n        super(MNIST_MLR, self).__init__()\n        self.linear = nn.Sequential(nn.Flatten(),\n                                    nn.Linear(784, 10))\n    def forward(self, x):\n        return self.linear(x)\n\nmlr_model = MNIST_MLR()\nmlr_module = SimpleModule.classification(mlr_model,\n                                         num_classes=10)\nmlr_logger = CSVLogger('logs', name='MNIST_MLR')\n\nmlr_trainer = Trainer(deterministic=False,\n                      max_epochs=30,\n                      callbacks=[ErrorTracker()])\nmlr_trainer.fit(mlr_module, datamodule=mnist_dm)\nmlr_trainer.test(mlr_module,\n                 datamodule=mnist_dm)\n\n\n\n10.10.6.6 Torch for classificaiton (Binary Sentiment Analsyis)\nmax_num_workers=10\n(imdb_train,\n imdb_test) = load_tensor(root='data/IMDB')\nimdb_dm = SimpleDataModule(imdb_train,\n                           imdb_test,\n                           validation=2000,\n                           num_workers=min(6, max_num_workers),\n                           batch_size=512)\n                           \nclass IMDBModel(nn.Module):\n\n    def __init__(self, input_size):\n        super(IMDBModel, self).__init__()\n        self.dense1 = nn.Linear(input_size, 16)\n        self.activation = nn.ReLU()\n        self.dense2 = nn.Linear(16, 16)\n        self.output = nn.Linear(16, 1)\n\n    def forward(self, x):\n        val = x\n        for _map in [self.dense1,\n                     self.activation,\n                     self.dense2,\n                     self.activation,\n                     self.output]:\n            val = _map(val)\n        return torch.flatten(val)\n\nimdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\nsummary(imdb_model,\n        input_size=imdb_test.tensors[0].size(),\n        col_names=['input_size',\n                   'output_size',\n                   'num_params'])\n\n\nimdb_optimizer = RMSprop(imdb_model.parameters(), lr=0.001)\nimdb_module = SimpleModule.binary_classification(\n                         imdb_model,\n                         optimizer=imdb_optimizer)\n\nimdb_logger = CSVLogger('logs', name='IMDB')\nimdb_trainer = Trainer(deterministic=False,\n                       max_epochs=30,\n                       logger=imdb_logger,\n                       callbacks=[ErrorTracker()])\nimdb_trainer.fit(imdb_module,\n                 datamodule=imdb_dm)\n                 \ntest_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n\n\n\n10.10.6.7 Torch with CNN\ncifar_dm = SimpleDataModule(cifar_train, #torch TensorDataSet\n                            cifar_test,\n                            validation=0.2,\n                            num_workers=max_num_workers,\n                            batch_size=128)\n\nclass BuildingBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n\n        super(BuildingBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=(3,3),\n                              padding='same')\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n\n    def forward(self, x):\n        return self.pool(self.activation(self.conv(x)))\n\nclass CIFARModel(nn.Module):\n\n    def __init__(self):\n        super(CIFARModel, self).__init__()\n        sizes = [(3,32),\n                 (32,64),\n                 (64,128),\n                 (128,256)]\n        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n                                    for in_, out_ in sizes])\n\n        self.output = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(2*2*256, 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512, 100))\n    def forward(self, x):\n        val = self.conv(x)\n        val = torch.flatten(val, start_dim=1) # flatten starting from dim=1 (default), the first dim is batch dim\n        return self.output(val)\n\ncifar_model = CIFARModel()\nsummary(cifar_model,\n        input_data=X_,\n        col_names=['input_size',\n                   'output_size',\n                   'num_params'])\n\n### define loss, optimizer, etc\ncifar_optimizer = RMSprop(cifar_model.parameters(), lr=0.001)\ncifar_module = SimpleModule.classification(cifar_model,\n                                    num_classes=100,\n                                    optimizer=cifar_optimizer)\ncifar_logger = CSVLogger('logs', name='CIFAR100')\n\n# Training\ncifar_trainer = Trainer(deterministic=False,\n                        max_epochs=30,\n                        logger=cifar_logger,\n                        callbacks=[ErrorTracker()])\ncifar_trainer.fit(cifar_module,\n                  datamodule=cifar_dm)\n\ncifar_trainer.test(cifar_module,\n                   datamodule=cifar_dm)\n                   \n\n\n10.10.6.8 Transfer learning\n### Pre-processing\nresize = Resize((232,232), antialias=True) #target size: (232,232)\ncrop = CenterCrop(224) #centered and cropped to target size 224\nnormalize = Normalize([0.485,0.456,0.406],   # mean for each channel\n                      [0.229,0.224,0.225])   # std for each channel\nimgfiles = sorted([f for f in glob('book_images/*')])\nimgs = torch.stack([torch.div(crop(resize(read_image(f))), 255) # element-wise div by 255\n                    for f in imgfiles])\nimgs = normalize(imgs)\n\nresnet_model = resnet50(weights=ResNet50_Weights.DEFAULT) # get the model\nresnet_model.eval()\nimg_preds = resnet_model(imgs) #logit values\n\nimg_probs = np.exp(np.asarray(img_preds.detach())) # convert to propbabilities \nimg_probs /= img_probs.sum(1)[:,None] # the sum is along the row. \n\n\n\n\n10.10.6.9 RNN/LSTM with Torch for Documenation Classification\nimdb_seq_dm = SimpleDataModule(imdb_seq_train,\n                               imdb_seq_test,\n                               validation=2000,\n                               batch_size=300,\n                               num_workers=min(6, max_num_workers)\n                               )\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(input_size, 32)\n        self.lstm = nn.LSTM(input_size=32,\n                            hidden_size=32,\n                            batch_first=True)\n        self.dense = nn.Linear(32, 1)\n    def forward(self, x):\n        val, (h_n, c_n) = self.lstm(self.embedding(x))\n        return torch.flatten(self.dense(val[:,-1])) # select the last time step of val\n\nlstm_model = LSTMModel(X_test.shape[-1])\nsummary(lstm_model,\n        input_data=imdb_seq_train.tensors[0][:10],\n        col_names=['input_size',\n                   'output_size',\n                   'num_params'])\n        \nlstm_module = SimpleModule.binary_classification(lstm_model)\nlstm_logger = CSVLogger('logs', name='IMDB_LSTM')\n\nlstm_trainer = Trainer(deterministic=False,\n                       max_epochs=20,\n                       logger=lstm_logger,\n                       callbacks=[ErrorTracker()])\nlstm_trainer.fit(lstm_module,\n                 datamodule=imdb_seq_dm)\nlstm_trainer.test(lstm_module, datamodule=imdb_seq_dm)\n\n\n\n10.10.6.10 RNN/LSTM with Torch for Time Seires Prediciton\nclass NYSEModel(nn.Module):\n    def __init__(self):\n        super(NYSEModel, self).__init__()\n        self.rnn = nn.RNN(3, # number of features\n                          12,\n                          batch_first=True)\n        self.dense = nn.Linear(12, 1)\n        self.dropout = nn.Dropout(0.1)\n    def forward(self, x):\n        val, h_n = self.rnn(x)\n        val = self.dense(self.dropout(val[:,-1]))\n        return torch.flatten(val)\nnyse_model = NYSEModel()\n\ndatasets = []\nfor mask in [train, ~train]:\n    X_rnn_t = torch.tensor(X_rnn[mask].astype(np.float32))\n    Y_t = torch.tensor(Y[mask].astype(np.float32))\n    datasets.append(TensorDataset(X_rnn_t, Y_t))\nnyse_train, nyse_test = datasets\n\nnyse_dm = SimpleDataModule(nyse_train,\n                           nyse_test,\n                           num_workers=min(4, max_num_workers),\n                           validation=nyse_test,\n                           batch_size=64)\n\nnyse_optimizer = RMSprop(nyse_model.parameters(),\n                         lr=0.001)\nnyse_module = SimpleModule.regression(nyse_model,\n                                      optimizer=nyse_optimizer,\n                                      metrics={'r2':R2Score()})                           \n\nnyse_trainer = Trainer(deterministic=False,\n                       max_epochs=200,\n                       callbacks=[ErrorTracker()])\nnyse_trainer.fit(nyse_module,\n                 datamodule=nyse_dm)\nnyse_trainer.test(nyse_module,\n                  datamodule=nyse_dm)\n\n\n\n10.10.6.11 Linear and Nonlinear AR with Torch\n### Nonlinear-AR model\nday_dm = SimpleDataModule(day_train,\n                          day_test,\n                          num_workers=min(4, max_num_workers),\n                          validation=day_test,\n                          batch_size=64)\n\nclass NonLinearARModel(nn.Module):\ndef __init__(self):\n    super(NonLinearARModel, self).__init__()\n    self._forward = nn.Sequential(nn.Flatten(), #flatten a multi-dim tensor into a 1-d tensor while keeping the batch size\n    nn.Linear(20, 32),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(32, 1))\ndef forward(self, x):\n    return torch.flatten(self._forward(x))\n\nnl_model = NonLinearARModel()\nnl_optimizer = RMSprop(nl_model.parameters(),\n                           lr=0.001)\nnl_module = SimpleModule.regression(nl_model,\n                                        optimizer=nl_optimizer,\n                                        metrics={'r2':R2Score()})\n                                        \nnl_trainer = Trainer(deterministic=False,\n                         max_epochs=20,\n                         callbacks=[ErrorTracker()])\nnl_trainer.fit(nl_module, datamodule=day_dm)\nnl_trainer.test(nl_module, datamodule=day_dm) \n\n\n\n\n10.10.7 Useful code snippets\n\n10.10.7.1 Plotting training/validation learning curve\ndef summary_plot(results,\n                 ax,\n                 col='loss',\n                 valid_legend='Validation',\n                 training_legend='Training',\n                 ylabel='Loss',\n                 fontsize=20):\n    for (column,\n         color,\n         label) in zip([f'train_{col}_epoch',\n                        f'valid_{col}'],\n                       ['black',\n                        'red'],\n                       [training_legend,\n                        valid_legend]):\n        results.plot(x='epoch',\n                     y=column,\n                     label=label,\n                     marker='o',\n                     color=color,\n                     ax=ax)\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel(ylabel)\n    return ax\n    \nfig, ax = subplots(1, 1, figsize=(6, 6))\nax = summary_plot(hit_results,\n                  ax,\n                  col='mae',\n                  ylabel='MAE',\n                  valid_legend='Validation (=Test)')\nax.set_ylim([0, 400])\nax.set_xticks(np.linspace(0, 50, 11).astype(int));\n\n\n\n10.10.7.2 Viewing a set of images\nfig, axes = subplots(5, 5, figsize=(10,10))\nrng = np.random.default_rng(4)\nindices = rng.choice(np.arange(len(cifar_train)), 25,\n                     replace=False).reshape((5,5))\nfor i in range(5):\n    for j in range(5):\n        idx = indices[i,j]\n        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n                                      [1,2,0]), # transpose the channel to the last dim for display \n                                      interpolation=None)\n        axes[i,j].set_xticks([])\n        axes[i,j].set_yticks([])\n\n\n10.10.7.3 using sk-learn LogisticRegression() with Lasso\n#### Defining the \\lambda\nlam_max = np.abs(X_train.T * (Y_train - Y_train.mean())).max() # this is not divided by n, different that in the Lasso chapter 6. \nlam_val = lam_max * np.exp(np.linspace(np.log(1),\n                                       np.log(1e-4), 50))\n                                       \nlogit = LogisticRegression(penalty='l1', \n                           C=1/lam_max,\n                           solver='liblinear',\n                           warm_start=True,\n                           fit_intercept=True)\n                           \ncoefs = []\nintercepts = []\n\nfor l in lam_val:\n    logit.C = 1/l\n    logit.fit(X_train, Y_train)\n    coefs.append(logit.coef_.copy())\n    intercepts.append(logit.intercept_)   \n    \n\n\n\n10.10.7.4 Viewing the Lasso results with lambda values\nfig, axes = subplots(1, 2, figsize=(16, 8), sharey=True)\nfor ((X_, Y_),\n     data_,\n     color) in zip([(X_train, Y_train),\n                    (X_valid, Y_valid),\n                    (X_test, Y_test)],\n                    ['Training', 'Validation', 'Test'],\n                    ['black', 'red', 'blue']):\n    linpred_ = X_ * coefs.T + intercepts[None,:]\n    label_ = np.array(linpred_ &gt; 0)\n    accuracy_ = np.array([np.mean(Y_ == l) for l in label_.T])\n    axes[0].plot(-np.log(lam_val / X_train.shape[0]), #lambda is rescaled by diving N for only plotting. WHy?\n                 accuracy_,\n                 '.--',\n                 color=color,\n                 markersize=13,\n                 linewidth=2,\n                 label=data_)\naxes[0].legend()\naxes[0].set_xlabel(r'$-\\log(\\lambda)$', fontsize=20)\naxes[0].set_ylabel('Accuracy', fontsize=20)\n\n\n10.10.7.5 Insert lags to a time series\nfor lag in range(1, 6):\n    for col in cols:\n        newcol = np.zeros(X.shape[0]) * np.nan\n        newcol[lag:] = X[col].values[:-lag]\n        X.insert(len(X.columns), \"{0}_{1}\".format(col, lag), newcol)#insert at the end\nX.insert(len(X.columns), 'train', NYSE['train']) #insert the col training identifier \n\nX = X.dropna() # drop rows with nan\n\n\n10.10.7.6 Preparing time series data for RNN in Torch\nY, train = X['log_volume'], X['train']\nX = X.drop(columns=['train'] + cols)\n\nordered_cols = []\nfor lag in range(5,0,-1):\n    for col in cols:\n        ordered_cols.append('{0}_{1}'.format(col, lag))\nX = X.reindex(columns=ordered_cols)\n\nX_rnn = X.to_numpy().reshape((-1,5,3))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Chapter 10: Deep Learning</span>"
    ]
  },
  {
    "objectID": "ch11.html#survial-and-censoring-time",
    "href": "ch11.html#survial-and-censoring-time",
    "title": "11  Chapter 11: Survival Analysis",
    "section": "11.1 Survial and Censoring Time",
    "text": "11.1 Survial and Censoring Time\n\nWe observe either \\(T\\) or \\(C\\), i.e., we observe the r.v. \\[\nY= \\min (T,C)\n\\]\nDefine the status indicator \\[\n\\delta = \\begin{cases}\n1 & \\text{ if } T\\le C \\\\\n0 & \\text{ if } T &gt; C\n\\end{cases}\n\\]\nData set format: \\(n\\) pairs \\((y_1,\\delta_1), \\cdots, (y_n, \\delta_n)\\).\nindependent censoring: In general, we assume that conditional on the features, \\(T\\) is independent of \\(C\\).\nIn some situation, the above assumption is false. For example, When a patient drop out a study because the patient is too sick. In this case obtained \\(C\\) may lead to overestimate of \\(T\\).\nright censoring: when \\(Y\\le T\\).\nleft censoring: when \\(T\\le Y\\).\ninterval censoring: The exact \\(T\\) is not know, but we know it falls in some interval.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 11: Survival Analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#the-survival-curve",
    "href": "ch11.html#the-survival-curve",
    "title": "11  Chapter 11: Survival Analysis",
    "section": "11.2 The Survival Curve",
    "text": "11.2 The Survival Curve\n\nSurvival function \\[\nS(t) = Pr(T&gt;t)\n\\] is a decreasing function that quantifies the probability of surviving past time \\(t\\). For example, if \\(T\\) represents the time when a customer churns, then \\(S(t)\\) is the probability a customer cancels later than \\(t\\). The larger \\(S(t)\\) is, the less likely that the customer will cancel before time \\(t\\).\n\n\n11.2.1 How to estimate \\(S(t)\\)\n\n11.2.1.1 Kaplan-Meier Survival Curve\nLet \\(d_1&lt;d_2&lt;\\cdots&lt; d_K\\) be the unique death (event) times among the non-censored patients, \\(r_k\\) is the number of patients (risk set) at risk (still alive) at \\(d_k\\), and \\(q_k\\) is the number of patients who died at \\(d_k\\).\nThe idea is to use sequential construction. Use the total probability, \\[\nPr(T&gt;d_k) =Pr(T&gt;d_k|T&gt;d_{k-1})Pr(T&gt;d_{k-1})+Pr(T&gt;d_k|T\\le d_{k-1}) Pr(T\\le d_{k-1})\n\\] Note \\(Pr(T&gt;d_k|T\\le d_{k-1}) =0\\). Therefore \\[\nS(d_k) = Pr(T&gt;d_k|T&gt;d_{k-1})S(d_{k-1}) = Pr(T&gt;d_k|T&gt;d_{k-1})\\times \\cdots \\times Pr(T&gt;d_2|T&gt;d_{1})Pr(T&gt;d_1)\n\\] It is natural to estimate \\[\n\\hat{Pr}(T&gt;d_j|T&gt;d_{j-1})=(r_j-q_j)/r_j\n\\] This leads to the Kaplan-Meier estimator \\[\n\\hat{S}(d_k) =\\prod_{j=1}^k\\left(\\frac{r_j-q_j}{r_j} \\right)\n\\] for \\(d_j&lt;d_{k+1}\\), set \\(\\hat{S}(t) =\\hat{S}(d_k)\\), this leads to a step function.\n\n\n11.2.1.2 the log-rank test to compare the survial curves of two groups\nFurther, let \\(r_{ik}\\) be the number of patients who are at risk at \\(d_k\\) and \\(q_{ik}\\) be the number of patients who died at \\(d_k\\), for group \\(i\\), \\(i=1,2\\). \\[r_{1k}+r_{2k} =r_k \\qquad q_{1k}+q_{2k}=q_k \\] We can construct the following 2X2 table at each \\(d_k\\):\n\n\n\n\nGroup 1\nGroup 2\nTotal\n\n\n\n\nDied\n\\(q_{1k}\\)\n\\(q_{2k}\\)\n\\(q_k\\)\n\n\nSurvival\n\\(r_{1k}-q_{1k}\\)\n\\(r_{2k} -q_{2k}\\)\n\\(r_k-q_k\\)\n\n\nTotal\n\\(r_{1k}\\)\n\\(r_{2k}\\)\n\\(r_k\\)\n\n\n\nTo test \\(H_0: E(X)=\\mu\\) for a rv \\(X\\), construct the statistic \\[\nW = \\frac{X-E(X)}{\\sqrt{Var(X)}}\n\\] where \\(E(X)\\) and \\(Var(X)\\) are under \\(H_0\\). Let \\(X=\\sum_{k=1}^K q_{1k}\\), the total death of group 1 up to time \\(d_K\\). Assume that \\(q_{1k}\\) are uncorrelated. Plug in the formula for \\(X\\), one can obtain \\[\nW=\\frac{\\sum_{k=1}^K(q_{1k}-E(q_{1k}))}{\\sqrt{\\sum_{k=1}^K Var(q_{1k})}} = \\frac{\\sum_{k=1}^K(q_{1k}- \\frac{q_k}{r_k}r_{1k})}{\\sqrt{\\sum_{k=1}^K \\frac{q_k(r_{1k}/r_k)(1-r_{1k}/r_k)(r_k-q_k)}{r_k - 1}  } }\n\\] When the sample size is large, \\(W\\) has approximately a standard normal distribution. The null Hypothesis for the log-rank test is that there is no difference between the survival curves in the two groups.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 11: Survival Analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#regression-models-with-a-survival-response",
    "href": "ch11.html#regression-models-with-a-survival-response",
    "title": "11  Chapter 11: Survival Analysis",
    "section": "11.3 Regression models with a survival response",
    "text": "11.3 Regression models with a survival response\nThe goal is to predict the survival time \\(T\\). The observation is \\((Y, \\delta)\\). However fit a regression of \\(\\log Y\\) with \\(Y=\\min(T,C)\\) on \\(X\\) is not sound because of the censoring. To overcome this difficulty, use the idea of sequential construction again as in Kaplan-Meier survival curve. To this end, define the hazard function (also called hazard rate, or force of mortality) as \\[\nh(t) =\\lim_{\\Delta t\\to 0} \\frac{Pr(t&lt;T\\le t+\\Delta t |T&gt;t)}{\\Delta t}\n\\] where \\(T\\) is the (true) survival time. The hazard rate is the death rate (in fact the pdf for \\(T\\) conditional on \\(T&gt;t\\)) in the instant after \\(t\\), given survival up to that time. By definition of \\(h\\), we can derive \\[\nh(t) =\\frac{f(t)}{S(t)}\n\\] where \\[f(t)= \\lim_{\\Delta t \\to 0} \\frac{Pr(t&lt;T \\le t+\\Delta t)}{\\Delta t}\\] is the pdf associated with \\(T\\). Specifically, \\[\nf(t)=\\frac{d}{dt}F(t)=\\frac{d}{dt}[1-S(t)]=\\frac{d}{dt} [1-\\int_{0}^t h(u)du]\n\\] The likelihood associated with the \\(i\\)-th observation is [ \\[\\begin{align}\nL_i &= \\begin{cases}\nf(y_i) & \\text{if the }i\\text{-th observation is not censored, } \\delta_i =1 \\\\\nS(y_i) & \\text{ if the }i\\text{-th observation is censored, } \\delta_i = 0\n\\end{cases}\\\\\n&=f(y_i)^{\\delta_i}S(y_i)^{1-\\delta_i}\n\\end{align}\\] ] Assume the \\(n\\) observations are independent, the likelihood for the data is \\[\nL = \\prod_{i=1}^n f(y_i)^{\\delta_i}S(y_i)^{1-\\delta_i}=\\prod_{i=1}^n h(y_i)^{\\delta_i}S(y_i).\n\\] Some reasonable assumptions for \\(f\\) is:\n\nExponential: \\(f(t) = \\lambda \\exp (-\\lambda t)\\)\nGamma or Weibull family\nnon-parametrically such as by Kaplan-Meirer estimator.\n\nWith \\(h(t)\\), define the Cox’s proportional hazard for an individual with feature vector \\(x_i=(x_{i1}, \\cdots,x_{ip})\\) by \\[\nh(t|x_i)= h_0(t)\\exp \\left(\\sum_{j=1}^p x_{ij}\\beta_j  \\right)\n\\] where, \\(h_0(t)\\ge 0\\) is the baseline hazard function for an individual with features \\(x_{i1}=\\cdots =x_{ip}=0\\). \\(h_0\\) can take any functional form. The term \\(\\sum_{j=1}^px{ij}\\beta_j\\) is called the relative risk. The model assumes that one-unit increase in \\(x_{ij}\\) corresponds to an increase in \\(h(t|x_i)\\) by a factor of \\(\\exp(\\beta_j)\\).\n\nThere is no intercept in the model, as it can be absorbed into \\(h_0(t)\\)\nThe model can easily handle time-dependent covariate: simply replace \\(x_{ij}\\) with \\(x_{ij}(t)\\) for the \\(j\\)-th covariate of \\(i\\)-th observation .\nTo check the proportional hazards assumption: for qualitative feature, plot the log hazard function for each level of the feature, the log hazard functions should differ by a constant. For quantitative feature, we can take a similar approach by stratifying the feature.\n\nWe cannot directly estate \\(\\beta_j\\) by maximum likelihood using \\(h(t|x_i)\\) because \\(h_0\\) is not known. But we can use partial likelihood. Use the same “sequential in time” logic, the probability that the \\(i\\)-th observation fails at time \\(y_i\\) is: \\[\n\\frac{h_0(y_i)\\exp \\left(\\sum_{j=1}^p x_{ij}\\beta_j \\right)}{\\sum_{i':y_{i'}\\ge y_i} h_0(y_i) \\exp\\left(  \\sum_{j=1}^p x_{i'j}\\beta_j \\right)}=\\frac{\\exp \\left(\\sum_{j=1}^p x_{ij}\\beta_j \\right)}{\\sum_{i':y_{i'}\\ge y_i} \\exp\\left(  \\sum_{j=1}^p x_{i'j}\\beta_j \\right)}\n\\] which is called relative risk function at \\(y_i\\).\nThe partial likelihood (approximation to the likelihood ) over all of the uncensored observations is \\[\nPL(\\beta) = \\prod _{i:\\delta_i = 1} \\frac{\\exp \\left(\\sum_{j=1}^p x_{ij}\\beta_j \\right)}{\\sum_{i':y_{i'}\\ge y_i} \\exp\\left(  \\sum_{j=1}^p x_{i'j}\\beta_j \\right)} = \\prod_{i:\\delta_i=1}RR_i(\\beta)\n\\] In the above formula, it is assumed there are no tied failure times. If there are, then the formula needs to be modified. To estimate \\(\\beta\\), simply maximize \\(PL(\\beta)\\). To estimate \\(\\beta\\), simply maximize \\(PL(\\beta)\\). Lasso or ridge penalty terms of \\(\\beta\\) can be added to obtain shrinkage estimate.\n\n11.3.1 Connection with the log-rank test\n\nfor the case of a single binary covariate, the score test for \\(H_0: \\beta=0\\) in Cox’s hazards model is exactly equal to the log-rank test.\n\n\n\n11.3.2 AUC for Survival Analysis: the C-index\n\nfor each observation, calculate the estimated risk score: \\[\n\\hat{\\eta}_i = \\hat{\\beta}_1x_{i1} +\\cdots + \\hat{\\beta}_px_{ip}, \\quad i=1, \\cdots, n\n\\]\ncompute Harrell’s concordance index (C-index): \\[\nC = \\frac{\\sum _{i,i':y_i&gt;y_{i'}}I(\\hat{\\eta}_{i'} &gt;\\hat{\\eta}_i) \\delta_{i'}}{\\sum _{i,i':y_i&gt;y_{i'}}\\delta_{i'}}\n\\] The C-index is the proportion of pairs for which the model correctly predicts the relative survival time, among all pairs for which this can be determined. E.g., \\(C=0.733\\) indicates given two random observations from the test set, the model can predict with \\(73.3\\%\\) accuracy which will survive.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 11: Survival Analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#homework",
    "href": "ch11.html#homework",
    "title": "11  Chapter 11: Survival Analysis",
    "section": "11.4 Homework:",
    "text": "11.4 Homework:\n\nConceptual: 1–\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 11: Survival Analysis</span>"
    ]
  },
  {
    "objectID": "ch11.html#code-snippet",
    "href": "ch11.html#code-snippet",
    "title": "11  Chapter 11: Survival Analysis",
    "section": "11.5 Code Snippet",
    "text": "11.5 Code Snippet\n\n11.5.1 Python\nrng = np.random.default_rng(10)\nN = 2000\nOperators = rng.choice(np.arange(5, 16),\n                       N,\n                       replace=True)\n                       \nnp.clip(W, 0, 1000)\nD['Failed'] = rng.choice([1, 0],\n                         N,\n                         p=[0.9, 0.1])\n\n\n\n11.5.2 Numpy\n\n\n11.5.3 Pandas\nBrainCancer['sex'].value_counts()\n\n#### return mean or mode of columns of a df\ndef representative(series):\n    if hasattr(series.dtype, 'categories'): # hasattr(object, attribute)\n        return pd.Series.mode(series)\n    else:\n        return series.mean()\nmodal_data = cleaned.apply(representative, axis=0)\n\n\n\n\n11.5.4 Graphics\n\n\n\n\n11.5.5 ISLP and statsmodels\nfrom lifelines import \\\n     (KaplanMeierFitter,\n      CoxPHFitter)\nfrom lifelines.statistics import \\\n     (logrank_test,\n      multivariate_logrank_test)\nfrom ISLP.survival import sim_time\n\n### Kaplan-Meier estimator\nfig, ax = subplots(figsize=(8,8))\nkm = KaplanMeierFitter()\nkm_brain = km.fit(BrainCancer['time'], BrainCancer['status'])\nkm_brain.plot(label='Kaplan Meier estimate', ax=ax)\n\n11.5.5.1 stratified K-M estimator\nfig, ax = subplots(figsize=(8,8))\nby_sex = {}\nfor sex, df in BrainCancer.groupby('sex'):\n    by_sex[sex] = df\n    km_sex = km.fit(df['time'], df['status'])\n    km_sex.plot(label='Sex=%s' % sex, ax=ax)\n\n\n11.5.5.2 Log-rank test\nlogrank_test(by_sex['Male']['time'],\n             by_sex['Female']['time'],\n             by_sex['Male']['status'],\n             by_sex['Female']['status'])\n\n\n\n11.5.5.3 Cox proportional Hazards model\ncoxph = CoxPHFitter # shorthand\nsex_df = BrainCancer[['time', 'status', 'sex']]\nmodel_df = MS(['time', 'status', 'sex'],\n              intercept=False).fit_transform(sex_df) #MS has coded 'sex` column to binary. Male[1] Female[0]\ncox_fit = coxph().fit(model_df,\n                      'time',\n                      'status')\ncox_fit.summary[['coef', 'se(coef)', 'p']]\n\ncox_fit.log_likelihood_ratio_test()\n\n# fit all variables\nall_MS = MS(cleaned.columns, intercept=False)\nall_df = all_MS.fit_transform(cleaned)\nfit_all = coxph().fit(all_df,\n                      'time',\n                      'status')\nfit_all.summary[['coef', 'se(coef)', 'p']]\nmodal_X = all_MS.transform(modal_df)\npredicted_survival = fit_all.predict_survival_function(modal_X)\nfig, ax = subplots(figsize=(8, 8))\npredicted_survival.plot(ax=ax);\n\n\n\n\n11.5.6 sklearn\n\n\n11.5.7 Useful code snippets",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chapter 11: Survival Analysis</span>"
    ]
  },
  {
    "objectID": "class_project.html",
    "href": "class_project.html",
    "title": "12  Class Project",
    "section": "",
    "text": "Goal\nto use various ML algorithms to predict a meaningful target \\(Y\\) by classification algorithms or regression algorithms. Present it at COS Research Sympsium in the end of April.\nData set: real-world stock price and volume data. We could start with just one stock, e.g. APAL, S&P 500, index, DJ index.\nSome ideas:\n\nCreate one model for each stock.\ncreate a single model for all stocks. Needs to embed each stock in a feature space. Research?\n\nStart-up code: refer to the page https://github.com/ywanglab/Predicting_stock_movement/blob/main/Time_series_stock_data_analysis_ver2.ipynb. Perform some EDA to feel the data.\nreference ticker symbols: https://gist.github.com/quantra-go-algo/ac5180bf164a7894f70969fa563627b2\nQuestions: Which are the \\(X\\) variables? price, volume, return, day of week, month of year, etc. What is the \\(Y\\) variable? next-day price, next-day return, next-five-day average price, next-five-day-average return, etc.\nModels\n\nLinear regression:\n\nincluding continuous variables (price, volume), categorical variables (day-of-week, month-of-year)\ntransforming \\(X\\) (for including non-linear relation between \\(Y\\) and \\(X\\)) or \\(Y\\) (when \\(Y\\) is heteroschedatic, i.e., with varied \\(\\epsilon_i\\))\nplot residual plot to see if \\(Var(\\epsilon_i)\\) is changing. If yes, may appeal to transforming \\(Y\\), e.g., \\(\\log Y\\), \\(\\sqrt{Y}\\).\ninvestigate outliers (points with unusual \\(Y\\)-values) using the residual plot or looking at studentized residual.\ninvestigate high leverage points (with unusual \\(x\\) values), by calculating leverage statistics.\nInvestigate if there is colinearity among the variables by calculating VIF.\n\nclassification: predicting directions of the stock price movement. binary (with two direction), or multinomial (more than two values: e.g., up, same, down), LDA, QDA\nregularization of the parameters: lasso (\\(L^1\\)), ridge (\\(L^2\\))\nselection of variables: forward, backward, mixture, regularization, cross-validation\ndecision tree: random forest, boosting\nSVM: support vector machines\nNN",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Class Project</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "James, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023.\nAn Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "References"
    ]
  }
]