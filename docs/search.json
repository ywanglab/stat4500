[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stat4500notes",
    "section": "",
    "text": "Preface\nThis is a Lecture note written for the course STAT 4500: Machine Learning offered at Auburn University at Montgomery. The course uses the textbook James et al. (2023).\nThis is a book wrtieen by Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#on-your-own-computer",
    "href": "intro.html#on-your-own-computer",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.1 on Your own computer",
    "text": "1.1 on Your own computer\n\nyou can either git clone or download a zipped file containing the codes from the site: https://github.com/intro-stat-learning/ISLP_labs/tree/stable. If downloaded a zipped file of the codes, unzipped the file to a folder, for example, named islp. If git clone (preferred, you need to have Git installed on your computer, check this link for how to install Git https://ywanglab.github.io/stat1010/git.html), do git clone https://github.com/intro-stat-learning/ISLP_labs.git\nDownload and install the following software:\n\nAnaconda: Download anaconda and install using default installation options\nVisual Studio Code (VSC): Download VSC and install\nstart VSC and install VSC extensions in VSC: Python, Jupyter, intellicode\n(optional) Quarto for authoring: Download Quarto and install\n\nCreate a virtual environment named islp for Python. Start an anaconda terminal.\n  conda create -n islp python==3.10\n  conda activate islp\n  conda install pip ipykernel\n  pip install -r https://raw.githubusercontent.com/intro-stat-learning/ISLP_labs/v2.1.2/requirements.txt\nYou are ready to run the codes using VSC or jupyter lab.\n\nActivate the venv: conda activate islp\nStart a Anaconda terminal, navigate to the folder using the command cd path/to/islp, where path/to/islp means the file path to the folder islp, such as \\Users\\ywang2\\islp. Start VSC by typing code . in the anaconda terminal.\nopen/create a .ipynb or .py file.\nSelect the kernel islp\nRun a code cell by pressing Shift+Enter or click the triangular play button.\nContinue to run other cells.\nAfter finishing using VSC, close the VSC, and deactivate the virtual environment in a conda terminal: conda deactivate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "intro.html#use-google-colab",
    "href": "intro.html#use-google-colab",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.2 Use Google Colab",
    "text": "1.2 Use Google Colab\nAll you need is a Google account. Sign in your Google account in a browser, and navigate to Google Colab. Google Colab supports both Python and R. Python is the default engine. Change the engine to R in Connect-&gt;change runtime type. Then you are all set. Your file will be saved to your Google Drive or you can choose to send it to your GitHub account (recommended).\n\n1.2.1 How to run a project file from your Google Drive?\nMany times, when you run a python file in Colab, it needs to access other files, such as data files in a subdirectory. In this case, it would be convenient to have the same file structure in the Google Colab user home directory. To do this, you can use Google Drive to store your project folder, and then mount the Google Drive in Colab.\nLet’s assume the project folder name, islp/.Here are the steps:\n\ngit clone the project folder (example: git clone https://github.com/intro-stat-learning/ISLP_labs.git) to your local folder. This step is only needed when you want to clone some remote repo from GitHub.\nUpload the folder (ex: islp) to Google Drive.\nOpen the file using Colab. In Google Drive, double click on the ipynb file, example, ch06.ipynb (or click on the three dots on the right end, and choose open with, then Google Colaborotary), the file will be opened by Google Colab.\nMount the Google Drive. In Google Colab, with the specific file (example, ch06.ipynb) being opened, move your cursor to the first code cell, and then click on the folder icon (this should be the fourth icon) on the upper left border in the Colab browser. This will open the file explorer pane. Typically you would see a folder named sample_data shown. On the top of the pane, click on the Google Drive icon to mount the Google Drive. Google Colab will insert the following code below the cursor in your opened ipynb file:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nRun this code cell by pressing SHIFT+ENTER, and follow the prompts to complete the authentication. Wait for ~10 seconds, your Google Drive will be mounted in Colab, and it will be displayed as a folder named drive in the file explorer pane. You might need to click on the Refresh folder icon to see the folder drive.\nOpen a new code cell below the above code cell, and type the code\n  %cd /content/drive/MyDrive/islp/\nThis is to change the directory to the project directory on the Google Drive. Run this code cell, and you are ready to run the file ch06.ipynb from the folder islp on your personal Google Drive, just like it’s on your local computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "ch2.html#what-is-statistical-learning",
    "href": "ch2.html#what-is-statistical-learning",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.1 What is statistical learning?",
    "text": "2.1 What is statistical learning?\nFor the input variable \\(X\\in \\mathbb{R}^p\\) and response variable \\(Y\\in \\mathbb{R}\\), assume that \\[Y=f(X) + \\epsilon, \\] where \\(\\epsilon\\) is a random variable representing irreducible error. We assume \\(\\epsilon\\) is independent of \\(X\\) and \\(E[\\epsilon]=0\\). \\(\\epsilon\\) may include unmeasured variables or unmeasurable variation.\nStatistical learning is to estimate \\(f\\) using various methods. Denote the estimate by \\(\\hat{f}\\).\n\nregression problem: when \\(Y\\) is a continuous (quantitative) variable . In this case \\(f(x)=E(Y|X=x)\\) is the population regression function, that is, regression finds a conditional expectation of \\(Y\\).\nclassification problem: when \\(Y\\) only takes small number of discrete values, i.e., qualitative (categorical).\n\nLogistic regression is a classification problem, but since it estimates class probability, it may be considered as a regression problem.\n\nsupervised learning: training data \\(\\mathcal{Tr}=\\{(x_i, y_i):i\\in \\mathbb{Z}_n\\}\\): linear regression, logistic regression\nunsupervised learning: when only \\(x_i\\) are available. clustering analysis, PCA\nsemi-supervised learning: some data with labels (\\(y_i\\)), some do not.\nreinforcement learning: learn a state-action policy function for an agent to interacting with an environment to maximize a reward function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#why-estimate-f",
    "href": "ch2.html#why-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.2 Why estimate \\(f\\)?",
    "text": "2.2 Why estimate \\(f\\)?\nWe can use estimated \\(\\hat{f}\\) to\n\nmake predictions for a new \\(X\\), \\[\\hat{Y} =\\hat{f}(X). \\] The prediction error may be quantified as \\[E[(Y-\\hat{Y})^2] = (f(X)-\\hat{f})^2 +\\text{Var}[\\epsilon].\\] The first term of the error is reducible by trying to improve \\(\\hat{f}\\), where we assume \\(f\\), \\(\\hat{f}\\) and \\(X\\) are fixed.\nmake inference, such as\n\nWhich predictors are associated with the response?\nwhat is the relationship between the response and each predictor?\nis the assumed relationship adequate? (linear or more complicated?)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-estimate-f",
    "href": "ch2.html#how-to-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.3 How to estimate \\(f\\)",
    "text": "2.3 How to estimate \\(f\\)\nWe use obtained observations called training data \\(\\{(x_k, y_k): k \\in \\mathbb{Z}_n \\}\\) to train an algorithm to obtain the estimate \\(\\hat{f}\\).\n\nParametric methods: first assume there is a function form (shape) with some parameters. For example, a linear regression model with two parameters. Then use the training data to train or fit the model to determine the values of the parameters.\nAdvantages: simplify the problem of fit an arbitrary function to estimate a set of parameters.\nDisadvantages: may not be flexible unless with large number of parameters and/or complex function shapes.\nExample: linear regression,\nNon-parametric methods: Do not explicitly assume a function form of \\(f\\). They seek to estimate \\(f\\) directly using data points, can be quite flexible and accurate.\n**Disadvantage: need large number of data points\nExample: KNN (but breakdown for higher dimention. Typically only for \\(p\\le 4\\)), spline fit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-assess-model-accuracy",
    "href": "ch2.html#how-to-assess-model-accuracy",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.4 How to assess model accuracy",
    "text": "2.4 How to assess model accuracy\nFor regression problems, the most commonly used measure is the mean squared error (MSE), given by \\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{f}(x_i))^2\n\\] For classification problems, typically the following error rate (classifications error) is calculated: \\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i\\ne \\hat{y}_i)\n\\] The accuracy on a training set can be arbitrarily increased by increasing the model flexibility. However, we are in general interested in the error on the test set rather on the training set, the model accuracy should be assessed on a test set.\nFlexible models tend to overfit the data, which essentially means they follow the error or noise too closely in the training set, therefore cannot be generalized to unseen cases (test set).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#model-selection",
    "href": "ch2.html#model-selection",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.5 Model Selection:",
    "text": "2.5 Model Selection:\nNo free lunch theorem\nThere is no single best method for all data sets, which means some method works better than other methods for a particular dataset. Therefore, one needs to perform model selections. Here are some principles.\n\n2.5.1 Trade-off between Model flexibility and Model Interpretability\nMore flexible models have higher degree of freedom and are less interpretable because it’s difficult to interpret the relationship between a predictor and the response.\nLASSO is less flexible than linear regression. GAM (generalized additive model) allows some non-linearity. Full non-linear models have higher flexibility, such as bagging, boosting, SVM, etc.\nWhen inference is the goal, then there are advantages to using simple and less flexible models for interpretability.\nWhen prediction is the main goal, more flexible model may be a choice. But sometimes, we obtain more accurate prediction using a simpler model because the underlying dataset has a simpler structure. Therefore, it is not necessarily true that a more flexible model has a higher prediction accuracy.\nOccam’s Razor: Among competing hypotheses that perform equally well, the one with the fewest assumptions should be selected.\n\n\n2.5.2 Model Selection: the Bias-Variance Trade-off\nAs the model flexibility increases, the training MSE (or error rate for classificiton) will decrease, but the test MSE (error rate) in general will not and will show a characteristic U-shape. This is because when evaluated at a test point \\(x_0\\), the expected test MSE can be decomposed into \\[\nE\\left[ (y_0-\\hat{f}(x_0))^2 \\right] = \\text{Var}[\\hat{f}(x_0)] + (\\text{Bias}(\\hat{f}(x_0)))^2+\\text{Var}[\\epsilon]\n\\] where the expectation is over different \\(\\hat{f}\\) on a different training set or on a different training step if the training process is stochastic, and \\[\n\\text{Bias}(\\hat{f}(x_0))= E[\\hat{f}(x_0)]-f(x_0)\n\\] To obtain the least test MSE, one must trade off between variance and bias. Less flexible model tendes to have higher bias, and more flexible models tend to have higher variance. An optimal flexibility for the least test MSE varies with different data sets. Non-linear data tends to require higher optimal flexibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#bayes-classifier",
    "href": "ch2.html#bayes-classifier",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.6 Bayes Classifier",
    "text": "2.6 Bayes Classifier\nIt can be shown that Bayes Classifier minimizes the classification test error \\[\n\\text{Ave}(I(y_0\\ne \\hat{y}_0)).\n\\] A Bayes Classifier assigns a test observation with predictor \\(x_0\\) to the class for which \\[\n\\text{Pr}(Y=j|X=x_0)\n\\] is largest. It’s error rate is given by \\[\n1-E[\\max_{j} \\text{Pr}(Y=j|X)]\n\\]\nwhere the expectation is over \\(X\\). The Bayes error is analogous to the irreducible error \\(\\epsilon\\).\nBayes Classifier is not attainable as we do not know \\(\\text{Pr}(Y|X)\\). We only can estimate \\(\\text{Pr}(Y|X)\\). One way to do this is by KNN. KNN estimate the conditional probability simply with a majority vote. The flexibility of KNN increases as \\(1/K\\) increases with \\(K=1\\) being the most flexible KNN. The training error is 0 for \\(K=1\\). A suitable \\(K\\) should be chosen for an appropriate trade off between bias and variance. The KNN classifier will classify the test point \\(x_0\\) based on the probability calculated from the \\(k\\) nearest points. KNN regression on the other hand will assign the test point \\(x_0\\) the average value of the \\(k\\) nearest neighbors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#homework-indicates-optional",
    "href": "ch2.html#homework-indicates-optional",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.7 Homework (* indicates optional):",
    "text": "2.7 Homework (* indicates optional):\n\nConceptual: 1,2,3,4*,5,6,7\nApplied: 8, 9*, 10*",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#code-gist",
    "href": "ch2.html#code-gist",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.8 Code Gist",
    "text": "2.8 Code Gist\n\n2.8.1 OS\nimport os\nos.chdir(path) # change dir\n\n\n2.8.2 Python:\nConcatenation using +\n\"hello\" + \" \" + \"world\"  # 'hello world'\n[3,4,5] + [4,9,7] # [3,4,5, 4,9,7]\n\nString formatting using string.format()\nprint('Total is: {0}'.format(total))\n\nzip to loop over a sequence of tuples\nfor value, weight in zip([2,3,19],\n                         [0.2,0.3,0.5]):\n    total += weight * value\n\n\n\n2.8.3 Numpy\n\n2.8.3.1 Numpy functions:\nnp.sum(x), np.sqrt(x) (entry wise). x**2 (entry wise power), np.corrcoef(x,y) (find the correlation coefficient of array x and array y)\nnp.mean(axis=None): axis could be None (all entries), 0(along row), 1(along column)\nnp.var(x, ddof=0), np.std(x, ddof=0), # Note both np.var and np.std accepts an argument ddof, the divisor is N-ddof.\nnp.linspace(-np.pi, np.pi, 50) # start, end, number of points 50\nnp.multiply.outer(row,col) # calculate the product over the mesh with vectors row and col.\nnp.zeros(shape or int, dtype) #eg: np.zeros(5,bool)\nnp.ones(Boston.shape[0])\nnp.all(x), np.any(x): check if all or any entry of x is true.\nnp.unique(x): find unique values in x. np.isnan(x): return a boolean array of len(x). np.isnan(x).mean(): find the percentage of np.nan values in x.\n\n\n2.8.3.2 Array Slicing and indexing\nnp.arange(start, stop, step) # numpy version of range\nx[slice(3:6)] # equivalent to x[3:6]\nIndexing an array using [row, col] format. If col is missing, then index the entire rows. len(row) must be equal to len(col). Otherwise use iterative indexing or use np.ix_(x_idx, y_idx) function, or use Boolean indexing, see below.\nA[1,2]: index entry at row 1 and col 2 (recall Python index start from 0)\nA[[1,3]] # row 1 and 3. Note the outer [] is considered as the operator, so only row indices are provided. \nA[:,[0,2]] # cols 0 and 2\nA[[1,3], [0,2,3]] # entry A[1,0] and A[3,2]\nA[1:4:2, 0:3:2] # entries in rows 1 and 3, cols 0 and 2\nA[[1,3], [0,2,3]] # syntax error\n# instead one can use the following two methods \nA[[1,3]][:,[0,2]] # iterative subsetting\nA[np.ix_([1,3],[0,2,3])] # use .ix_ function to create an index mesh\nA[keep_rows, keep_cols] # keep_rows, keep_cols are boolean arrays of the same length of rows or cols, respectively\nA[np.ix_([1,3],keep_cols)] # np.ix_()can be applied to mixture of integer array and boolean array\n\n\n2.8.3.3 Random numbers and generators\nnp.random.normal(loc=0.0, scale=1.0,size=None) # size can be an integer or a tuple.\n# \nrng = np.random.default_rng(1303) # set random generator seed\nrng.normal(loc=0, scale=5, size=2) # \nrng.standard_normal(10) # standard normal distribution of size 10\nrng.choice([0, np.nan], p=[0.8,0.2], size=A.shape)\n\n\n2.8.3.4 Numpy array atributes\n.dtype, .ndim, .shape\n\n\n2.8.3.5 Numpy array methods\nx.sum(axis=None) (equivalent to np.sum(x)), x.T (transpose),\nx.reshape((2,3)) # x.reshape() is a reference to x.\nx.min(), x.max()\n\n\n\n2.8.4 Graphics\n\n2.8.4.1 2-D figure\n# Using the subplots + ax methods\nfig, ax = subplots(nrows=2, ncols=3, figsize=(8, 8)) \n# explicitly name each axis in the grid \nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n\nax[0,1].plot(x, y,marker='o', 'r--', linewidth=3); #line plot. `;` suppresses the text output. pick ax[0,1] when there are  multiple axes\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.scatter(x, y, marker='o'); #scatter plot\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.set_xlabel(\"this is the x-axis\")\nax.set_ylabel(\"this is the y-axis\")\nax.set_title(\"Plot of X vs Y\");\naxes[0,1].set_xlim([-1,1]) # set x_lim. similarly `set_ylim()`\n\nfig = ax.figure  # get the figure object from an axes object\nfig.set_size_inches(12,3) # access the fig object to change fig size (width, height)\nfig # re-render the figure\nfig.savefig(\"Figure.pdf\", dpi=200); #save a figure into pdf. Other formats: .jpg, .png, etc\n\n\n2.8.4.2 Contour and image\nfig, ax = subplots(figsize=(8, 8))\nx = np.linspace(-np.pi, np.pi, 50)\ny = x\nf = np.multiply.outer(np.cos(y), 1 / (1 + x**2))\nax.contour(x, y, f, levels=None); # numbre of levels. if None, automatically choose\nax.imshow(f); # heatmap colorcoded by f\n\n\n\n2.8.5 Pandas\n\n2.8.5.1 loading data\npd.read_csv('Auto.csv') # read csv\npd.read_csv('Auto.data', \n            na_values =['?'], #specifying the na_values in the datafile. \n            delim_whitespace=True) # read whitespaced text file\npd.read_csv('College.csv', index_col=0) # use column `0` as the row labels \n\n\n\n2.8.5.2 Pandas Dataframe attributes and methods\nAuto.shape\nAuto.columns # gets the list of column names\nAuto.index #return the index (labels) objects\nAuto['horsepower'].to_numpy() # convert to numpy array\nAuto['horsepower'].sum()\n\nAuto.dropna() # drop the rows containing na values. \ndf.drop('B', axis=1, inplace=True) # drop a column 'B' inplace. \n#equivalent to df.drop(columns=['B'], inplace=True)\ndf.drop(index=['Ohio','Colorado']) #eqivalent to: df.drop(['Ohio','Colorado'], axis=0)\nauto_df.drop(auto_df.index[10:86]) # drop rows with index[10:86] not including 86\n\nAuto.set_index('name')# rename the index using the column 'name'.\n\npd.Series(Auto.cylinders, dtype='category') # convert the column `cylinders` to 'category` dtype\n# the convertison can be done using `astype()` method\nAuto.cylinders.astype('category')\nAuto.describe() # statistics summary of all columns\nAuto['mpg'].describe() # for selected columns\n\ncollege.rename({'Unnamed: 0': 'College'}, axis=1): # change column name, \n# alternavie way\ncollege_df.rename(columns={college_df.columns[0] : \"College\"}, inplace=True) #\n\ncollege['Elite'] = pd.cut(college['Top10perc'],  # binning a column\n                          [0,0.5,1],  #bin edges\n                          labels=['No', 'Yes'],  # bin labels (names)\n                          right=True,# True: right-inclusive (default) for each bin ( ]; False:rigth-exclusive \n                          )   \ncollege['Elite'].value_counts() # frequency counts\nauto.columns.tolist() # equivalent to  auto.columns.format() (rarely used)\n\n\n\n2.8.5.3 Selecting rows and columns\nSelect Rows:\nAuto[:3] # the first 3 rows. \nAuto[Auto['year'] &gt; 80] # select rows with boolean array\nAuto_re.loc[['amc rebel sst', 'ford torino']] #label_based row selection\nAuto_re.iloc[[3,4]] #integer-based row seleciton: rows 3 and 4 (index starting from 0)\nSelect Columns\nAuto['horsepower'] # select the column 'horsepower', resulting a pd.Series.\nAuto[['horsepower']] #obtain a dataframe of the column 'horsepower'. \nAuto_re.iloc[:,[0,2,3]] # intger-based selection\nauto_df.select_dtypes(include=['int16','int32']) # select columns by dtype\nSelect a subset\nAuto_re.iloc[[3,4],[0,2,3]] # integer-based \nAuto_re.loc['ford galaxie 500', ['mpg', 'origin']] #label-based \nAuto_re.loc[Auto_re['year'] &gt; 80, ['weight', 'origin']] # mix bolean indexing with labels\n\nAuto_re.loc[lambda df: (df['year'] &gt; 80) & (df['mpg'] &gt; 30),\n            ['weight', 'origin']\n           ]  # using labmda function with loc[]\n\n\n2.8.5.4 Pandas graphics\nWithout using subplots to get axes and figure objects\nax = Auto.plot.scatter('horsepower', 'mpg') #scatter plot of 'horsepower' vs 'mpg' from the dataframe Auto\nax.set_title('Horsepower vs. MPG');\nfig = ax.figure\nfig.savefig('horsepower_mpg.png');\n\nplt.gcf().subplots_adjust(bottom=0.05, left=0.1, top=0.95, right=0.95) #in percentage of the figure size. \nax1.fig.suptitle('College Scatter Matrix', fontsize=35)\nUsing subplots\nfig, axes = subplots(  ncols=3, figsize=(15, 5))\nAuto.plot.scatter('horsepower', 'mpg', ax=axes[1]);\nAuto.hist('mpg', ax=ax);\nAuto.hist('mpg', color='red', bins=12, ax=ax); # more customized \nBoxplot using subplots\nAuto.cylinders = pd.Series(Auto.cylinders, dtype='category') # needs to convert the `cylinders` column to categorical dtype\nfig, ax = subplots(figsize=(8, 8))\nAuto.boxplot('mpg', by='cylinders', ax=ax);\nScatter matrix\npd.plotting.scatter_matrix(Auto); # all columns\npd.plotting.scatter_matrix(Auto[['mpg',\n                                 'displacement',\n                                 'weight']]);  # selected columns\n                                 \n                                 \n#Alternatively with sns.pairplot\n\n\nSns Graphic\n# Scatter matrix\nax1 = sns.pairplot(college_df[college_df.columns[0:11]])\n\n# Boxplot\nsns.boxplot(ax=ax, x=\"Private\", y=\"Outstate\", data=college_df)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch3.html#simple-linear-regression",
    "href": "ch3.html#simple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.1 Simple Linear Regression",
    "text": "3.1 Simple Linear Regression\nAssumes the population regression line model \\[\nY = \\beta_0 + \\beta_1 X +\\epsilon,\n\\] where, \\(\\beta_0\\) is the expected value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) is the average change in \\(Y\\) with a one-unit increase in \\(X\\). \\(\\epsilon\\) is a “catch all” error term.\nAfter training using the training data, we can obtain the parameter estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The we can obtain the prediction for \\(x\\) given by the least square line: \\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\] The error at a data point \\(x_i\\) is given by \\(e_i = y_i -\\hat{y}_i\\), and the residual sum of squares (RSS) is \\[\n\\text{RSS} =e_1^2+\\cdots +e_n^2.\n\\] One can use the least square approach to minimize RSS to obtain \\[\n\\hat{\\beta}_1 =\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=r_{xy}\\frac{\\sigma_y}{\\sigma_x}\n\\] \\[\n\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\n\\] where, \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\). If we assume the data matrix \\(X\\) is demeaned, then \\(\\hat{beta}_0=\\bar{y}\\). and the correlation \\[\nr_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}=\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\tag{3.1}\\] is the normalized covariance. Note \\(-1\\le r_{xy} \\le 1\\). When there is no intercept, that is \\(\\beta_0=0\\), then \\[\n\\hat{y}_i=x_i \\hat{\\beta}=\\sum_{i=1}^n a_i y_i\n\\] where, \\[\n\\hat{\\beta} =\\frac{\\sum_{i=1}^n x_iy_i}{ \\sum_{i=1}^{n} x_i^2}\n\\] That is, the fitted values are linear combinations of the response values when there is no intercept.\n\n3.1.1 Assessing the accuracy of the coefficients\nLet \\(\\sigma^2=\\text{Var}(\\epsilon)\\), that is, \\(\\sigma^2\\) is the variance of \\(Y\\), (estimated by \\(\\sigma^2\\approx =\\text{RSE} =\\text{RSS}/(n-p-1)\\). ) Assume each observation have common variance (homoscedasticity) and are uncorrelated, then the standard errors under repeated sampling \\[\n(\\text{SE}[\\hat{\\beta}_1])^2 = \\frac{1}{\\sigma^2_x}\\cdot \\frac{\\sigma^2}{n}\n\\] \\[\n(\\text{SE}[\\hat{\\beta}_0])^2 = \\left[1+ \\frac{\\bar{x}^2}{\\sigma^2_x} \\right]\\cdot \\frac{\\sigma^2}{n}\n\\]\n\nwhen \\(x_i\\) are more spread out (with large \\(\\sigma_x^2\\)), then \\(\\text{SE}[\\hat{\\beta}_1]\\) is small. This is because there are more leverage (of \\(x\\) values) to estimate the slope.\nwhen \\(\\bar{x} =0\\) , then \\(\\text{SE}[\\hat{\\beta}_0] = \\text{SE}[\\bar{y}]\\). In this case, \\(\\hat{\\beta}_0 = \\bar{y}\\).\n\nStandard errors are used to construct CI and perform hypothesis test for the estimated \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\). Under the assumption of Gaussian error, One can construct the CI of significance level \\(\\alpha\\) (e.g., \\(\\alpha=0.05\\)) as \\[\n\\hat{\\beta}_j = [\\hat{\\beta}_j- t_{1-\\alpha/2,n-p-1}\\cdot \\text{SE}[\\hat{\\beta}_j], \\hat{\\beta}_j+ t_{1-\\alpha/2,n-p-1} \\cdot \\text{SE}[\\hat{\\beta}_j]  ]\n\\] Where \\(j=0, 1\\). Large interval including zero indicates \\(\\beta_j\\) is not statistically significant from 0. When \\(n\\) is sufficient large, \\(t_{0.975,n-p-1} \\approx 2\\). With the standard errors of the coefficients, one can also perform hypothesis test on the coefficients. For \\(j=0,1\\),\n\\[H_0: \\beta_j=0\\] \\[H_A: \\beta_j\\ne 0\\] The \\(t\\)-statistic of degree \\(n-p-1\\), given by \\[\nt = \\frac{\\hat{\\beta}_j - 0}{\\text{SE}[\\hat{\\beta}_j]}\n\\] shows how far away \\(\\hat{\\beta}_j\\) is away from zero, normalized by its error \\(\\text{SE}[\\hat{\\beta}_j]\\). One can then compute the \\(p\\)-value corresponding to this \\(t\\) and test the hypothesis. Small \\(p\\)-value indicates strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#multiple-linear-regression",
    "href": "ch3.html#multiple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.2 Multiple Linear Regression",
    "text": "3.2 Multiple Linear Regression\n\\[\nY= \\beta_0 + \\beta_1X_1 +\\cdots + \\beta_pX_p + \\epsilon.\n\\] The estimate of the coefficients \\(\\hat{\\beta_j}\\), \\(j\\in \\mathbb{Z}_{p+1}\\) are found by using the same least square method to minimize RSS. we interpret \\(\\beta_j\\) as the expected (average) effect on \\(Y\\) with one unit increase in \\(X_j\\), holding all other predictors fixed. This interpretation is based on the assumptions that the predictors are uncorrelated, so each predictor can be estimated and tested separately. When there are correlations among predictors, the variance of all coefficients tends to increase, sometimes dramatically, and the previous interpretation becomes hazardous because when \\(X_j\\) changes, everything else changes.\n\n3.2.1 Model Assumption\n\nlinearity: \\(Y\\) is linear in \\(X\\). The change in Y associated with one unit of change in \\(X_j\\) is constant, regardless of the value of \\(X_j\\). This can be examined visually by plotting the residual plot (\\(e_i\\) vs. \\(x_i\\) for \\(p=1\\) or \\(e_i\\) vs \\(\\hat{y}_i\\) for multiple regression). If the linear assumption is true, then the residual plot should not exhibit obvious pattern. If there is a nonlinear relationship suggested by by the residual plot, then a simple approach is to include transformed \\(X\\), such as \\(\\log X\\), \\(\\sqrt{X}\\), or \\(X^2\\).\nadditive: The association between \\(X_j\\) and \\(Y\\) is independent of other predictors.\nErrors \\(\\epsilon_i\\) are uncorrelated. This means \\(\\epsilon_i\\) provides no information for \\(\\epsilon_{i+1}\\). Otherwise (for example, frequently observed in a time series, where error terms are positively correlated, and tracking is observed in the residuals, i.e., adjacent error terms take similar values), the estimated standard error will tend to be underestimated, hence leading less confidence in the estimated model.\nHomoscedasticity: \\(\\text{Var}(\\epsilon_i) =\\sigma^2\\). The error terms have constant variance. If not (heteroscedasticity), one may use transformed \\(Y\\), such as \\(\\sqrt{Y}\\), or \\(\\log(Y)\\) to mitigate this; or use weighted least squares if it’s known that for example \\(\\sigma_i^2=\\sigma^2/n_i\\).\nNon-colinearity: two variables are colinear if they are highly correlated with each other. Co-linearity causes a great deal of uncertainty in the coefficient estimates, that is, reducing the accuracy of the coefficient estimates, thus cause the standard error of \\(\\beta_j\\) to grow, and hence smaller \\(t\\)-statistic. As a result, we may fail to reject \\(H_0: \\beta_j=0\\). This in turn means the power of Hypothesis test, the probability of correctly detecting a non-zero coefficient is reduced by colinearity. To detect colinearity,\n\nuse the correlation matrix of predictors. Large value of the matrix in absolute value indicates highly correlated variable pairs. But this approach cannnot detect multicolinearity.\nUse VIF (Variance inflation factor, VIF \\(\\ge 1\\)) to detect multicolinearity. It is possible for colinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This is the multicolinearity situation.\n\nVIF is the ratio of the variance of \\(\\hat{\\beta}_j\\) when fitting the full model divided by the variance of \\(\\hat{\\beta}_j\\) if fit on its own. It can be calculated by \\[\n  \\text{VIF}(\\hat{\\beta}_j) =\\frac{1}{1-R^2_{X_j|X_{-j}}}\n  \\] Where \\(R^2_{X_j|X_{-j}}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors. A VIF value exceeds 5 or 10 (i.e., \\(R^2_{X_j|X_{-j}}\\) close to 1) indicates colinearity.\nTo remedy a colinearity problem:\n\ndrop a redundant variable (variables with colinearity should have similar VIF values. )\nCombine the colinear variables into a single predictor, e.g., taking the average of the standardized versions of those variables.\n\n\nClaims of causality should be avoided for observational data.\n\n\n3.2.2 Assessing existence of linear relationship\n\ntest Hypothesis (test if there is a linear relationship between the response and predictors) \\[\nH_0: \\beta_1=\\beta_2=\\cdots = \\beta_p=0\n\\] \\[\nH_a: \\text{at least one } \\beta_j \\text{ is non-zero.}\n\\] using \\(F\\)-statistic \\[\nF=\\frac{\\text{SSB/df(B)}}{\\text{SSW/df(W)}}=\\frac{(\\text{TSS}-\\text{RSS})/p}{\\text{RSS}/(n-p-1)}\\sim F_{p,n-p-1}\n\\] If \\(H_0\\) is true, \\(F\\approx 1\\); if \\(H_a\\) is true, \\(F&gt;&gt;1\\). \\(F\\)-statistic adjust with \\(p\\). Note that one cannot conclude if an individual \\(t\\)-statistic is significant, then there is at least one predictor is related to the response, especially when \\(p\\) is large. This is related to multiple testing. The reason is that when \\(p\\) is large, there is \\(\\alpha\\) (eg 5%) chance that a predictor will have a small \\(p\\)-value by chance. When \\(p&gt;n\\), \\(F\\)-statistic cannot be used.\n\nIf the goal is to test that a particular subset of \\(q\\) of the coefficients are zero, that is, (for convenience, we put the \\(q\\) variables chosen at the end of the variabale list) \\[\nH_0: \\beta_{p-q+1} = \\beta_{p-q+2}=\\cdots = \\beta_p=0\n\\tag{3.2}\\] In this case, use \\[\nF = \\frac{(\\text{RSS}_0-\\text{RSS})/q}{\\text{RSS}/(n-p-1)}\\sim F_{q,n-p-1}\n\\] where, \\(\\text{RSS}_0\\) is the residual sum of squares of a second model that uses all variables except those last \\(q\\) variables. When \\(q=1\\), \\(F\\)-statistic in Equation 3.2 is the square of the \\(t\\)-statistic of that variable. The \\(t\\)-statistic reported in a regression model gives the partial effect of adding that variable, while holding other variables fixed.\n\n\n3.2.3 Assess the accuracy of the future prediciton\n\nconfidence interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from the population average \\(f(X)\\) because the coefficients \\(\\hat{\\beta}_{j}\\) are estimated, It quantifies reducible error around the predicted average response \\(\\hat{f}(X)\\), does-not include \\(\\epsilon\\).\nprediction interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from \\(Y\\). predict an individual response \\(Y\\approx \\hat{f}(X)+\\epsilon\\). Prediction interval is always wider than the confidence interval, because it includes irreducible error \\(\\epsilon\\).\n\n\n\n3.2.4 Assessing the overall accuracy of the model\n\nRSE. To this end, first define the lack of fit measure Residual Standard Error \\[\n\\text{RSE} = \\sqrt{\\frac{1}{n-p-1}\\text{RSS}} = \\sqrt{\\frac{1}{n-p-1}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2} \\approx \\sigma=\\sqrt{\\text{Var}(\\epsilon)}\n\\] It is the average amount in \\(\\hat{Y}\\) that a response deviates from the true regression line (\\(\\beta_0+\\beta_1 X\\)). Note, RSE can increase with more variables if the decrease of RSS doesnot offset the increase of \\(p\\).\nApproach 2: Using R-squared (fraction of variance in \\(Y\\) explained by \\(X\\)), which is independent of of the scale of \\(Y\\), and \\(0\\le R^2 \\le 1\\): \\[\nR^2 =\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}} = 1-\\frac{\\text{RSS}}{\\text{TSS}}\n\\] where, \\(\\text{TSS}=\\sum_{i=1}^n(y_i- \\bar{y})\\). When \\(R^2\\) is near 0 indicates that 1) either the linear model is wrong 2) or th error variance \\(\\sigma^2\\) is high, or both. \\(R^2\\) measures the linear relationship between \\(X\\) and \\(Y\\). If computed on the training set, when adding more variables, the RSS always decrease, hence \\(R^2\\) will always increase.\n\nFor simple linear regression, \\(R^2=r_{xy}^2\\), where the sample correlation measures the linear relationship between variables \\(X\\) and \\(Y\\). See the formula \\(r_{xy}\\) above Equation 3.1. For multiple linear regression, \\(R^2=(\\text{Cor}(Y, \\hat{Y}))^2\\). The fitted linear model maximizes this correlation among all possible linear models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "href": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.3 Model Selection/Variable Selections: balance training errors with model size",
    "text": "3.3 Model Selection/Variable Selections: balance training errors with model size\n\nAll subsets (best subsets) regression: compute the least square fit for all \\(2^p\\) possible subsets and then choose among them based on certain criterion that balance training error and model size\nForward selection: Start from the null model that only contains \\(\\beta_0\\). Then find the best model containing one predictor that minimizing RSS. Denote the variable by \\(\\beta_1\\). Then continue to find the best model with the lowest RSS by adding one variable from the remaining predictors, and so on. Continue until some stopping rule is met: e.g., when all remaining variables have a \\(p\\)-value greater than some threshold.\nBackward selection: start with all variables in the model. Remove the variable with the largest \\(p\\)-value (least statistically significant). The new \\((p-1)\\) model is fit, and remove the variable with the largest \\(p\\)-value. Continue until a stopping rule is satisfied, e.g., all remaining variables have \\(p\\)-value less than some threshold.\nMixed selection: Start with forward selection. Since the \\(p\\)-value for variables can become larger as new predictors are added, at any point if the \\(p\\)-value of a variable in the model rises above a certain threshold, then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low \\(p\\)-value, and all variables outside the model would have a large \\(p\\)-value if added to the model.\nBackward selection cannot be used if \\(p&gt;n\\). Forward selection can always be used, but might include variables early that later become redundant. Mixed selection can remedy this problem.\nothers (Chapter 6): including Mallow’s \\(C_p\\), AIC (Akaike Informaton Criterion), BIC, adjusted \\(R^2\\), Cross-validation, test set performance.\nnot valid: we could look at individual \\(p\\)-values, but when the number of variables \\(p\\) is large, we likely to make a false discoveries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#handle-categorical-variables-factor-variables",
    "href": "ch3.html#handle-categorical-variables-factor-variables",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.4 Handle categorical variables (factor variables)",
    "text": "3.4 Handle categorical variables (factor variables)\nFor a categorical variable \\(X_i\\) with \\(m\\) levels, create one fewer dummy variables (\\(x_{ij}, 1\\le j \\le m-1\\))&gt;. The level with no dummy variable is called the baseline. The coefficient corresponding to a dummy variable is the expected difference in change in \\(Y\\) when compared to the baseline, while holding other predictors fixed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#adding-non-linearity",
    "href": "ch3.html#adding-non-linearity",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.5 Adding non-linearity",
    "text": "3.5 Adding non-linearity\n\n3.5.1 Modeling interactions (synergy)\nWhen two variables have interaction, then their product \\(X_iX_j\\) can be added into the regression model, and the product maybe considered as a single variable for inference, for example, compute its SE, \\(t\\)-statistics, \\(p\\)-value, Hypothesis test, etc.\nIf we include an interaction in a model, then the Hierarchy principle should be followed: always include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant. This is because without the main effects, the interactions are hard to interpret, as they would also contain the main effect.\n\n\n3.5.2 Adding terms of transformed predictors\n\nPolynomial regression: Add a term involving \\(X_i^k\\) for some \\(k&gt;1\\).\nother forms: Adding root or logarithm terms of the predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "href": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))",
    "text": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))\nIt is typical for an outlier that does not have an unusual predictor value (with low levarage) to have little effect on the least squares fit, but it will increase RSE, hence deteriorate CI, \\(p\\)-value and \\(R^2\\), thus affecting interpreting the model.\nAn outlier can be identified by computing the \\[\\text{studentized residual}=\\frac{e_i}{\\text{RSE}_i}\\] A studentized residual great than 3 may be considered as an outlier.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#high-leverage-points-unusual-x_i",
    "href": "ch3.html#high-leverage-points-unusual-x_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.7 High leverage points (unusual \\(x_i\\))",
    "text": "3.7 High leverage points (unusual \\(x_i\\))\nHigh leverage points tend to have sizeable impact on the regression line. To quantify the observation’s leverage, one needs to compute the leverage statistic \\[h_i = \\frac{1}{n}+ \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n (x_j-\\bar{x})^2}.\\] \\(1/n \\le h_i\\le 1\\) and \\(\\text{Ave}(h_i)=(p+1)/n\\). A large value of this statistic (for example, great than \\((p+1)/n\\)) indicates an observation with high leverage. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#compared-to-knn-regression",
    "href": "ch3.html#compared-to-knn-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.8 Compared to KNN Regression",
    "text": "3.8 Compared to KNN Regression\nKNN regression is a non-parametric method that makes prediction at \\(x_0\\) by taking the average in a \\(K\\)-point neightborhood \\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_{x_0}}{y_i}\n\\] A small value of \\(K\\) provides more flexible model with low bias but high variance while a larger value of \\(K\\) provides smoother fit with less variance. An optimal value of \\(K\\) depend on the bias-variance tradeoff. For non-linear data set, KNN may provides better fit than a linear regression model. However, in higher dimension (e.g., \\(p\\ge 4\\)), even for nonlinear data set, KNN may perform much inferior to linear regression, because of the curse of dimensionality, as the \\(K\\) observations that are nearest to \\(x_0\\) may in fact far away from \\(x_0\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#homework-indicates-optional",
    "href": "ch3.html#homework-indicates-optional",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.9 Homework (* indicates optional):",
    "text": "3.9 Homework (* indicates optional):\n\nConceptual: 1–6\nApplied: 8–15. at least one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#code-gist",
    "href": "ch3.html#code-gist",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.10 Code Gist",
    "text": "3.10 Code Gist\n\n3.10.1 Python\ndir() # provides a list of objects at the top level name space\ndir(A) # display addtributes and methods for the object A\n' + '.join(X.columns) # form a string by joining the list of column names by \"+\"\n\n\n3.10.2 Numpy\nnp.argmax(x) # identify the location of the largest element\nnp.concatenate([x,y],axis=0) # concatenate two arrays x and y. \n\n\n\n3.10.3 Pandas\nX = pd.DataFrame(data=X, columns=['a','b'])\n\npd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']}) # make a dataframe using a dictionary\nBoston.columns.drop('medv','age') # drop the elements 'medv' and 'age' from the list of column names\n\npd.DataFrame({'vif':vals},\n                   index=X.columns[1:]) # form a df by specifying index labels\n\nX.values  # Convert dataframe X to numpy array\nX.to_numpy() # recommended to replace the above method\nDataFrame.corr(numeric_only=True) # correlations between columns \nx.sort_values(ascending=False)\npd.to_numeric(auto_df['horsepower'], errors='coerce') # if error, denote it by \"NaN\".\nauto_df.dropna(subset= ['horsepower', 'mpg',], inplace=True) # looking for NaN in the columns in `subset`, otherwise, all columns\n\nauto_df.drop('name', axis=1, inplace=True)\n\nleft2.join(right2, how=\"left\") #join two databases by index. \nleft1.join(right1, on=\"key\") # left-join by left1[\"key\"] and the index of right1. \npd.concat([s1, s4], axis=\"columns\", join=\"outer\")\n\n\n\n3.10.4 Graphics\nxlim = ax.get_xlim() # get the x_limit values xlim[0], xlim[1]\nax.axline() # add a line to a plot\nax.axhline(0, c='k', ls='--'); # horizontal line\nline, = ax.plot(x,y,label=\"line 1\") # \"line 1\" is the legend\n# alternatively the label can be set by \nline.set_label(\"line 1\")\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.legend(loc=\"upper left\", fontsize=25) # adding legendes\nax.annotate(i,xy=(fitted[i],residuals[i])) # annote at the xy position with i. \n\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rcParams.update({'font.size': 16})\nplt.rcParams[\"figure.figsize\"] = (8,7)\n\nplt.rc('font', size=10)\nplt.rc('figure', titlesize=13)\nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=13)\nplt.rc('legend', fontsize=8) # adjust legend globally\n    \n\n\n3.10.5 Using Sns\nsns.set(font_scale=1.25) # set font size 25% larger than default\nsns.heatmap(corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})\nax = sns.regplot(x=x, y=y)\n\n\n3.10.6 Using Sklearn\nfrom sklearn.linear_model import LinearRegression\n## Set the target and predictors\nX = auto_df['horsepower']\n\n### To get polynomial features\npoly = PolynomialFeatures(interaction_only=True,include_bias = False)\nX = poly.fit_transform(X)\n\ny = auto_df['mpg']\n\n## Reshape the columns in the required dimensions for sklearn\nlength = X.values.shape[0]\nX = X.values.reshape(length, 1) #both X and y needs to be 2-D\ny = y.values.reshape(length, 1)\n\n## Initiate the linear regressor and fit it to data using sklearn\nregr = LinearRegression()\nregr.fit(X, y)\nregr.intercept_\nregr.coef_\n\npred_y = regr.predict(X)\n\n\n3.10.7 Using statsmodels and ISLP\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n                         \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\n#Training\nBoston = load_data(\"Boston\") \n#hand-craft the design matrix X\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), #design matrix. intercept column\n                  'lstat': Boston['lstat']}) \n#the following is the preferred method to create X\ndesign = MS(['lstat']) # specifying the model variables. Automatically add an intercept, adding \"intercept=False\" if no intercept. \ndesign = design.fit(Boston) # do intial computation as specified in the model object design by MS(), such as means or sd. This attached some statistics to the `design` object, and need to be applied to the new data for prediciton\n\nX = design.transform(Boston) # apply the fitted transformation to the data to create X\n#alternatiely, \nX = design.fit_transform(Boston) # this combines the .fit() and .transform() two lines\n\ny = Boston['medv']\nmodel = sm.OLS(y, X) # setup the model\nmodel = smf.ols('mpg ~ horsepower', data=auto_df) # alternatively use smf formula, y~x\nsmf.ols(\"y ~ x -1\" , data=df).fit() # \"-1\" not inclding the intercept\nresults = model.fit() # results is a dictionary:.summary(), .params \n\nresults.summary()\nresults.params # coefficients\nresults.resid # reisdual array\nresults.rsquared # R^2\nresults.pvalues\nnp.sqrt(results.scale) # RSE\nresults.fittedvalues # fitted \\hat(y)_i at x_i in the traning set\n\n\nsummarize(results) # summzrize() is from ISLP to show the esstial results from model.fit()\n\n# Makding prediciton \nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})  # new test-set containing data where to make predicitons\nnewX = design.transform(new_df) # apply the same transform to the test-set\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean #predicted values\nnew_predictions.conf_int(alpha=0.05) #for the predicted values\n\nnew_predictions.conf_int(obs=True, alpha=0.05) # prediction intervals by setting obs=True\n\n# Including an interaction term\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston) #interaction term ('lstat', 'age')\n\n# Adding a polynomial term of higher degree\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston) # Note poly is from ISLP, # adding deg1 and deg2 terms. by default poly creates ortho. poly. not including an intercept. \n# Given a qualitative variable, `ModelSpec()` generates dummy\nvariables automatically, to avoid collinearity with an intercept, the first column is dropped in the design matrix generated by 'ModelSpec()` by default.\n\n# Compare nested models using ANOVA\nanova_lm(results1, results3) # result1 is the result of linear model, an result3 is the result of a larger model\n\n# Identify high leverage x\ninfl = results.get_influence() \n# hat_matrix_diag calculate the leverate statistics\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage\n\n# Calculate VIF\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])] #excluding column 0 because it's all 1's in X.\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif # VIF exceeds 5 or 10 indicates a problematic amount of colinearity\n\nUseful Code Snippets\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n# Plot scatter plot with a regression line\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n# Plot residuals vs. fitted values (note, not vs x, therefore works for multiple regression)\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n# Alternatively\nsns.residplot(x=X, y=y, lowess=True, color=\"g\", ax=ax)\n\n# Plot the smoothed residuals~fitted by LOWESS\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nsmoothed = lowess(residuals,fitted) # Note the order (y,x)\nax.plot(smoothed[:,0],smoothed[:,1],color = 'r')\n\n# QQ plot for the residuas (obtain studentized residuals for identifying outliers)\nimport scipy.stats as stats\nsorted_student_residuals = pd.Series(smf_model.get_influence().resid_studentized_internal)\nsorted_student_residuals.index = smf_model.resid.index\nsorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)\ndf = pd.DataFrame(sorted_student_residuals)\ndf.columns = ['sorted_student_residuals']\n\n#stats.probplot() #assess whether a dataset follows a specified distribution\ndf['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0] \n    \nx = df['theoretical_quantiles']\ny = df['sorted_student_residuals']\nax.scatter(x,y, edgecolor = 'k',facecolor = 'none')\n\n# Plot leverage statistics\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch4.html#linear-regression-and-classification",
    "href": "ch4.html#linear-regression-and-classification",
    "title": "4  Chapter 4: Classification",
    "section": "4.1 Linear regression and Classification",
    "text": "4.1 Linear regression and Classification\n\nFor a binary classification, one can use linear regression and does a good job. In this case, the linear regression classifier is equivalent to LDA, because \\[\nP(Y=1|X=x)= E[Y|X=x]\n\\] However, linear regression may not represent a probability as it may give a value outside the interval \\([0,1]\\).\nWhen there are more than two classes, linear regression is not appropriate, because any chosen coding of the \\(Y\\) variable imposes an ordering and fixed differences among categories, which may not be implied by the data set. If the coding changes, a dramatic function will be fitted, which is not reasonable. One should turn to multiclass logistic regression or Discriminant Analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#logistic-regression",
    "href": "ch4.html#logistic-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nLogistic regression is a discriminative learning, because it directly calculates the conditional probability \\(P(Y|X)\\) to make classification.\n\n4.2.1 Binary classification\nwith a single variable Logistic regression simply convert the linear regression to probability by \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\beta_0+\\beta_1 X}}{1+ e^{\\beta_0+\\beta_1X}}.\n\\] Note the logit or log odds is linear \\[\n\\log\\left( \\frac{p(X)}{1-p(X)}  \\right) =\\beta_0 +\\beta_1 X.\n\\] Increasing \\(X\\) by one unit, changes the log odds by \\(\\beta_1\\). Equivalently, it multiplied the odds by \\(e^{\\beta_1}\\). The rate of change of \\(p(X)\\) is no longer a constant, but depends on the current value of \\(X\\). Positive \\(\\beta_1\\) implies increasing \\(p(X)\\), and vice vesa.\nThe parameters are estimated by maximizing the liklihood \\[\n\\ell(\\beta_0, \\beta_1) =\\prod_{i: y_i=1}p(x_i) \\prod_{i:y_i=0}(1-p(x_i))\n\\] With the estimated parameters \\(\\hat{\\beta_j}, j=0,1\\), one can calculate the probability \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1} X}}{1+ e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}\n\\]\n\n\n4.2.2 with multiple variables\nIn this case, simply let the logit be a linear function of \\(p\\) variables.\nNote when there are multiple variables, it’s possible to have variables confounding (especially when two variables are correlated): the coefficient of a variable may changes significantly or may change sign, this is because the coefficient represents the rate of change in \\(Y\\) of that variable when holding other variable constants. The coefficient reflects the effect when other variables are hold constant, how the variable affects \\(Y\\), and this effect may be different than when only this variable is used in the model.\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the logit model, similar to a linear regression that includes a non-linear term.\n\n\n\n\n4.2.3 Multi-class logistic regression (multinomial regression) with more than two classes\nin this case, we use the softmax function to model \\[\n\\text{Pr} (Y=k|X) =\\frac{e^{\\beta_{0k}+\\beta_{1k}X_1+ \\cdots + \\beta_{pk}X_p}}{\\sum_{\\ell=1}^K e^{\\beta_{0\\ell}+\\beta_{1\\ell}X_1+ \\cdots + \\beta_{p\\ell}X_p}} =a_k\n\\] for each class \\(k\\). Note \\(\\Sigma_k a_k=1\\) and the cross-entropy loss function is given by \\(-\\log \\ell(\\beta)= -\\Sigma_k \\mathbb{1}_k \\log a_k\\), where \\(\\beta\\) represents all the parameters.\nThe log odds between \\(k\\)th and \\(k'\\)th classes equals \\[\n\\log(\\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=k'|X=x)})=(\\beta_{k0}-\\beta_{k'0}) + (\\beta_{k1}-\\beta_{k'1}) + \\cdots + (\\beta_{kp}-\\beta_{k'p})\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "href": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier",
    "text": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier\nApply the Bayes Theorem, the model \\[\n\\text{Pr}(Y=k|X=x)=\\frac{\\text{Pr}(X=x|Y=k)\\cdot \\text{Pr}(Y=k)}{\\text{Pr}(X=x)}=\\frac{\\pi_k f_k(x)}{\\sum_{\\ell =}^ K \\pi_{\\ell}f_\\ell(x)}\n\\] where \\(\\pi_k=\\text{Pr(Y=k)}\\) is the marginal or prior probability for class \\(k\\), and \\(f_k(x)=\\text{Pr}(X=x|Y=k\\)) is the density for \\(X\\) in class \\(k\\). Note the denominator is a normalizing constant. So when making decisions, effectively we compare \\(\\pi_kf_k(x)\\), and assign \\(x\\) to a class \\(k\\) with the largest \\(\\pi_kf_k(x)\\).\nDiscriminant uses the full liklihood \\(P(X,Y)\\) to calculate \\(P(Y|X)\\) to make a classification, so it’s known as generative learning.\n\nwhen \\(f_k\\) is chosen as a normal distribution with constant variance (\\(\\sigma^2\\)) for \\(p=1\\) or correlation matrix \\(\\Sigma\\) for \\(p&gt;1\\), this leads to the LDA. For \\(p=1\\), the discriminant score is given by \\[\n\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log (\\pi_k)\n\\] when \\(K=2\\) and \\(\\pi_1=\\pi_2=0.5\\)m then the decision boundary is given by \\[\nx=\\frac{\\mu_1+\\mu_2}{2}.\n\\] When \\(p\\ge 2\\), assume that \\(X=(X_1, X_2, \\cdots, X_p)\\) is drawn from a multivariate Gaussian distribution \\(X \\sim N(\\mu_k, \\Sigma)\\), with a class-specific mean vector and a a common variance matrix. \\[\n\\delta_k(x) =x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k +\\log \\pi_k=c_{k0}+c_{k1}x_1+\\cdots +c_{kp}x_p.\n\\] The score function (posterior probability) is linear in \\(x\\). With \\(\\hat{\\delta}_k(x)\\) for each \\(k\\), it can be converted to the class probability by the softmax function \\[\n\\hat{\\text{Pr}}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{\\ell=1}^K e^{\\hat{\\delta}_{\\ell}(x)}}\n\\] The \\(\\pi_k\\), \\(\\mu_k\\) and \\(\\sigma\\) are estimate the follwing way: \\[\n\\hat{\\pi}_k =\\frac{n_k}{n}\n\\] \\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i:y_i=k} x_i\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2=\\sum_{k=1}^{K}\\frac{n_k-1}{n-K}\\hat{\\sigma}^2_k\n\\] where \\(\\hat{\\sigma}_k^2=\\frac{1}{n_k-1} \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\) is the estimated variance for the \\(k\\)-th class.\n\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the LDA model, similar to a linear regression that includes a non-linear term.\n\n\n\nwhen each class chooses a different \\(\\Sigma_k\\), then it’s QDA. It assumes an observation from the \\(k\\)-th class is \\(X\\sim N(\\mu_k, \\Sigma_k)\\).The score function has a quadratic term \\[\n\\delta_k(x)=-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+\\log \\pi_k -\\frac{1}{2}\\log |\\Sigma_k|\n\\] QDA has much more parameters \\(Kp(p+1)/2\\) to estimate compared to LDA (\\(Kp\\)), hence has higher flexibility and may lead to higher variance. When there are few training examples, LDA tend to perform better and reducing variance is crucial. When there is a large traning set, QDA is recommended as variance is not a major concern. LDA is a special case of QDA.\nwhen the features are modeled independently, i.e., there is no association between the \\(p\\) predictors, \\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\), the method is naive Bayes, and \\(\\Sigma_k\\) are diagonal. Any classifier with a linear decision boundary is a special case of NB. So LDA is a special case of NB. To estimate \\(f_kj\\), one can\n\nassume that \\(X_j|Y=k \\sim N(\\mu_{jk,\\sigma^2_{jk}})\\), that is, a class specific covariance but is diagonal. QDA’s \\(\\Sigma_k\\) is not diagonal. If we model \\(f_{kj}(x_j)\\sim N(\\mu_{kj}+\\sigma_j^2)\\) (Note \\(\\sigma_j^2\\) is shared among clases), In this case NB is a special case of LDA that has a diagonal \\(\\Sigma\\) and\nuse a non-parametric estimate such as histogram (or a smooth kernel density estimator) for the observations of the jth Predictor within each class.\nIf \\(X_j\\) is qualitative, then one can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\nCan applied to mixed feature vectors (qualitative and quantitative). NB does not assume normally distributed predictors.\nDespite strong assumptions, performs well, especially when \\(n\\) is not large enough relative to \\(p\\), when estimating the joint distribution is difficult. It introduces some biases but reduces variance, leading to a classifier that works quite well as a result of bias-variance trade-off.\nUseful when \\(p\\) is very large.\nNB is a generalized additive model.\nNeigher NB nor QDA is a special case of the other. Because QDA contains interaction term \\(x_ix_j\\), while NB is purely additive, in the sense that a function of \\(x_i\\) is added to a function of \\(x_j\\). Therefore, QDA potentially is a better fit when the interactions among predictors are important.\n\n\n\n4.3.1 Why discriminant analysis\n\nWhen the classes are well-separated, the parameter estimation of logistic regression is unstable, while LDA does not suffer from this problem.\nif the data size \\(n\\) is small and the distribution of \\(X\\) is approximately normal in each of the classes, then LDA is more stable than logistic regression. Also used when \\(K&gt;2\\).\nwhen there are more than two classes, LDA provides low-dimensional views of the data hence popular. Specifically, when there are \\(K\\) classes, LDA can be viewed exactly in \\(K-1\\) dimensional plot. This is because it essentially classifies to the closest centroid, and they span a \\(K-1\\) dimensional plane.\nFor a two-class problem, the logit of \\(p(Y=1|X=x\\)) by LDA (generative learning) is a linear function in \\(X\\), the same as a logistic regression (discriminative learning). The difference lies in how the parameters are estimated. But in practice, they are similar.\nLDA assumes the predictors follow a multivariable normal distribution with a shared \\(\\Sigma\\) among classes. So when this assumption holds, we expect LDA performs better; and Logistic regress performs better when this asuumption does not hold.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#knn",
    "href": "ch4.html#knn",
    "title": "4  Chapter 4: Classification",
    "section": "4.4 KNN",
    "text": "4.4 KNN\nKNN is a non-parametric method and doesnot assume a shape for the decision boundary. KNN assign the class of popularity to \\(X=x\\) in a \\(K\\)-neighborhood.\n\nKNN dominates LDA and Logistic Regression when the decision boundary is highly non-linear, provided \\(n\\) is large and \\(p\\) is small. As KNN breaks down when \\(p\\) is large.\nKNN requires large \\(n&gt;&gt;p\\), this is because KNN is non-parametric and tends to reduce bias but increase variance.\nWhen the decision boundary is non-linear but \\(n\\) is only modest and \\(p\\) is not very small, QDA may outperform KNN. This is because QDA provides a non-linear boundary while taking advantage of a parametric form, which means that if requires smaller size for accurate classification.\nUnlike logistic regression, KNN does not tell which predictors are more importnat: We dont get a table of coefficients.\nWhen the decision boundary is linear, LDA or logistic regression may perform better, when the boundary is moderately non-linear, QDA or NB may perform better; For a much more complicated decision boundary, KNN may perform better.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#poisson-regression",
    "href": "ch4.html#poisson-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.5 Poisson Regression",
    "text": "4.5 Poisson Regression\nWhen \\(Y\\) is discrete and non-negative, a linear regression model is not satisfactory, even with the transformation of \\(\\log (Y)\\), because \\(\\log\\) does not allow \\(Y=0\\).\n\nPoisson Regression: typically used to model counts, \\[\n\\text{Pr}(Y=k)= \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\qquad k=0,1,2, \\cdots,\n\\] where, \\(\\lambda = E(Y)= \\text{Var}(Y)\\). This means that if \\(Y\\) follows a Poissson distribution, the larger the mean of \\(Y\\), the larger its variance. Posisson regression can handle this when variance changes with mean, but linear regression cannot, because it assumes constant variance.\n\nAssume \\[\n\\log(\\lambda(X_1, X_2, \\cdots, X_p))=\\beta_0+\\beta_1X_1+\\cdots +\\beta_pX_p\n\\] Then one can use maximum likelihood \\[\n\\ell(\\beta_0, \\beta_1, \\cdots, \\beta_p)=\\prod_{i=1}^n \\frac{e^{-\\lambda(x_i)}\\lambda(x_i)^{y_i}}{y_i!}\n\\] to estimate the parameters.\n\nInterpretation: An increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y)=\\lambda\\) by a factor (percentage) of \\(\\exp(\\beta_j)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#generalized-linear-models-glm",
    "href": "ch4.html#generalized-linear-models-glm",
    "title": "4  Chapter 4: Classification",
    "section": "4.6 Generalized Linear Models (GLM)",
    "text": "4.6 Generalized Linear Models (GLM)\nPerform a regression by modeling \\(Y\\) from a particular member of the exponential family (Gaussian, Bernoulli, Poisson, Gamma, negative binomial), and then transform the mean of \\(Y\\) to a linear function.\n\nUse predictors \\(X_1, \\cdots, X_p\\) to predict \\(Y\\). Assume \\(Y\\) conditional on \\(X\\) follow some distribution: For linear regression, assume \\(Y\\) follows a normal distribution; for logistic regression, assume \\(Y\\) follows a Bernoulli (multinomial distribution for multi-class logistic regression) distribution; For poisson distribution, assume \\(Y\\) follows a poisson distribution.\nEach approach models the mean of \\(Y\\) as a function of \\(X\\) using a linking function \\(\\eta\\) to transform \\(E[Y|X]\\) to a linear function.\n\nfor linear regression \\[\nE(Y|X)= \\beta_0+\\beta_1 X_1+\\cdots +\\beta_p X_p\n\\] \\(\\eta(\\mu) =\\mu\\)\nfor logistic regression \\[\nE(Y|X)=P(Y=1|X)=\\frac{e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}\n\\] \\(\\eta(\\mu) = \\log (\\mu/(1-\\mu))\\)\nfor Poisson regression \\[\nE(Y|X) = \\lambda(X) = e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}\n\\] \\(\\eta(\\mu) = \\log(\\mu)\\).\nGamma regression and negative binomial regression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#assessment-of-a-classifier",
    "href": "ch4.html#assessment-of-a-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.7 Assessment of a classifier",
    "text": "4.7 Assessment of a classifier\n\nConfusion matrix\nOverall error rate: equals to \\[\n\\frac{FP+FN}{N+P}\n\\]\nClass-specific performance: One can adjust the decision boundary (posterior probability threshold) to improve class specific performance at the expense of lowered overall performance.\n\npercentage of TP detected among all positives\n\\[\\text{sensitivity (recall, power)} = TPR = \\frac{TP}{TP+FN}=\\frac{TP}{P}= 1-\\text{Type II error}=1-\\beta\\] this is equal to \\(1- FNR\\), where, FNR is The fraction of positive examples that are classified as negatives \\[\nFNR = \\frac{FN}{FN+TP}=\\frac{FN}{P}\n\\]\npercentage of TN detected among all negatives \\[\\text{specificity}= TNR = \\frac{TN}{TN+FP}=\\frac{TN}{N}\\] This is equal to \\(1-FPR\\), where, False positive rate (FPR): the fraction of negative examples (N) that are classified as positive: \\[\nFPR=\\frac{FP}{FP+TN}=\\frac{FP}{N} = \\text{Type I error} (\\alpha)\n\\]\nROC (receiver operating characteristic curve): plot true positive rate (TPR=1-Type II error) ~ false positive rate (FPR= 1- specificity=Type I error) as a threshold for the posterior probability of positive class changes from 0 to 1. The point on the ROC curve closest to the point (0,1) corresponds to the best classifier.\nAUC (area under the ROC): Overall performance of a classifier summarized over all thresholds. AUC measures the probability a random positive example is ranked higher than a random negative example. A larger AUC indicates a better classifier.\n\nclass-specific prediction performance\n\n\\[\\text{precision} = \\frac{TP}{TP+FP}=\\frac{TP}{\\text{predicted postives}}=1-\\text{false discovery proportion}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#homework",
    "href": "ch4.html#homework",
    "title": "4  Chapter 4: Classification",
    "section": "4.8 Homework:",
    "text": "4.8 Homework:\n\nConceptual: 1,2,3,4, 5,6,7,8, 9, 10, 12\nApplied: 13, 14*,15*,16*",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#code-gist",
    "href": "ch4.html#code-gist",
    "title": "4  Chapter 4: Classification",
    "section": "4.9 Code Gist",
    "text": "4.9 Code Gist\n\n4.9.1 Python\n\n\n4.9.2 Numpy\nnp.where(lda_prob[:,1] &gt;= 0.5, 'Up','Down')\nnp.argmax(lda_prob, 1) #argmax along axis=1 (col)\nnp.asarray(feature_std) # convert to np array\nnp.allclose(M_lm.fittedvalues, M2_lm.fittedvalues) \n#check if corresponding elts are equal within rtol=1e-5 and atol=-1e08\n\n\n4.9.3 Pandas\nSmarket.corr(numeric_only=True)\ntrain = (Smarket.Year &lt; 2005)\nSmarket_train = Smarket.loc[train] # equivalent to Smarket[train]\nPurchase.value_counts() # frequency table\nfeature_std.std() #calculate column std\nS2.index.str.contains('mnth')\nBike['mnth'].dtype.categories # get the categories of the categorical data\nobj2 = obj.reindex([\"a\", \"b\", \"c\", \"d\", \"e\"])# rearrange the entries in obj according to the new index, introducing missing values if any index values were not already present. \n\n\n4.9.4 Graphics\nax_month.set_xticks(x_month) # set_xticks at the place given by x_month\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize=20)\nax.axline([0,0], c='black', linewidth=3,  \n          linestyle='--', slope=1);#axline method draw a line passing a given point with a given slope. \n\n\n4.9.5 ISLP and Statsmodels\nfrom ISLP import confusion_table\nfrom ISLP.models import contrast\n\n# Logistic Regression using sm.GLM() syntax similar to sm.OLS()\ndesign = MS(allvars)\nX = design.fit_transform(Smarket)\ny = Smarket.Direction == 'Up'\nglm = sm.GLM(y,\n             X,\n             family=sm.families.Binomial())\nresults = glm.fit()\nsummarize(results)\nresults.pvalues\nprobs = results.predict() #without data set, calculate predictions on the training set. \nresults.predict(exog=X_test) # on test set\n# Prediction on a new dataset\nnewdata = pd.DataFrame({'Lag1':[1.2, 1.5],\n                        'Lag2':[1.1, -0.8]});\nnewX = model.transform(newdata)\nresults.predict(newX)\nconfusion_table(labels, Smarket.Direction) #(predicted_labels, true_labels)\nnp.mean(labels == Smarket.Direction) # calculate the accuracy\n\nhr_encode = contrast('hr', 'sum') #coding scheme for categorical data: the unreported coefficient for the missing level equals to the negative ofthe sum of the coefficients of all other variables. In this a coefficient for a level may be interpreted as the differnece from the mean level of response. \n\n#Poisson Regression \nM_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()\n#`family=sm.families.Gamma()` fits a Gamma regression\nmodel.\n\n\n\n4.9.6 sklearn\nfrom sklearn.discriminant_analysis import \\\n     (LinearDiscriminantAnalysis as LDA,\n      QuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n#LDA\nlda = LDA(store_covariance=True) #store the covariance of each class\nX_train, X_test = [M.drop(columns=['intercept']) # drop the intercept column\n                   for M in [X_train, X_test]]\nlda.fit(X_train, L_train) # LDA() model will automatically add a intercept term\n\nlda.means_ # mu_k (n_classes, n_features)\nlda.classes_\nlda.priors_ # prior probability of each class\n#Linear discrimnant vectors\nlda.scalings_ #Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.\n\nlda_pred = lda.predict(X_test) #predict class labels\nlda_prob = lda.predict_proba(X_test) #ndarray of shape (n_samples, n_classes)\n\n#QDA\nqda = QDA(store_covariance=True)\nqda.fit(X_train, L_train)\nqda.covariance_[0] #estimated covariance for the first class\n\n# Naive Bayes\nNB = GaussianNB()\nNB.fit(X_train, L_train)\nNB.class_prior_\nNB.theta_ #means for (#classes, #features)\nNB.var_ #variances (#classes, #features)\nNB.predict_proba(X_test)[:5]\n\n# KNN\nknn1 = KNeighborsClassifier(n_neighbors=1)\nX_train, X_test = [np.asarray(X) for X in [X_train, X_test]]\nknn1.fit(X_train, L_train)\nknn1_pred = knn1.predict(X_test)\n\n# When using KNN one should standarize each varaibles\nscaler = StandardScaler(with_mean=True,\n                        with_std=True,\n                        copy=True) # do calculaton on a copy of the dataset\nscaler.fit(feature_df)\n\n#train test split\nX_std = scaler.transform(feature_df)\n(X_train,\n X_test,\n y_train,\n y_test) = train_test_split(np.asarray(feature_std),\n                            Purchase,\n                            test_size=1000,\n                            random_state=0)\n\n# Logistic Regression\nlogit = LogisticRegression(C=1e10, solver='liblinear') #use solver='liblinear'to avoid warning that the alg doesnot converge.  \nlogit.fit(X_train, y_train)\nlogit_pred = logit.predict_proba(X_test)\n\n\n\n\n4.9.7 Useful code snippet\n# Tuning KNN\nfor K in range(1,6):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn_pred = knn.fit(X_train, y_train).predict(X_test)\n    C = confusion_table(knn_pred, y_test)\n    templ = ('K={0:d}: # predicted to rent: {1:&gt;2},' +  # &gt; for right alighment\n            '  # who did rent {2:d}, accuracy {3:.1%}')\n    pred = C.loc['Yes'].sum()\n    did_rent = C.loc['Yes','Yes']\n    print(templ.format(\n          K,\n          pred,\n          did_rent,\n          did_rent / pred))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch5.html#how-to-estimate-test-error",
    "href": "ch5.html#how-to-estimate-test-error",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.1 how to estimate test error",
    "text": "5.1 how to estimate test error\n\nuse a large designated test set, but often not available.\nmake adjustment to the training error to estimate the test error, e.g., Cp statistic, AIC and BIC.\nvalidation set approach: estimate the test error by holding out a subset of the training set, also called a validation set.\n\nthe estimate of the test error can be highly variable, depending on the random train-validation split.\nOnly a subset of the training set is used to fit the model. Since statistical methods tend to perform worse when trained on a smaler data set, which suggests the validation error tends to overestimate the test error compared to the model that uses the entire training set.\n\nK-fold Cross-Validation: randomly divide the data into \\(K\\) equal-sized parts \\(C_1, C_2, \\cdots, C_K\\). For each \\(k\\), leave out part \\(k\\), fit the model on the remaining \\(K-1\\) parts (combined, \\((K-1)/K\\) of the original traning set), and then evaluate the model on the part \\(k\\). Then repeat this for each \\(k\\), and weighted average of the errors is computed: \\[\nCV_{(K)} = \\sum_{k=1}^K \\frac{n_k}{n}\\text{MSE}_k\n\\] where \\(\\text{MSE}_k=\\sum_{i\\in C_k}(y_i\\ne \\hat{y}_i)/n_k\\).\n\nFor classification problem, simply replace \\(\\text{MSE}_k\\) with the misclassificaiton rate \\(\\text{Err}_k =\\sum_{i\\in C_k}I(y_i\\ne \\hat{y}_i)/n_k\\).\nThe estimated standard error of \\(CV_k\\) can be calculated by \\[\n\\hat{\\text{SE}}(CV_k)=\\sqrt{\\frac{1}{K}\\sum_{k=1}^K\\frac{(\\text{Err}_k-\\overline{\\text{Err}_k})^2}{K-1}}\n\\]\nThe estimated error tends bias upward because it uses only \\((K-1)/K\\) of the training set. This bias is minimized with \\(K=n\\) (LOOCV), but LOOCV estimate has high variance due to the high correlation between folds.\n\nLOOCV: it’s a special case of K-fold CV with \\(K=n\\). For least squares linear or polynomial regression, the LOOCV error can be computed by \\[\n\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{y_i-\\hat{y}_i}{1-h_i} \\right)^2\n\\] Where \\(h_i\\) is the leverage statistic of \\(x_i\\). There is no randomness in the error. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit. The above formula doesn’t hold in genearl, in which case the model has to refit \\(n\\) times to estimate the test error.\nfor LOOCV, the estimate from each fold are highly correlated, hence their average can have high variance.\nbetter choice is \\(K=5\\) or \\(K=10\\) for bias-variance trade-off, because large \\(k\\) leads to low bias but high variance due to the increased correlation between models. Despite the estimated test error sometimes underestimate the true test error, they then to be close to identify the correct flexibility where the test error is minimum.\nBootstrap: Primarily used to estimate the standard error, or a CI (called bootstrap percentile) of an estimate . Repeatedly sampling the training set with replacement and obtain a bootstrap set of the the same size as the original training set. One can fit a model and estimate a parameter with each bootstrap data set, and then estimate the standard error using the estimated parameters by the bootstrap model, assuming there are \\(B\\) bootstrap data sets: \\[\nSE_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{r=1}^B (\\hat{\\alpha}^{*r}-\\bar{\\hat{\\alpha}}^*)^2   }\n\\]\n\nNote sometimes sampling with replacement must take caution, for example, one can’t simply sample a time series with replacement because the data are sequential.\nEstimate prediction error: Each bootstrap sample has significant overlap with the original data, in fact, about 2/3 of the original data points appear in each bootstrap sample. If we use the original data set as the validation set, This will cause the bootstrap to seriously underestimate the true prediction error. To fix this, one can only use predictions on those samples that do not occur (by chance) in a bootstrap sample.\nBootstrap vs. Permutation test: permutation methods sample from an estimated null distribution for the data, and use this to estimate \\(p\\)-values and False Discovery Rates for hypothesis tests.\nThe bootstrap can be used to test a null hypothesis in simple situation. Eg. If \\(H_0: \\theta=0\\), we can check whether the confidence interval for \\(\\theta\\) contains zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#homework",
    "href": "ch5.html#homework",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.2 Homework",
    "text": "5.2 Homework\n\nConceptual: 1,2,3,4\nApplied: 5–9, at least one.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#code-gist",
    "href": "ch5.html#code-gist",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.3 Code Gist",
    "text": "5.3 Code Gist\n\n5.3.1 Python\nnp.empty(1000) #create an array without initializing\nquartiles = np.percentile(arr, [25, 50, 75])\n\n\n5.3.2 Numpy\nc = np.power.outer(row, col) # mesh of row[i]^col[j] power. \n# random choice \nrng = np.random.default_rng(0)\nalpha_func(Portfolio,\n           rng.choice(100, # random numbers are selected from arange(100)\n                      100, #size\n                      replace=True))\n\n    \n\n\n5.3.3 Pandas\nnp.cov(D[['X','Y']].loc[idx], rowvar=False) #cov compute corr of variables. rowvar-False: cols are vars.\n\n\n5.3.4 Graphics\n\n\n5.3.5 ISLP and statsmodels\n# function that evalues MSE for training a model\ndef evalMSE(terms,    #predictor variables\n            response, #response variable\n            train,\n            test):\n\n   mm = MS(terms)\n   X_train = mm.fit_transform(train)\n   y_train = train[response]\n\n   X_test = mm.transform(test)\n   y_test = test[response]\n\n   results = sm.OLS(y_train, X_train).fit()\n   test_pred = results.predict(X_test)\n\n   return np.mean((y_test - test_pred)**2)\n\n# Compare polynomial models of different degrees\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)],\n                       'mpg',\n                       Auto_train,\n                       Auto_valid)\nMSE\n\n# Estimating the accuracy of a LR model using bootstrap\n\n# Compute the SE of the boostraped values computed by func                      \ndef boot_SE(func,\n            D,\n            n=None,\n            B=1000,\n            seed=0):\n    rng = np.random.default_rng(seed)\n    first_, second_ = 0, 0\n    n = n or D.shape[0]\n    for _ in range(B):\n        idx = rng.choice(D.index,\n                         n,\n                         replace=True)\n        value = func(D, idx)\n        first_ += value\n        second_ += value**2\n    return np.sqrt(second_ / B - (first_ / B)**2) #compute var. \ndef boot_OLS(model_matrix, response, D, idx):\n    D_ = D.loc[idx]\n    Y_ = D_[response]\n    X_ = clone(model_matrix).fit_transform(D_) #clone create a deep copy. \n    return sm.OLS(Y_, X_).fit().params\n    \nquad_model = MS([poly('horsepower', 2, raw=True)]) #raw=True: not normalize the feature\nquad_func = partial(boot_OLS,\n                    quad_model,\n                    'mpg')\nboot_SE(quad_func, Auto, B=1000)\n\n\n\n5.3.6 sklearn\nfrom functools import partial\nfrom sklearn.model_selection import \\\n     (cross_validate,\n      KFold,\n      ShuffleSplit)\nfrom sklearn.base import clone\nfrom ISLP.models import sklearn_sm #wrapper to feed a sm model to sklearn\n\n#Cross Validation\nhp_model = sklearn_sm(sm.OLS,\n                      MS(['horsepower']))\nX, Y = Auto.drop(columns=['mpg']), Auto['mpg']\ncv_results = cross_validate(hp_model,\n                            X,\n                            Y,\n                            cv=Auto.shape[0]) #cv=K.loocv. Can use cv=KFold()object\ncv_err = np.mean(cv_results['test_score']) # test_score: MSE\ncv_err\n\n# Use KFold to partition instead of using an integer. \ncv_error = np.zeros(5)\ncv = KFold(n_splits=10,\n           shuffle=True,#shuffle before splitting\n           random_state=0) # use same splits for each degree\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M,\n                          X,\n                          Y,\n                          cv=cv)\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\n\n# using ShuffleSplit() method \nvalidation = ShuffleSplit(n_splits=10,\n                          test_size=196,\n                          random_state=0)\nresults = cross_validate(hp_model,\n                         Auto.drop(['mpg'], axis=1),\n                         Auto['mpg'],\n                         cv=validation)\nresults['test_score'].mean(), results['test_score'].std()\n\n\n#View skleanrn fitting results using model.results_\nhp_model.fit(Auto, Auto['mpg']) # hp_model is a sklearn model sk_model.fit(X, Y) for trainning\nmodel_se = summarize(hp_model.results_)['std err'] #summarize is an ISLP function\nmodel_se\n\n\n\n5.3.7 Useful code snippet",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch6.html#best-subset-selecttion",
    "href": "ch6.html#best-subset-selecttion",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.1 Best Subset Selecttion",
    "text": "6.1 Best Subset Selecttion\nAlgorithm\n\nFit the data with the null model \\(\\mathcal{M}_0\\), which contains no predictors. This model simply set \\(Y=\\text{mean}(y_i)\\).\nfor \\(k=1, 2, \\cdots, p\\): fit \\(p \\choose k\\) models containing exactly \\(k\\) predictors. Pick the best one that having the smallest RSS or largest \\(R^2\\) (or deviance for classification problem, i.e., \\(-2\\max \\log (\\text{likelihood})\\) on the training set, called \\(\\mathcal{M}_k\\). Note for each categorical variable with \\(L\\)-level, there are \\(L-1\\) dummy variables.\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\). If cross-validation is used, then Step 2 is repeated on each training fold, and the validation errors are averaged to select the best \\(k\\). The the model \\(\\mathcal{M}_k\\) fit on the full training set is delivered for chosen \\(k\\).\n\nBest subset selection suffers - high computation: needs to compute \\(2^p\\) models - overfitting due to the large search space of models",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#stepwise-selection",
    "href": "ch6.html#stepwise-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.2 Stepwise selection",
    "text": "6.2 Stepwise selection\nBoth Forward and Backward selection are stepwise selection. They are used when \\(p\\) is large. They searches over \\(1+p(p+1)/2\\) models and are greedy algorithm and is not guaranteed to find the best possible model out of all \\(2^p\\) models.\n\nBackward selection requires \\(n&gt;p\\) (so that the full model can be fit);\nForward selection can be used when \\(n&lt;p\\) but only fits up to models with \\(n\\) variables.\nOne can combine forward and backward selection to a hybrid selection.\n\n\n6.2.1 Forward Stepwise Selection\nAdding one variable at at time that offers the greatest additional improvement.\nAlgorithm\n\nFit the data with the null model \\(\\mathcal{M}_0\\), which contains no predictors. This model simply set \\(Y=\\text{mean}(y_i)\\).\nfor \\(k=1, 2, \\cdots, p-1\\):\n\n\nfit all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nPick the best one that having the smallest RSS or largest \\(R^2\\) on the training set, called \\(\\mathcal{M}_{k+1}\\).\n\n\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\).\n\n\n\n6.2.2 Backward Stepwise Selection\nIt begins with the full model with all variables, and iteratively removing one variable at at time.\nAlgorithm\n\nFit the data with the full model \\(\\mathcal{M}_p\\), which contains all predictors.\nfor \\(k=p, p-1, \\cdots, 1\\):\n\n\nfit all \\(k\\) models that contains all but one of the predictors in \\(\\mathcal{M}_k\\).\nPick the best one that having the smallest RSS or largest \\(R^2\\) on the training set, called \\(\\mathcal{M}_{k-1}\\).\n\n\nSelect the best one among \\(\\mathcal{M}_0, \\cdots, \\mathcal{M}_p\\) using cross-validation or other measures such as \\(C_p (AIC)\\), \\(BIC\\) or adjusted \\(R^2\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#model-selection",
    "href": "ch6.html#model-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.3 Model selection",
    "text": "6.3 Model selection\nModels with all predictors always have the smallest \\(RSS\\) or largest \\(R^2\\) on the training set. Therefore they are not suitable to choose the best one among models with different number of predictors.\nWe ought to estimate the test error on a test set. This may be done - indirectly by adjusting the training error to account for the bias due to overfitting: \\(C_p\\) (equivalently, AIC in case of linear model with Gaussian errors), BIC and adjusted \\(R^2\\).\n\nMallow’s \\(C_p\\): \\[\n  C_p =\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\n  \\] where, \\(d\\) us the number of parameters and \\(\\hat{\\sigma}^2 \\approx Var{\\epsilon}\\), typically estimated by using the full model containing all variables. \\(C_p\\) is an unbiased estimate of test MSE.\nAIC is defined for models fit by maximmum likelihood. \\[\n  AIC = -2\\log L + 2d\n  \\] where, \\(L\\) is the maximum likelihood function for the estimated model. For linear regression with Gaussian error, \\(AIC\\propto C_p\\).\nBIC \\[\n  BIC = \\frac{1}{n}( RSS + \\log (n)d \\hat{\\sigma}^2 )\n  \\] Since \\(\\log n&gt;\\) for \\(n&gt;7\\), the BIC places a higher penalty on models with many variables, and hence select smaller models than \\(C_p\\).\nAdjusted \\(R^2\\) (larger value is better)\n\\[\n\\text{Adjusted }R^2=1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}\n\\] \\(RSS/(n-d-1)\\) may increase or decrease depends on \\(d\\). Unlike \\(R^2\\), adjusted \\(R^2\\) pays a price for the inclusion of unnecessary variables in a model.\n\\(C_p\\), AIC, BIC, adjusted \\(R^2\\),are not appropriate in high-dimentional setting, as the estimated \\(\\hat{\\sigma}^2 \\approx 0\\) (when \\(p\\ge n\\)).\ndirectly by cross-validation (or validation). It does not require estimate \\(\\sigma^2\\). It has a wide range of usage, as it may difficult to estimate \\(d\\) or \\(\\sigma^2\\). One can choose the model that has the smallest test error or using the one-standard-error rule to select the model that has a smaller size:\n\ncalculate the SE of the estimated test MSE for each model size.\nidentify the lowest test MSE\nchoose the smallest model for which its test MSE is within one SE of the lowest point.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#shrinkage-methods-for-variable-selection",
    "href": "ch6.html#shrinkage-methods-for-variable-selection",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.4 Shrinkage methods for Variable selection",
    "text": "6.4 Shrinkage methods for Variable selection\nThe shrinkage offers an alternative to selecting variables by adjusting a hyperparameter that trades-off RSS and the model parameter magnitudes. Shrinkage methods will produce a different set of coefficients for a different \\(\\lambda\\). Cross-validation may be used to select the best \\(\\lambda\\). After the \\(\\lambda\\) is selected, one can fit a final model using the entire training data set.\nThe reason shrinkage methods may perform better than OLS is rooted in bias-variance trade-off: as \\(\\lambda\\) increases, the flexibility of the model decreases b ecause of shrunk coefficients, leading to decreased variance but increased bias.\n\n6.4.1 Ridge regression: minimize the following objective\n\\[\nRSS + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] The ridge regression is equivalent to \\[\n\\text{minimize } RSS \\qquad \\text{subject to } \\sum_{j=1}^p \\beta_j^2 \\le s\n\\] for some \\(s\\ge 0\\).\n\nIt encourages the model parameters to shrink toward zero and find a balance between RSS and model parameter magnitudes. Cross-validation is used to find the best tuning parameter \\(\\lambda\\). When \\(\\lambda\\) is large, \\(\\beta_j\\to 0\\). Note, Ridge shrinks all coefficients and include all \\(p\\) variables.\nThe OLS coefficients estimates are scale equivariant: regardless of how \\(X_j\\) is scaled, \\(X_j\\hat{\\beta}_j\\) remain the same: if \\(X_j\\) is multiplied by \\(c\\), this will simply leads to \\(\\hat{\\beta}_j\\) be scaled by a factor of \\(1/c\\).\nIn contrast, when multiplying \\(X_j\\) by a factor, this may significantly change the ridge coefficients. Ridge coefficients depends on \\(\\lambda\\) and the scale of \\(X_j\\), and may even on the scaling of other predictors. Therefore, it is best practice to standardize the predictors before fitting a ridge model: \\[\n\\tilde{x}_{ij} =\\frac{x_{ij}}{\\frac{1}{n} \\sum_{i=1}^n(x_{ij} - \\bar{x}_j)}\n\\]\nRidge regression works best in situations where the OLS estimates have high variance, especially when \\(p\\) is large.\nRidge will include all \\(p\\) variables in the final model.\n\n\n\n6.4.2 The Lasso (Least Absolute Shrinkage and Selection Operator)\n\nThe Lasso replaces the \\(\\ell^2\\) error with \\(\\ell^1\\) penalty. Lasso can force some coefficients to become exactly zero when \\(\\lambda\\) is large enough. Thus it can actually performs variable selection hence better interpretation. Again, cross-validation is employed to select \\(\\lambda\\).\nThe reason Lasso can perform variable selection is because the objective function is equivalent to\n\n\\[\\text {minimizing RSS}, \\text{subject to } \\sum_{j=1}^p |\\beta_j| \\le s\n  \\] for some \\(s\\). The contour of RSS in general only touch the \\(\\ell_1\\) ball at its vertex, at which a minimum is obtained with some variables vanishes. In contrast, in the ridge situation, the \\(\\ell_2\\) ball is round, and in general, the contour of the RSS function only touches the sphere at a surface point where a minimum is obtained with no variable vanishes.\n\nNeither ridge nor the lasso will universally dominate the other. When the response depends on a small number of predictors, one may expect lasso performs better; but in practice, this is never known in advance.\nCombining ridge and lasso leads to elastic net method.\nit is well known that ridge tends to give similar values coefficient values to correlated variables, while lasso may give quite different coefficient values to correlated variables.\nridge regression shrinks all coefficients by the same proportion. While lasso perform soft-thresholding, shrink all coefficients by similar amount, and sufficient small coefficients are shrunk all the way to zero.\nboth ridge and lasso can be considered as computationally feasible approximation to the best subset selection which can be equivalently formulated as: \\[\n\\text{minimize } RSS \\qquad \\text{subject to } \\sum_{j=1}^p I(\\beta_j\\ne 0) \\le s.\n\\]\nBayesian formulation: Both ridge and lasso can be interpreted as maximize the posterior probability (MAP) \\[\np(\\beta|X,Y)\\propto f(Y|X,\\beta) p(\\beta|X)=f(Y|X,\\beta)p(\\beta)\n\\] where \\(p(\\beta)= \\prod_{j=1}^p g(\\beta_j)\\) with some density function \\(g\\) is the believed prior on \\(\\beta\\).\n\nif \\(g\\) is Gaussian with mean zero and standard deviation a function of \\(\\lambda\\), then it follows the solution \\(\\beta\\) given by the ridge is the same as maximizing the posterior \\(p(\\beta|X,Y)\\), that is, \\(\\beta\\) is the posterior mode. In fact, \\(\\beta\\) is also the posterior mean. Since the Gaussian prior is flat at near zero, ridge assumes the coefficients are randomly distributed about zero.\nif \\(g\\) is double-exponential (Laplace) with mean zero and scale parameter a function of \\(\\lambda\\), then it follows the solution \\(\\beta\\) given by the lasso is the same as maximizing the posterior \\(p(\\beta|X,Y)\\), that is, \\(\\beta\\) is the posterior mode. In this case \\(\\beta\\) is not the posterior mean. Since the Laplacian prior is steeply peaked at zero, lasso expects a priori that many coefficients are (exactly) zero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#dimension-reduction-methods-transforming-x_j.",
    "href": "ch6.html#dimension-reduction-methods-transforming-x_j.",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.5 Dimension reduction methods: transforming \\(X_j\\).",
    "text": "6.5 Dimension reduction methods: transforming \\(X_j\\).\nThere are two types of dimension reduction methods for regression: a) PCA regression, b) Partial list squares PLS. Both are designed to handle when the OLS breaks down due to that there are large number of correlated variables.\n\n6.5.1 PCA regression: first use PCA to obtain \\(M\\)- PCA as linear combinations (directions) of the original \\(p\\) predictors:\n\\[\n  Z_m =\\sum_{j=1}^p \\phi_{mj} X_j, \\qquad 1\\le m \\le M,  \n   \\tag{6.1}\\] where, the \\(\\phi_{mj}\\) are called PCA loadings, and subject to the norm \\(\\sum_{j=1}^p\\phi_{mj}^2=1\\) for each \\(m\\). Note \\(Z_m\\) is a vector of length equal to the length of \\(X_j\\), which is the number of data points \\(n\\). The component of \\(Z_m\\): \\(z_{im}\\), \\(1\\le i \\le n\\) are called PCA scores. \\(z_{im}\\) is a single number summary of the \\(p\\) predictors with the \\(m\\)-th PCA for the \\(i\\)-th observation. PCA is not a feature selection method.\nThe first PCA defines the direction that contains the largest variance in \\(X\\), and minimize the sum of squared perpendicular distances to each point (the projection error on the PCA), that is it defines the line that is as close as possible to the data; (In fact, the first PCA is given by the eigenvector of the largest eigenvalue of the covariance matrix \\(\\frac{1}{n-1}X^TX\\)). The second PCA is orthogonal to the first PCA and has the second largest variance and is uncorrelated with the first, and so on. These directions are obtained in an unsupervised way, as \\(Y\\) is not used to obtain these components. Consequently, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\nPCA is typically conducted after standardizing the data \\(X\\), as without scaling, the high variance variables will tend to have higher influence on the obtained PCAs.\nWe then use OLS to fit a linear regression model \\[\ny_i =\\theta_0 +\\sum_{m=1}^M \\theta_m z_{im}+\\epsilon_i, \\qquad i=1,2,\\cdots, n\n\\tag{6.2}\\]\nAfter substitute Equation 6.1 into equation Equation 6.2, one can find that \\[\n\\sum_{m=1}^M \\theta_mz_{im} =\\sum_{j=1}^p \\beta_jx_{ij}\n\\] with \\[\n\\beta_j = \\sum_{m=1}^M \\theta_m\\phi_{mj}.\n\\tag{6.3}\\] Eq. Equation 6.3 has the potential to bias the coefficient estimates, but selecting \\(M&lt;&lt; p\\) can significantly reduce the variance. So model Equation 6.2 is a special case of linear regression subject to the constants Equation 6.3.\nPCR and ridge are closely related and one can think of ridge regression as a continuous version of PCR.\n\n\n6.5.2 Partial Least Squares\nSimilar to PCAR, PLS also first identifies a new set of features \\(Z_1, Z_2, \\cdots, Z_m\\), each of which is a linear combinations of the original features, and then fits a linear model via OLS with these new \\(M\\) features.\nBut PLS identifies these new features in a supervised way, that is, PLS uses \\(Y\\) in order to identify the new features that not only approximate the old features well, but also are related to the response, i.e., these new features explain both the response and the predictors.\nFirst PLS standardizes the \\(p\\) predictors. PLS identifies the first component \\(Z_1 = \\sum_{j=1}^p \\phi_{1j}X_j\\) by choosing \\(\\phi_{1j}=&lt;X_j, Y&gt;\\), the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\). Since this coefficient is equal to the correlation between \\(Y\\) and \\(X_j\\), PLS places the highest weight on the variables that are most strongly related to \\(Y\\). The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response.\nNext, PLS orthogonality each \\(X_j\\) with respect to \\(Z_1\\), that is, replace each \\(X_j\\) with the residual by regressing \\(X_j\\) on \\(Z_1\\), and then repeat the same process.\nWhen \\(p\\) is large, especially \\(p&gt;n\\), the forward selection method, shrinkage methods (lasso or ridge), PCR, PLR fit a less flexible model, hence particularly useful in performing regression in high-dimensional settings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#homework",
    "href": "ch6.html#homework",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.6 Homework:",
    "text": "6.6 Homework:\n\nConceptual: 1–4\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch6.html#code-snippet",
    "href": "ch6.html#code-snippet",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.7 Code Snippet",
    "text": "6.7 Code Snippet\n\n6.7.1 Python\nnp.isnan(Hitters['Salary']).sum()\n\n\n\n6.7.2 Numpy\nnp.linalg.norm(beta_hat) #L2 norm. ord=1: L1  ord='inf': max norm.\n\n\n6.7.3 Pandas\nHitters.dropna();\nsoln_path = pd.DataFrame(soln_array.T,\n                         columns=D.columns,\n                         index=-np.log(lambdas))\nsoln_path.index.name = 'negative log(lambda)'\n\n\n6.7.4 Graphics\nax.errorbar(np.arange(n_steps), \n            cv_mse.mean(1), #mean of each row (model)\n            cv_mse.std(1) / np.sqrt(K), #estimate standard error of the mean\n            label='Cross-validated',\n            c='r') # color red\n            \nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--') # plot a verticalline\n\n\n6.7.5 ISLP and statsmodels\n#Estimate Var(epsilon)\n\ndesign = MS(Hitters.columns.drop('Salary')).fit(Hitters)\ndesign.terms # to see the variable names in the design matrix\nY = np.array(Hitters['Salary'])\nX = design.transform(Hitters)\nsigma2 = OLS(Y,X).fit().scale  #.scale: RSE: residual standard error estimating \n\n# Forward Selection using ISLP.models and a scoring function\nfrom ISLP.models import \\\n     (Stepwise,\n      sklearn_selected,\n      sklearn_selection_path)\nstrategy = Stepwise.first_peak(design,\n                               direction='forward',\n                               max_terms=len(design.terms))\nhitters_Cp = sklearn_selected(OLS,\n                               strategy,\n                               scoring=neg_Cp)\n                               #default scoring MSE, will choose all variables\nhitters_Cp.fit(Hitters, Y) # the same as hitters_Cp.fit(Hitters.drop('Salary', axis=1), Y)\nhitters_Cp.selected_state_\n\n#Forward selection using cross-validation\nstrategy = Stepwise.fixed_steps(design,\n                                len(design.terms),\n                                direction='forward')\nfull_path = sklearn_selection_path(OLS, strategy) #using default scoring MSE\nfull_path.fit(Hitters, Y) # there are , 19 variables, 20 models\nYhat_in = full_path.predict(Hitters)\n\n#calculate in-sample mse\n\nmse_fig, ax = subplots(figsize=(8,8))\ninsample_mse = ((Yhat_in - Y[:,None])**2).mean(0) #Y[:,None]: add a second axis, create a column vector\n                        #[yw] mean(0): calculate mean along row, i.e., for each col. mean(1): calculate mean for each row\n\n#Cross-validation\nK = 5\nkfold = skm.KFold(K,\n                  random_state=0,\n                  shuffle=True)\nYhat_cv = skm.cross_val_predict(full_path,\n                                Hitters,\n                                Y,\n                                cv=kfold)\n# Cross-validation mse\ncv_mse = []\nfor train_idx, test_idx in kfold.split(Y):\n    errors = (Yhat_cv[test_idx] - Y[test_idx,None])**2\n    cv_mse.append(errors.mean(0)) # column means\ncv_mse = np.array(cv_mse).T\n\n#validation approach using ShuffleSplit\nvalidation = skm.ShuffleSplit(n_splits=1, # only split one time. \n                              test_size=0.2,\n                              random_state=0)\nfor train_idx, test_idx in validation.split(Y):\n    full_path.fit(Hitters.iloc[train_idx], #note needing to use iloc\n                  Y[train_idx])\n    Yhat_val = full_path.predict(Hitters.iloc[test_idx])\n    errors = (Yhat_val - Y[test_idx,None])**2\n    validation_mse = errors.mean(0)\n\n\n\n6.7.6 sklearn\nrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\n\n#Best subset selection using 10bnb\n\nD = design.fit_transform(Hitters)\nD = D.drop('intercept', axis=1) #needs to drop intercept\nX = np.asarray(D)\npath = fit_path(X, \n                Y,\n                max_nonzeros=X.shape[1]) #fit_path: a funciton from l0nb. use all variables\n                # max_nonzeros: max nonzero coefficients in the fitted model.\n\n# Ridge Regression\nsoln_array = skl.ElasticNet.path(Xs, # standardized, no intercept\n                                 Y,\n                                 l1_ratio=0., #ridge\n                                 alphas=lambdas)\n# Using pipline\nridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)\nscaler = StandardScaler(with_mean=True,  with_std=True)\npipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\npipe.fit(X, Y)\nridge.coef_\n\n# Validation\n\nvalidation = skm.ShuffleSplit(n_splits=1,\n                              test_size=0.5,\n                              random_state=0) # validation is a generator\nridge.alpha = 0.01\nresults = skm.cross_validate(ridge,\n                             X,\n                             Y,\n                             scoring='neg_mean_squared_error',\n                             cv=validation) # using the strategy defined in validation\n-results['test_score']\n\n# GridSearchCV()\nparam_grid = {'ridge__alpha': lambdas}\ngrid = skm.GridSearchCV(pipe,\n                        param_grid,\n                        cv=validation, # or use cv=kfold (5-fold CV defined separately)\n                        scoring='neg_mean_squared_error') #default scoring=R^2\ngrid.fit(X, Y)\ngrid.best_params_['ridge__alpha']\ngrid.best_estimator_\ngrid.cv_results_['mean_test_score']\ngrid.cv_results_['std_test_score']\n\n\n#Plot CV MSE\nridge_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            -grid.cv_results_['mean_test_score'],\n            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\nax.set_ylim([50000,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);\n\n# Use ElasticNetCV()\nridgeCV = skl.ElasticNetCV(alphas=lambdas, # ElasticNetCV accepts a sequence of alphas\n                           l1_ratio=0,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler), # scaling is done once. \n                         ('ridge', ridgeCV)])\npipeCV.fit(X, Y)\ntuned_ridge = pipeCV.named_steps['ridge']\ntuned_ridge.mse_path_\ntuned_ridge.alpha_ # best alpha\ntuned_ridge.coef_\n\n# Evaluating test Error of Cross-validated Ridge \n\nouter_valid = skm.ShuffleSplit(n_splits=1, \n                               test_size=0.25,\n                               random_state=1)\ninner_cv = skm.KFold(n_splits=5,\n                     shuffle=True,\n                     random_state=2)\nridgeCV = skl.ElasticNetCV(alphas=lambdas, # a sequence of lambdas\n                           l1_ratio=0,\n                           cv=inner_cv) # K-fold validation\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('ridge', ridgeCV)]);\n                         \n                         \nresults = skm.cross_validate(pipeCV, \n                             X,\n                             Y,\n                             cv=outer_valid,\n                             scoring='neg_mean_squared_error')\n-results['test_score']\n\n# Lasso regression\nlassoCV = skl.ElasticNetCV(n_alphas=100, #test 100 alpha values\n                           l1_ratio=1,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('lasso', lassoCV)])\npipeCV.fit(X, Y)\ntuned_lasso = pipeCV.named_steps['lasso']\ntuned_lasso.alpha_\ntuned_lasso.coef_\nnp.min(tuned_lasso.mse_path_.mean(1)) # miminum avg mse\n\n#to get the soln path\nlambdas, soln_array = skl.Lasso.path(Xs, # standarsized, no -intercept\n                                    Y,\n                                    l1_ratio=1,\n                                    n_alphas=100)[:2]\n\n#PCA and PCR\npca = PCA(n_components=2)\nlinreg = skl.LinearRegression()\npipe = Pipeline([('scaler', scaler), \n                 ('pca', pca),\n                 ('linreg', linreg)])\npipe.fit(X, Y)\npipe.named_steps['linreg'].coef_\npipe.named_steps['pca'].explained_variance_ratio_\n\n# perform Grid search\nparam_grid = {'pca__n_components': range(1, 20)} #PCA needs n_components &gt;0\ngrid = skm.GridSearchCV(pipe,\n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, Y)\n\n# cross-validation a null model\ncv_null = skm.cross_validate(linreg,\n                             Xn,\n                             Y,\n                             cv=kfold,\n                             scoring='neg_mean_squared_error')\n-cv_null['test_score'].mean()\n\n#PLS\npls = PLSRegression(n_components=2, \n                    scale=True) # standarsize the data \npls.fit(X, Y) # X has no-intercept \n\n# Cross-validation\nparam_grid = {'n_components':range(1, 20)}\ngrid = skm.GridSearchCV(pls,\n                        param_grid,\n                        cv=kfold,\n                        scoring='neg_mean_squared_error')\ngrid.fit(X, Y)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "ch7.html#polynomials",
    "href": "ch7.html#polynomials",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.1 Polynomials",
    "text": "7.1 Polynomials\nThe basis functions are simply the polynomial functions of different degrees. Polynomial terms of higher powers for \\(X_j\\) or interaction terms \\(X_iX_j\\) are used, but the model is still a linear model in the coefficients \\(\\beta_j\\). The optimum degree \\(d\\) can be chosen by cross-validation. polynomial terms can be included in either a linear regression model or a logistic regression model.\nIn practice hardly a degree greater than 3 or 4 is used because a higher degree polynomial exhibits high degree of oscillation, especially near the boundary (Runge’s phenomenon). This is because a polynomial imposes a global structure.\nThe standard error at a point \\(x_0\\)is calculated by \\[\nSE[\\hat{f}(x_0)] = \\ell_0^T \\hat{\\mathbf{C}} {\\ell}_0\n\\] where, \\(\\ell_0^T =(1, x_0, x_0^2, \\cdots, x_0^d)\\), and \\(\\hat{\\mathbf{C}}\\) is the covariance matrix of the estimated coefficients \\(\\beta_j\\), \\(j=0, 1, \\cdots, d\\) obtained from the OLS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#step-functions",
    "href": "ch7.html#step-functions",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.2 Step functions",
    "text": "7.2 Step functions\nA step function is a piece-wise constant function. Cut a \\(X\\) variable into \\(K+1\\) regions using \\(K\\) cut points and then either use one-hot coding with \\(K+1\\) dummy variables (and no intercept, in this case, each coefficient can be interpreted as the average value in that region) or create \\(K\\) dummy variables with an intercept to represent all those regions (in this case, the average value in that region equals to the intercept plus the coefficient). Choice of cut-points (knots) can be problematic. Binning the \\(X\\) variable amounts to convert a continuous variable into an ordered categorical variable.\nThe basis functions are simply indicator functions on each region: \\[\nb_j(x_i)= I(c_j\\le x_i&lt; c_{j+1}).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#piececwise-polynomials",
    "href": "ch7.html#piececwise-polynomials",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.3 Piececwise polynomials",
    "text": "7.3 Piececwise polynomials\nIt overcomes the disadvantage of polynomial basis which imposes a global structure. It fits separate low-degree polynomials over different regions of \\(X\\) separated by knots.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#splines",
    "href": "ch7.html#splines",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.4 Splines",
    "text": "7.4 Splines\nSplines are piece-wise polynomials of degree \\(d\\) that are continuous up to \\(d-1\\) derivatives at each knot. E.g. a cubic spline with \\(K\\) knots has continuity up to second derivative at each knot, and has degree of freedom of \\(K+4\\).\n\nlinear spline: with knots \\(\\xi_k\\), \\(k=1, \\cdots, K\\) is a piece-wise linear polynomial that is continuous at each knot. \\[\ny= \\beta_0+\\beta_1 b_1(x)+\\cdots + \\beta_{K+1}b_{K+1}(x)+\\epsilon,\n\\] where, \\(b_k\\) are basis functions defined by \\[\\begin{align}\nb_1(x) & = x \\\\\nb_{k+1}(x) & = (x- \\xi_k)_{+}, \\qquad k=1, \\cdots K\n\\end{align}\\] Here the positive part is defined by \\[x_{+}=\\begin{cases}\nx, &  \\text{ if } x&gt;0 \\\\\n0 &  \\text{ otherwise}\n\\end{cases}\n\\]\ncubic splines: with knots \\(\\xi_k\\), \\(k=1, \\cdots, K\\) is a piecewise cubic polynomials with continuous derivatives up to order 2 at each knot.\n\n\\[\ny= \\beta_0+\\beta_1 b_1(x)+\\cdots + \\beta_{K+3}b_{K+3}(x)+\\epsilon,\n\\] knot placement: General principle is that placing more knots in places where the function might vary most rapidly. In practice, it is common to place them at uniform quantiles of the observed \\(X\\). This can be done by specifying a dof, and then let the algorithm to calculate the knots at uniform quantiles. Note a natural spline have more internal knots than a regression spline for the same dof.\nFor a regular cubic spline, the basis functions are defined by \\[\\begin{align}\nb_1(x) & = x \\\\\nb_2(x) & = x^2 \\\\\nb_3(x) & = x^3 \\\\\nb_{k+3}(x) & = (x- \\xi_k)^3_{+}, \\qquad k=1, \\cdots K\n\\end{align}\\]\ndof: \\(K+4\\) number of parameters (including the intercept). A regression spline can have high variance at the outer range of the predictors. To remedy this, one can use a natural cubic spline.\na natural cubic spline extrapolates linearly ( as a linear function) beyond the internal knots. This adds \\(2\\times 2\\) extra constrains. A natural cubic spline allows to put more internal knots for the same degree of freedom as a regular cubic spline.\ndof: \\(K+2\\) (\\(K\\) only counts the internal knots; including the intercept).\nFor a smoothing spline: it is the solution \\(g\\) to the following problem: \\[\n  \\text{minimize}_{g\\in S}\\sum_{i=1}^n(y_i-g(x_i))^2 +\\lambda \\int g''(t)dt\n  \\]\nThe first term is the loss RSS and encourages \\(g(x_i)\\) matches \\(y_i\\). The second term is the penalty that penalize the variability in \\(g\\) (measured by \\(g''(t)\\)) by a tuning parameter \\(\\lambda \\ge 0\\). If \\(\\lambda=0\\) (no constraints on \\(g\\)), then the solution is just an interpolating polynomial. If \\(\\lambda\\to \\infty\\), then \\(g\\) is a linear function (because its second derivative is zero). \\(\\lambda\\) controls the bias-variance trade-off.\nThe smoothing spline is in fact a natural spline with knots at unique values of \\(x_i\\). But it is different than the natural spline. It is a shrunk version of a natural cubic spline, otherwise it would have too large (nominal) dof (number of parameters) because it has knots at unique values of \\(x_i\\). It avoids the knot selection issue and leaving a single \\(\\lambda\\) to tune. An effective degrees of freedom can be calculated for a smoothing spline as \\[\ndf_\\lambda =\\sum_{i=1}^n {\\{\\mathbf{S}_\\lambda}\\}_{ii},\n\\] where \\(\\mathbf{S}_\\lambda\\) is a \\(n\\times n\\) matrix determined by \\(\\lambda\\) and \\(x_i\\) such that the vector of \\(n\\) fitted values can be written as \\[\n\\hat{\\mathbf{g}}_\\lambda =\\mathbf{S}_\\lambda \\mathbf{y}.\n\\] \\(df_\\lambda\\) decreases from \\(n\\) to 2 as \\(\\lambda\\) increases from 0 to \\(\\infty\\).\nThe LOO cross-validation error can be efficiently computed by \\[\n\\text{RSS}_{cv}(\\lambda) =\\sum_{i=1}^n (y_i-\\hat{g}_\\lambda ^{(-i)}(x_i))^2=\\sum_{i=1}^n \\left[ \\frac{y_i-\\hat{g}_\\lambda (x_i)}{1-\\{ {\\mathbf S}_\\lambda\\}_{ii}} \\right]^2\n\\] - Local Regression: a non-parametric method. It is similar to spline, but allowing regions overlap. With a sliding weight function of span \\(s\\), fit separate (constant, linear, quadratic, for instance) fits over the range of \\(X\\) by weighted least squares.\nThe span \\(s\\) plays the similar role as \\(\\lambda\\) in a smoothing spline, it controls the flexibility of the local regression. The smaller \\(s\\) is, the more local and wiggle will be the fit.\nLocal regression is a memory based procedure, because like KNN, all training data are needed each time when making a prediction.\nLocal regression can be generalized to varying coefficient models that fits a multiple linear regression model that is global in some variables but local in another, such as time.\nLocal regression can be naturally extends to \\(p\\)-dimension using a \\(p\\)-dimensional neighborhood, but really used when \\(p\\) is larger than 3 or 4 because there will be generally very few training examples near \\(x_0\\) (curse of dimensionality)\n\nGAM (Generalized Additive Models): can be considered as an extension of multiple linear regression, replacing each feature \\(\\beta_jX_j\\) with an nonlinear function \\(f_j(X_j)\\). \\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) +\\cdots + f_p(x_{ip})  + \\epsilon\n\\] GAM can mix different \\(f_j\\), for example, a spline, or a linear term or even include low order interactive terms. The coefficients are hard to interpret, but the fitted values are of interest.\n\nGAM can be used in fitting a logistic regression model, that is \\[\n\\log \\frac{p(X)}{1-p(X)} =\\beta_0+f_1(X_1)+f_2(X_2)+\\cdots +f_p(X_p)\n\\]\nWhen fitting a GAM, and if OLS can not be used (such as when a smoothing spline is used), then the back fitting iterative method can be used: randomly initialize all variable coefficients; repeatedly hold all but one variable fixed, and perform a simple linear regression on that single variable, and update the corresponding coefficients until convergence. Convergence is typically very fast.\nPros and Cons of GAM\n\nflexible to model \\(f_j\\), eliminating the need to try different transformations on each variable\npotentially more accurate prediction\nbecause the model is additive, can easily examine the effects of \\(X_j\\) on \\(Y\\) by holding all of the other variables fixed.\nThe smoothness of \\(f_j\\) can be summarized by the effective dof.\ninteraction terms \\(X_jX_k\\) can be added.\nlow dimensional interaction functions of the form \\(f_{jk}(X_j, X_k)\\) can be added. Such term can be fit using a two-dimensional smoothers such as local regression or two dimensional splines.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#homework",
    "href": "ch7.html#homework",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.5 Homework:",
    "text": "7.5 Homework:\n\nConceptual: 1–5\nApplied: At least one.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "ch7.html#code-snippet",
    "href": "ch7.html#code-snippet",
    "title": "7  Chapter 7: Moving Beyond Linearity",
    "section": "7.6 Code Snippet",
    "text": "7.6 Code Snippet\n\n7.6.1 Python\n\n\n\n\n7.6.2 Numpy\nWage['education'].cat.categories # .cat is the categorical method accessor\nWage['education'].cat.codes\npd.crosstab(Wage['high_earn'], Wage['education'])\n\nnp.column_stack([Wage_['age'],\n                         Wage_['year'],\n                         Wage_['education'].cat.codes-1])\n\nXs = [ns_age.transform(age),\n      ns_year.transform(Wage['year']),\n      pd.get_dummies(Wage['education']).values] # -&gt; 5 education levels: 1-hot coding\nX_bh = np.hstack(Xs)\n\n\n\n7.6.3 Pandas\ncut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints. pd.cut is similar\n\n\n7.6.4 Graphics\nax.legend(title='$\\lambda$');\n\n\n\n7.6.5 ISLP and statsmodels\n\n\n\n\n7.6.6 sklearn\n\n\n7.6.7 Useful code snippets\n\n7.6.7.1 plot a model fit with confidence interval\ndef plot_wage_fit(age_df, \n                  basis, # ISL model object\n                  title):\n\n    X = basis.transform(Wage)\n    Xnew = basis.transform(age_df)\n    M = sm.OLS(y, X).fit()\n    preds = M.get_prediction(Xnew)\n    bands = preds.conf_int(alpha=0.05)\n    fig, ax = subplots(figsize=(8,8))\n    ax.scatter(age,\n               y,\n               facecolor='gray',\n               alpha=0.5)\n    for val, ls in zip([preds.predicted_mean,\n                      bands[:,0],\n                      bands[:,1]],\n                     ['b','r--','r--']):\n        ax.plot(age_df.values, val, ls, linewidth=3)\n    ax.set_title(title, fontsize=20)\n    ax.set_xlabel('Age', fontsize=20)\n    ax.set_ylabel('Wage', fontsize=20);\n    return ax\n\n\n7.6.7.2 Fitting with a step function\ncut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints\n# note pd.get_dummies(cut_age) is the X matrix\nsummarize(sm.OLS(y, pd.get_dummies(cut_age)).fit()) \n\n\n\n7.6.7.3 Fitting a spline\n#specifying internal knots\n\nbs_age = MS([bs('age',\n                internal_knots=[25,40,60],\n                name='bs(age)')]) #rename the variable names \nXbs = bs_age.fit_transform(Wage) # Xbs == bs_age above\nM = sm.OLS(y, Xbs).fit()\nsummarize(M)\n\n# specifying df \nbs_age0 = MS([bs('age',\n                 df=3, # df count does not include intercept. df=degree+ #knots\n                 degree=0)]).fit(Wage)\nXbs0 = bs_age0.transform(Wage)\nsummarize(sm.OLS(y, Xbs0).fit())\n\nBSpline(df=3, degree=0).fit(age).internal_knots_\n\n# Fit a natural spline\nns_age = MS([ns('age', df=5)]).fit(Wage) #df=degree+ #knots -2\nM_ns = sm.OLS(y, ns_age.transform(Wage)).fit()\nsummarize(M_ns)\n\n# fit a smoothing spline\nX_age = np.asarray(age).reshape((-1,1))\ngam = LinearGAM(s_gam(0, lam=0.6)) #gam is the smoothing spline model with a given lambda\ngam.fit(X_age, y)\n\n#Fiting a smoothing spline with an optimized lambda\ngam_opt = gam.gridsearch(X_age, y)\n\n\n# Fitting a smoothin spline by specifying a df (not including intercept)\nfig, ax = subplots(figsize=(8,8))\nax.scatter(X_age,\n           y,\n           facecolor='gray',\n           alpha=0.3)\nfor df in [1,3,4,8,15]:\n    lam = approx_lam(X_age, age_term, df+1) # find the lambda corresponding to a df. \n    age_term.lam = lam # update lambda\n    gam.fit(X_age, y)\n    ax.plot(age_grid,\n            gam.predict(age_grid),\n            label='{:d}'.format(df),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='Degrees of freedom');\n\n\n\n\n\n\n\n7.6.8 GAM\n### manually contruct basis \nns_age = NaturalSpline(df=4).fit(age) #df counts do not include intercepts. -&gt; 4 columns\nns_year = NaturalSpline(df=5).fit(Wage['year']) # -&gt; 5 cols\nXs = [ns_age.transform(age),\n      ns_year.transform(Wage['year']),\n      pd.get_dummies(Wage['education']).values] # -&gt; 5 education levels: 1-hot coding\nX_bh = np.hstack(Xs)\ngam_bh = sm.OLS(y, X_bh).fit()\n\n### Examinge partial effect\n\nage_grid = np.linspace(age.min(),\n                       age.max(),\n                       100)\nX_age_bh = X_bh.copy()[:100] # Take the first 100 rows of X_bh\n# calculate the row mean and make it a row vector in the shape of 1Xp, then broadcast\nX_age_bh[:] = X_bh[:].mean(0)[None,:] \nX_age_bh[:,:4] = ns_age.transform(age_grid)# replace the first 4 cols with basis functions evalued at age_grid\npreds = gam_bh.get_prediction(X_age_bh) #gam_bh is the GAM model with all 14 basis\nbounds_age = preds.conf_int(alpha=0.05)\npartial_age = preds.predicted_mean\ncenter = partial_age.mean() # center of the prediction \npartial_age -= center # center the prediction for better viz\nbounds_age -= center\nfig, ax = subplots(figsize=(8,8))\nax.plot(age_grid, partial_age, 'b', linewidth=3)\nax.plot(age_grid, bounds_age[:,0], 'r--', linewidth=3)\nax.plot(age_grid, bounds_age[:,1], 'r--', linewidth=3)\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage', fontsize=20);\n\n\n### Using a smoothing spline and pygam package\n#### Specifying lambda\n#### default \\lambda = 0.6 is used.\ngam_full = LinearGAM(s_gam(0) + # spline smoothing applies to the first col of the feature matrix\n                     s_gam(1, n_splines=7) + # smoothing applied to the 2nd col \n                     f_gam(2, lam=0)) # smothing applied to the 3rd col: a factor col\nXgam = np.column_stack([age,  #stack as columns\n                        Wage['year'],\n                        Wage['education'].cat.codes]) \ngam_full = gam_full.fit(Xgam, y)\n\ngam_full.summary() # verbose summary\n\n#### Plot partial effect using a plot_gam from ISLP.pygam\nfig, ax = subplots(figsize=(8,8))\nplot_gam(gam_full, 0, ax=ax) # 0: partial plot of the first component: age\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage - default lam=0.6', fontsize=20);\n\n### Specifying df\nage_term = gam_full.terms[0]\nage_term.lam = approx_lam(Xgam, age_term, df=4+1)\nyear_term = gam_full.terms[1]\nyear_term.lam = approx_lam(Xgam, year_term, df=4+1)\ngam_full = gam_full.fit(Xgam, y)\n\n#### Plot partial effect\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_full, 2)\nax.set_xlabel('Education')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of wage on education',\n             fontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n\n\n7.6.8.1 Anova for GAM\ngam_0 = LinearGAM(age_term + f_gam(2, lam=0)) # note age_term is a s_gam with df=4 defined above \ngam_0.fit(Xgam, y)\ngam_linear = LinearGAM(age_term +\n                       l_gam(1, lam=0) +\n                       f_gam(2, lam=0))\ngam_linear.fit(Xgam, y)\nanova_gam(gam_0, gam_linear, gam_full)\n\n\n\n7.6.8.2 Logistic GAM\ngam_logit = LogisticGAM(age_term + \n                        l_gam(1, lam=0) +\n                        f_gam(2, lam=0))\ngam_logit.fit(Xgam, high_earn)\n\n\n\n7.6.8.3 LOESS\nlowess = sm.nonparametric.lowess\nfig, ax = subplots(figsize=(8,8))\nax.scatter(age, y, facecolor='gray', alpha=0.5)\nfor span in [0.2, 0.5]:\n    fitted = lowess(y,\n                    age,\n                    frac=span,\n                    xvals=age_grid)\n    ax.plot(age_grid,\n            fitted,\n            label='{:.1f}'.format(span),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='span', fontsize=15);",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 7: Moving Beyond Linearity</span>"
    ]
  },
  {
    "objectID": "class_project.html",
    "href": "class_project.html",
    "title": "8  Class Project",
    "section": "",
    "text": "Goal\nto use various ML algorithms to predict a meaningful target \\(Y\\) by classification algorithms or regression algorithms. Present it at COS Research Sympsium in the end of April.\nData set: real-world stock price and volume data. We could start with just one stock, e.g. APAL, S&P 500, index, DJ index.\nSome ideas: * Create one model for each stock. * create a single model for all stocks. Needs to embed each stock in a feature space. Research?\nStart-up code: refer to the page https://github.com/ywanglab/Predicting_stock_movement/blob/main/Time_series_stock_data_analysis_ver2.ipynb. Perform some EDA to feel the data.\nreference ticker symbols: https://gist.github.com/quantra-go-algo/ac5180bf164a7894f70969fa563627b2\nQuestions: Which are the \\(X\\) variables? price, volume, return, day of week, month of year, etc. What is the \\(Y\\) variable? next-day price, next-day return, next-five-day average price, next-five-day-average return, etc.\nModels\n\nLinear regression:\n\nincluding continuous variables (price, volume), categorical variables (day-of-week, month-of-year)\ntransforming \\(X\\) (for including non-linear relation between \\(Y\\) and \\(X\\)) or \\(Y\\) (when \\(Y\\) is heteroschedatic, i.e., with varied \\(\\epsilon_i\\))\nplot residual plot to see if \\(Var(\\epsilon_i)\\) is changing. If yes, may appeal to transforming \\(Y\\), e.g., \\(\\log Y\\), \\(\\sqrt{Y}\\).\ninvestigate outliers (points with unusual \\(Y\\)-values) using the residual plot or looking at studentized residual.\ninvestigate high leverage points (with unusual \\(x\\) values), by calculating leverage statistics.\nInvestigate if there is colinearity among the variables by calculating VIF.\n\nclassification: predicting directions of the stock price movement. binary (with two direction), or multinomial (more than two values: e.g., up, same, down), LDA, QDA\nregularization of the parameters\nselection of variables: forward, backward, mixture, regularization, cross-validation\ndecision tree: random forest, boosting\nSVM: support vector machines\nNN",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Class Project</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "James, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023.\nAn Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "References"
    ]
  }
]