[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stat4500notes",
    "section": "",
    "text": "Preface\nThis is a Lecture note written for the course STAT 4500: Machine Learning offered at Auburn University at Montgomery. The course uses the textbook James et al. (2023).\nThis is a book wrtieen by Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\n\nJames, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023. An Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#on-your-own-computer",
    "href": "intro.html#on-your-own-computer",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.1 on Your own computer",
    "text": "1.1 on Your own computer\n\nyou can either git clone or download a zipped file containing the codes from the site: https://github.com/intro-stat-learning/ISLP_labs/tree/stable. If downloaded a zipped file of the codes, unzipped the file to a folder, for example, named islp. If git clone (preferred, you need to have Git installed on your computer, check this link for how to install Git https://ywanglab.github.io/stat1010/git.html), do git clone https://github.com/intro-stat-learning/ISLP_labs.git\nDownload and install the following software:\n\nAnaconda: Download anaconda and install using default installation options\nVisual Studio Code (VSC): Download VSC and install\nstart VSC and install VSC extensions in VSC: Python, Jupyter, intellicode\n(optional) Quarto for authoring: Download Quarto and install\n\nCreate a virtual environment named islp for Python. Start an anaconda terminal.\n  conda create -n islp python==3.10\n  conda activate islp\n  conda install pip ipykernel\n  pip install -r https://raw.githubusercontent.com/intro-stat-learning/ISLP_labs/v2.1.2/requirements.txt\nYou are ready to run the codes using VSC or jupyter lab.\n\nActivate the venv: conda activate islp\nStart a Anaconda terminal, navigate to the folder using the command cd path/to/islp, where path/to/islp means the file path to the folder islp, such as \\Users\\ywang2\\islp. Start VSC by typing code . in the anaconda terminal.\nopen/create a .ipynb or .py file.\nSelect the kernel islp\nRun a code cell by pressing Shift+Enter or click the triangular play button.\nContinue to run other cells.\nAfter finishing using VSC, close the VSC, and deactivate the virtual environment in a conda terminal: conda deactivate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "intro.html#use-google-colab",
    "href": "intro.html#use-google-colab",
    "title": "1  Setting up Python Computing Environment",
    "section": "1.2 Use Google Colab",
    "text": "1.2 Use Google Colab\nAll you need is a Google account. Sign in your Google account in a browser, and navigate to Google Colab. Google Colab supports both Python and R. Python is the default engine. Change the engine to R in Connect-&gt;change runtime type. Then you are all set. Your file will be saved to your Google Drive or you can choose to send it to your GitHub account (recommended).\n\n1.2.1 How to run a project file from your Google Drive?\nMany times, when you run a python file in Colab, it needs to access other files, such as data files in a subdirectory. In this case, it would be convenient to have the same file structure in the Google Colab user home directory. To do this, you can use Google Drive to store your project folder, and then mount the Google Drive in Colab.\nLet’s assume the project folder name, islp/.Here are the steps:\n\ngit clone the project folder (example: git clone https://github.com/intro-stat-learning/ISLP_labs.git) to your local folder. This step is only needed when you want to clone some remote repo from GitHub.\nUpload the folder (ex: islp) to Google Drive.\nOpen the file using Colab. In Google Drive, double click on the ipynb file, example, ch06.ipynb (or click on the three dots on the right end, and choose open with, then Google Colaborotary), the file will be opened by Google Colab.\nMount the Google Drive. In Google Colab, with the specific file (example, ch06.ipynb) being opened, move your cursor to the first code cell, and then click on the folder icon (this should be the fourth icon) on the upper left border in the Colab browser. This will open the file explorer pane. Typically you would see a folder named sample_data shown. On the top of the pane, click on the Google Drive icon to mount the Google Drive. Google Colab will insert the following code below the cursor in your opened ipynb file:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nRun this code cell by pressing SHIFT+ENTER, and follow the prompts to complete the authentication. Wait for ~10 seconds, your Google Drive will be mounted in Colab, and it will be displayed as a folder named drive in the file explorer pane. You might need to click on the Refresh folder icon to see the folder drive.\nOpen a new code cell below the above code cell, and type the code\n  %cd /content/drive/MyDrive/islp/\nThis is to change the directory to the project directory on the Google Drive. Run this code cell, and you are ready to run the file ch06.ipynb from the folder islp on your personal Google Drive, just like it’s on your local computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Python Computing Environment</span>"
    ]
  },
  {
    "objectID": "ch2.html#what-is-statistical-learning",
    "href": "ch2.html#what-is-statistical-learning",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.1 What is statistical learning?",
    "text": "2.1 What is statistical learning?\nFor the input variable \\(X\\in \\mathbb{R}^p\\) and response variable \\(Y\\in \\mathbb{R}\\), assume that \\[Y=f(X) + \\epsilon, \\] where \\(\\epsilon\\) is a random variable representing irreducible error. We assume \\(\\epsilon\\) is independent of \\(X\\) and \\(E[\\epsilon]=0\\). \\(\\epsilon\\) may include unmeasured variables or unmeasurable variation.\nStatistical learning is to estimate \\(f\\) using various methods. Denote the estimate by \\(\\hat{f}\\).\n\nregression problem: when \\(Y\\) is a continuous (quantitative) variable . In this case \\(f(x)=E(Y|X=x)\\) is the population regression function, that is, regression finds a conditional expectation of \\(Y\\).\nclassification problem: when \\(Y\\) only takes small number of discrete values, i.e., qualitative (categorical).\n\nLogistic regression is a classification problem, but since it estimates class probability, it may be considered as a regression problem.\n\nsupervised learning: training data \\(\\mathcal{Tr}=\\{(x_i, y_i):i\\in \\mathbb{Z}_n\\}\\): linear regression, logistic regression\nunsupervised learning: when only \\(x_i\\) are available. clustering analysis, PCA\nsemi-supervised learning: some data with labels (\\(y_i\\)), some do not.\nreinforcement learning: learn a state-action policy function for an agent to interacting with an environment to maximize a reward function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#why-estimate-f",
    "href": "ch2.html#why-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.2 Why estimate \\(f\\)?",
    "text": "2.2 Why estimate \\(f\\)?\nWe can use estimated \\(\\hat{f}\\) to\n\nmake predictions for a new \\(X\\), \\[\\hat{Y} =\\hat{f}(X). \\] The prediction error may be quantified as \\[E[(Y-\\hat{Y})^2] = (f(X)-\\hat{f})^2 +\\text{Var}[\\epsilon].\\] The first term of the error is reducible by trying to improve \\(\\hat{f}\\), where we assume \\(f\\), \\(\\hat{f}\\) and \\(X\\) are fixed.\nmake inference, such as\n\nWhich predictors are associated with the response?\nwhat is the relationship between the response and each predictor?\nis the assumed relationship adequate? (linear or more complicated?)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-estimate-f",
    "href": "ch2.html#how-to-estimate-f",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.3 How to estimate \\(f\\)",
    "text": "2.3 How to estimate \\(f\\)\nWe use obtained observations called training data \\(\\{(x_k, y_k): k \\in \\mathbb{Z}_n \\}\\) to train an algorithm to obtain the estimate \\(\\hat{f}\\).\n\nParametric methods: first assume there is a function form (shape) with some parameters. For example, a linear regression model with two parameters. Then use the training data to train or fit the model to determine the values of the parameters.\nAdvantages: simplify the problem of fit an arbitrary function to estimate a set of parameters.\nDisadvantages: may not be flexible unless with large number of parameters and/or complex function shapes.\nExample: linear regression,\nNon-parametric methods: Do not explicitly assume a function form of \\(f\\). They seek to estimate \\(f\\) directly using data points, can be quite flexible and accurate.\n**Disadvantage: need large number of data points\nExample: KNN (but breakdown for higher dimention. Typically only for \\(p\\le 4\\)), spline fit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#how-to-assess-model-accuracy",
    "href": "ch2.html#how-to-assess-model-accuracy",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.4 How to assess model accuracy",
    "text": "2.4 How to assess model accuracy\nFor regression problems, the most commonly used measure is the mean squared error (MSE), given by \\[\nMSE = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{f}(x_i))^2\n\\] For classification problems, typically the following error rate (classifications error) is calculated: \\[\n\\frac{1}{n} \\sum_{i=1}^{n} I(y_i\\ne \\hat{y}_i)\n\\] The accuracy on a training set can be arbitrarily increased by increasing the model flexibility. However, we are in general interested in the error on the test set rather on the training set, the model accuracy should be assessed on a test set.\nFlexible models tend to overfit the data, which essentially means they follow the error or noise too closely in the training set, therefore cannot be generalized to unseen cases (test set).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#model-selection",
    "href": "ch2.html#model-selection",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.5 Model Selection:",
    "text": "2.5 Model Selection:\nNo free lunch theorem\nThere is no single best method for all data sets, which means some method works better than other methods for a particular dataset. Therefore, one needs to perform model selections. Here are some principles.\n\n2.5.1 Trade-off between Model flexibility and Model Interpretability\nMore flexible models have higher degree of freedom and are less interpretable because it’s difficult to interpret the relationship between a predictor and the response.\nLASSO is less flexible than linear regression. GAM (generalized additive model) allows some non-linearity. Full non-linear models have higher flexibility, such as bagging, boosting, SVM, etc.\nWhen inference is the goal, then there are advantages to using simple and less flexible models for interpretability.\nWhen prediction is the main goal, more flexible model may be a choice. But sometimes, we obtain more accurate prediction using a simpler model because the underlying dataset has a simpler structure. Therefore, it is not necessarily true that a more flexible model has a higher prediction accuracy.\nOccam’s Razor: Among competing hypotheses that perform equally well, the one with the fewest assumptions should be selected.\n\n\n2.5.2 Model Selection: the Bias-Variance Trade-off\nAs the model flexibility increases, the training MSE (or error rate for classificiton) will decrease, but the test MSE (error rate) in general will not and will show a characteristic U-shape. This is because when evaluated at a test point \\(x_0\\), the expected test MSE can be decomposed into \\[\nE\\left[ (y_0-\\hat{f}(x_0))^2 \\right] = \\text{Var}[\\hat{f}(x_0)] + (\\text{Bias}(\\hat{f}(x_0)))^2+\\text{Var}[\\epsilon]\n\\] where the expectation is over different \\(\\hat{f}\\) on a different training set or on a different training step if the training process is stochastic, and \\[\n\\text{Bias}(\\hat{f}(x_0))= E[\\hat{f}(x_0)]-f(x_0)\n\\] To obtain the least test MSE, one must trade off between variance and bias. Less flexible model tendes to have higher bias, and more flexible models tend to have higher variance. An optimal flexibility for the least test MSE varies with different data sets. Non-linear data tends to require higher optimal flexibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#bayes-classifier",
    "href": "ch2.html#bayes-classifier",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.6 Bayes Classifier",
    "text": "2.6 Bayes Classifier\nIt can be shown that Bayes Classifier minimizes the classification test error \\[\n\\text{Ave}(I(y_0\\ne \\hat{y}_0)).\n\\] A Bayes Classifier assigns a test observation with predictor \\(x_0\\) to the class for which \\[\n\\text{Pr}(Y=j|X=x_0)\n\\] is largest. It’s error rate is given by \\[\n1-E[\\max_{j} \\text{Pr}(Y=j|X)]\n\\]\nwhere the expectation is over \\(X\\). The Bayes error is analogous to the irreducible error \\(\\epsilon\\).\nBayes Classifier is not attainable as we do not know \\(\\text{Pr}(Y|X)\\). We only can estimate \\(\\text{Pr}(Y|X)\\). One way to do this is by KNN. KNN estimate the conditional probability simply with a majority vote. The flexibility of KNN increases as \\(1/K\\) increases with \\(K=1\\) being the most flexible KNN. The training error is 0 for \\(K=1\\). A suitable \\(K\\) should be chosen for an appropriate trade off between bias and variance. The KNN classifier will classify the test point \\(x_0\\) based on the probability calculated from the \\(k\\) nearest points. KNN regression on the other hand will assign the test point \\(x_0\\) the average value of the \\(k\\) nearest neighbors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#homework-indicates-optional",
    "href": "ch2.html#homework-indicates-optional",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.7 Homework (* indicates optional):",
    "text": "2.7 Homework (* indicates optional):\n\nConceptual: 1,2,3,4*,5,6,7\nApplied: 8, 9*, 10*",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch2.html#code-gist",
    "href": "ch2.html#code-gist",
    "title": "2  Chapter 2: Statistical Learning",
    "section": "2.8 Code Gist",
    "text": "2.8 Code Gist\n\n2.8.1 OS\nimport os\nos.chdir(path) # change dir\n\n\n2.8.2 Python:\nConcatenation using +\n\"hello\" + \" \" + \"world\"  # 'hello world'\n[3,4,5] + [4,9,7] # [3,4,5, 4,9,7]\n\nString formatting using string.format()\nprint('Total is: {0}'.format(total))\n\nzip to loop over a sequence of tuples\nfor value, weight in zip([2,3,19],\n                         [0.2,0.3,0.5]):\n    total += weight * value\n\n\n\n2.8.3 Numpy\n\n2.8.3.1 Numpy functions:\nnp.sum(x), np.sqrt(x) (entry wise). x**2 (entry wise power), np.corrcoef(x,y) (find the correlation coefficient of array x and array y)\nnp.mean(axis=None): axis could be None (all entries), 0(along row), 1(along column)\nnp.var(x, ddof=0), np.std(x, ddof=0), # Note both np.var and np.std accepts an argument ddof, the divisor is N-ddof.\nnp.linspace(-np.pi, np.pi, 50) # start, end, number of points 50\nnp.multiply.outer(row,col) # calculate the product over the mesh with vectors row and col.\nnp.zeros(shape or int, dtype) #eg: np.zeros(5,bool)\nnp.ones(Boston.shape[0])\nnp.all(x), np.any(x): check if all or any entry of x is true.\nnp.unique(x): find unique values in x. np.isnan(x): return a boolean array of len(x). np.isnan(x).mean(): find the percentage of np.nan values in x.\n\n\n2.8.3.2 Array Slicing and indexing\nnp.arange(start, stop, step) # numpy version of range\nx[slice(3:6)] # equivalent to x[3:6]\nIndexing an array using [row, col] format. If col is missing, then index the entire rows. len(row) must be equal to len(col). Otherwise use iterative indexing or use np.ix_(x_idx, y_idx) function, or use Boolean indexing, see below.\nA[1,2]: index entry at row 1 and col 2 (recall Python index start from 0)\nA[[1,3]] # row 1 and 3. Note the outer [] is considered as the operator, so only row indices are provided. \nA[:,[0,2]] # cols 0 and 2\nA[[1,3], [0,2,3]] # entry A[1,0] and A[3,2]\nA[1:4:2, 0:3:2] # entries in rows 1 and 3, cols 0 and 2\nA[[1,3], [0,2,3]] # syntax error\n# instead one can use the following two methods \nA[[1,3]][:,[0,2]] # iterative subsetting\nA[np.ix_([1,3],[0,2,3])] # use .ix_ function to create an index mesh\nA[keep_rows, keep_cols] # keep_rows, keep_cols are boolean arrays of the same length of rows or cols, respectively\nA[np.ix_([1,3],keep_cols)] # np.ix_()can be applied to mixture of integer array and boolean array\n\n\n2.8.3.3 Random numbers and generators\nnp.random.normal(loc=0.0, scale=1.0,size=None) # size can be an integer or a tuple.\n# \nrng = np.random.default_rng(1303) # set random generator seed\nrng.normal(loc=0, scale=5, size=2) # \nrng.standard_normal(10) # standard normal distribution of size 10\nrng.choice([0, np.nan], p=[0.8,0.2], size=A.shape)\n\n\n2.8.3.4 Numpy array atributes\n.dtype, .ndim, .shape\n\n\n2.8.3.5 Numpy array methods\nx.sum(axis=None) (equivalent to np.sum(x)), x.T (transpose),\nx.reshape((2,3)) # x.reshape() is a reference to x.\nx.min(), x.max()\n\n\n\n2.8.4 Graphics\n\n2.8.4.1 2-D figure\n# Using the subplots + ax methods\nfig, ax = subplots(nrows=2, ncols=3, figsize=(8, 8)) \n# explicitly name each axis in the grid \nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n\nax[0,1].plot(x, y,marker='o', 'r--', linewidth=3); #line plot. `;` suppresses the text output. pick ax[0,1] when there are  multiple axes\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.scatter(x, y, marker='o'); #scatter plot\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.set_xlabel(\"this is the x-axis\")\nax.set_ylabel(\"this is the y-axis\")\nax.set_title(\"Plot of X vs Y\");\naxes[0,1].set_xlim([-1,1]) # set x_lim. similarly `set_ylim()`\n\nfig = ax.figure  # get the figure object from an axes object\nfig.set_size_inches(12,3) # access the fig object to change fig size (width, height)\nfig # re-render the figure\nfig.savefig(\"Figure.pdf\", dpi=200); #save a figure into pdf. Other formats: .jpg, .png, etc\n\n\n2.8.4.2 Contour and image\nfig, ax = subplots(figsize=(8, 8))\nx = np.linspace(-np.pi, np.pi, 50)\ny = x\nf = np.multiply.outer(np.cos(y), 1 / (1 + x**2))\nax.contour(x, y, f, levels=None); # numbre of levels. if None, automatically choose\nax.imshow(f); # heatmap colorcoded by f\n\n\n\n2.8.5 Pandas\n\n2.8.5.1 loading data\npd.read_csv('Auto.csv') # read csv\npd.read_csv('Auto.data', \n            na_values =['?'], #specifying the na_values in the datafile. \n            delim_whitespace=True) # read whitespaced text file\npd.read_csv('College.csv', index_col=0) # use column `0` as the row labels \n\n\n\n2.8.5.2 Pandas Dataframe attributes and methods\nAuto.shape\nAuto.columns # gets the list of column names\nAuto.index #return the index (labels) objects\nAuto['horsepower'].to_numpy() # convert to numpy array\nAuto['horsepower'].sum()\n\nAuto.dropna() # drop the rows containing na values. \ndf.drop('B', axis=1, inplace=True) # drop a column 'B' inplace. \n#equivalent to df.drop(columns=['B'], inplace=True)\ndf.drop(index=['Ohio','Colorado']) #eqivalent to: df.drop(['Ohio','Colorado'], axis=0)\nauto_df.drop(auto_df.index[10:86]) # drop rows with index[10:86] not including 86\n\nAuto.set_index('name')# rename the index using the column 'name'.\n\npd.Series(Auto.cylinders, dtype='category') # convert the column `cylinders` to 'category` dtype\n# the convertison can be done using `astype()` method\nAuto.cylinders.astype('category')\nAuto.describe() # statistics summary of all columns\nAuto['mpg'].describe() # for selected columns\n\ncollege.rename({'Unnamed: 0': 'College'}, axis=1): # change column name, \n# alternavie way\ncollege_df.rename(columns={college_df.columns[0] : \"College\"}, inplace=True) #\n\ncollege['Elite'] = pd.cut(college['Top10perc'],  # binning a column\n                          [0,0.5,1],  #bin edges\n                          labels=['No', 'Yes'],  # bin labels (names)\n                          right=True,# True: right-inclusive (default) for each bin ( ]; False:rigth-exclusive \n                          )   \ncollege['Elite'].value_counts() # frequency counts\nauto.columns.tolist() # equivalent to  auto.columns.format() (rarely used)\n\n\n\n2.8.5.3 Selecting rows and columns\nSelect Rows:\nAuto[:3] # the first 3 rows. \nAuto[Auto['year'] &gt; 80] # select rows with boolean array\nAuto_re.loc[['amc rebel sst', 'ford torino']] #label_based row selection\nAuto_re.iloc[[3,4]] #integer-based row seleciton: rows 3 and 4 (index starting from 0)\nSelect Columns\nAuto['horsepower'] # select the column 'horsepower', resulting a pd.Series.\nAuto[['horsepower']] #obtain a dataframe of the column 'horsepower'. \nAuto_re.iloc[:,[0,2,3]] # intger-based selection\nauto_df.select_dtypes(include=['int16','int32']) # select columns by dtype\nSelect a subset\nAuto_re.iloc[[3,4],[0,2,3]] # integer-based \nAuto_re.loc['ford galaxie 500', ['mpg', 'origin']] #label-based \nAuto_re.loc[Auto_re['year'] &gt; 80, ['weight', 'origin']] # mix bolean indexing with labels\n\nAuto_re.loc[lambda df: (df['year'] &gt; 80) & (df['mpg'] &gt; 30),\n            ['weight', 'origin']\n           ]  # using labmda function with loc[]\n\n\n2.8.5.4 Pandas graphics\nWithout using subplots to get axes and figure objects\nax = Auto.plot.scatter('horsepower', 'mpg') #scatter plot of 'horsepower' vs 'mpg' from the dataframe Auto\nax.set_title('Horsepower vs. MPG');\nfig = ax.figure\nfig.savefig('horsepower_mpg.png');\n\nplt.gcf().subplots_adjust(bottom=0.05, left=0.1, top=0.95, right=0.95) #in percentage of the figure size. \nax1.fig.suptitle('College Scatter Matrix', fontsize=35)\nUsing subplots\nfig, axes = subplots(  ncols=3, figsize=(15, 5))\nAuto.plot.scatter('horsepower', 'mpg', ax=axes[1]);\nAuto.hist('mpg', ax=ax);\nAuto.hist('mpg', color='red', bins=12, ax=ax); # more customized \nBoxplot using subplots\nAuto.cylinders = pd.Series(Auto.cylinders, dtype='category') # needs to convert the `cylinders` column to categorical dtype\nfig, ax = subplots(figsize=(8, 8))\nAuto.boxplot('mpg', by='cylinders', ax=ax);\nScatter matrix\npd.plotting.scatter_matrix(Auto); # all columns\npd.plotting.scatter_matrix(Auto[['mpg',\n                                 'displacement',\n                                 'weight']]);  # selected columns\n                                 \n                                 \n#Alternatively with sns.pairplot\n\n\nSns Graphic\n# Scatter matrix\nax1 = sns.pairplot(college_df[college_df.columns[0:11]])\n\n# Boxplot\nsns.boxplot(ax=ax, x=\"Private\", y=\"Outstate\", data=college_df)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 2: Statistical Learning</span>"
    ]
  },
  {
    "objectID": "ch3.html#simple-linear-regression",
    "href": "ch3.html#simple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.1 Simple Linear Regression",
    "text": "3.1 Simple Linear Regression\nAssumes the population regression line model \\[\nY = \\beta_0 + \\beta_1 X +\\epsilon,\n\\] where, \\(\\beta_0\\) is the expected value of \\(Y\\) when \\(X=0\\), and \\(\\beta_1\\) is the average change in \\(Y\\) with a one-unit increase in \\(X\\). \\(\\epsilon\\) is a “catch all” error term.\nAfter training using the training data, we can obtain the parameter estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). The we can obtain the prediction for \\(x\\) given by the least square line: \\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\\] The error at a data point \\(x_i\\) is given by \\(e_i = y_i -\\hat{y}_i\\), and the residual sum of squares (RSS) is \\[\n\\text{RSS} =e_1^2+\\cdots +e_n^2.\n\\] One can use the least square approach to minimize RSS to obtain \\[\n\\hat{\\beta}_1 =\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=r_{xy}\\frac{\\sigma_y}{\\sigma_x}\n\\] \\[\n\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\n\\] where, \\(\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i\\) and \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\), and the correlation \\[\nr_{xy} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}=\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}.\n\\tag{3.1}\\] is the normalized convariance. Note \\(-1\\le r_{xy} \\le 1\\). When there is no intercept, that is \\(\\beta_0=0\\), then \\[\n\\hat{y}_i=x_i \\hat{\\beta}=\\sum_{i=1}^n a_i y_i\n\\] where, \\[\n\\hat{\\beta} =\\frac{\\sum_{i=1}^n x_iy_i}{ \\sum_{i=1}^{n} x_i^2}\n\\] That is, the fitted values are linear combinations of the response values when there is no intercept.\n\n3.1.1 Assessing the accuracy of the coefficients\nLet \\(\\sigma^2=\\text{Var}(\\epsilon)\\), that is, \\(\\sigma^2\\) is the variance of \\(Y\\), (estimated by \\(\\sigma^2\\approx =\\text{RSE} =\\text{RSS}/(n-p-1)\\). ) Assume each observation have common variance (homoscedasticity) and are uncorrelated, then the standard errors under repeated sampling \\[\n(\\text{SE}[\\hat{\\beta}_1])^2 = \\frac{1}{\\sigma^2_x}\\cdot \\frac{\\sigma^2}{n}\n\\] \\[\n(\\text{SE}[\\hat{\\beta}_0])^2 = \\left[1+ \\frac{\\bar{x}^2}{\\sigma^2_x} \\right]\\cdot \\frac{\\sigma^2}{n}\n\\]\n\nwhen \\(x_i\\) are more spread out (with large \\(\\sigma_x^2\\)), then \\(\\text{SE}[\\hat{\\beta}_1]\\) is small. This is because there are more leverage (of \\(x\\) values) to estimate the slope.\nwhen \\(\\bar{x} =0\\) , then \\(\\text{SE}[\\hat{\\beta}_0] = \\text{SE}[\\bar{y}]\\). In this case, \\(\\hat{\\beta}_0 = \\bar{y}\\).\n\nStandard errors are used to construct CI and perform hypothesis test for the estimated \\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\). Under the assumption of Gaussian error, One can construct the CI of significance level \\(\\alpha\\) (e.g., \\(\\alpha=0.05\\)) as \\[\n\\hat{\\beta}_j = [\\hat{\\beta}_j- t_{1-\\alpha/2,n-p-1}\\cdot \\text{SE}[\\hat{\\beta}_j], \\hat{\\beta}_j+ t_{1-\\alpha/2,n-p-1} \\cdot \\text{SE}[\\hat{\\beta}_j]  ]\n\\] Where \\(j=0, 1\\). Large interval including zero indicates \\(\\beta_j\\) is not statistically significant from 0. When \\(n\\) is sufficient large, \\(t_{0.975,n-p-1} \\approx 2\\). With the standard errors of the coefficients, one can also perform hypothesis test on the coefficients. For \\(j=0,1\\),\n\\[H_0: \\beta_j=0\\] \\[H_A: \\beta_j\\ne 0\\] The \\(t\\)-statistic of degree \\(n-p-1\\), given by \\[\nt = \\frac{\\hat{\\beta}_j - 0}{\\text{SE}[\\hat{\\beta}_j]}\n\\] shows how far away \\(\\hat{\\beta}_j\\) is away from zero, normalized by its error \\(\\text{SE}[\\hat{\\beta}_j]\\). One can then compute the \\(p\\)-value corresponding to this \\(t\\) and test the hypothesis. Small \\(p\\)-value indicates strong relationship.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#multiple-linear-regression",
    "href": "ch3.html#multiple-linear-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.2 Multiple Linear Regression",
    "text": "3.2 Multiple Linear Regression\n\\[\nY= \\beta_0 + \\beta_1X_1 +\\cdots + \\beta_pX_p + \\epsilon.\n\\] The estimate of the coefficients \\(\\hat{\\beta_j}\\), \\(j\\in \\mathbb{Z}_{p+1}\\) are found by using the same least square method to minimize RSS. we interpret \\(\\beta_j\\) as the expected (average) effect on \\(Y\\) with one unit increase in \\(X_j\\), holding all other predictors fixed. This interpretation is based on the assumptions that the predictors are uncorrelated, so each predictor can be estimated and tested separately. When there are correlations among predictors, the variance of all coefficients tends to increase, sometimes dramatically, and the previous interpretation becomes hazardous because when \\(X_j\\) changes, everything else changes.\n\n3.2.1 Model Assumption\n\nlinearity: \\(Y\\) is linear in \\(X\\). The change in Y associated with one unit of change in \\(X_j\\) is constant, regardless of the value of \\(X_j\\). This can be examined visually by plotting the residual plot (\\(e_i\\) vs. \\(x_i\\) for \\(p=1\\) or \\(e_i\\) vs \\(\\hat{y}_i\\) for multiple regression). If the linear assumption is true, then the residual plot should not exhibit obvious pattern. If there is a nonlinear relationship suggested by by the residual plot, then a simple approach is to include transformed \\(X\\), such as \\(\\log X\\), \\(\\sqrt{X}\\), or \\(X^2\\).\nadditive: The association between \\(X_j\\) and \\(Y\\) is independent of other predictors.\nErrors \\(\\epsilon_i\\) are uncorrelated. This means \\(\\epsilon_i\\) provides no information for \\(\\epsilon_{i+1}\\). Otherwise (for example, frequently observed in a time series, where error terms are positively correlated, and tracking is observed in the residuals, i.e., adjacent error terms take similar values), the estimated standard error will tend to be underestimated, hence leading less confidence in the estimated model.\nHomoscedasticity: \\(\\text{Var}(\\epsilon_i) =\\sigma^2\\). The error terms have constant variance. If not (heteroscedasticity), one may use transformed \\(Y\\), such as \\(\\sqrt{Y}\\), or \\(\\log(Y)\\) to mitigate this; or use weighted least squares if it’s known that for example \\(\\sigma_i^2=\\sigma^2/n_i\\).\nNon-colinearity: two variables are colinear if they are highly correlated with each other. Co-linearity causes a great deal of uncertainty in the coefficient estimates, that is, reducing the accuracy of the coefficient estimates, thus cause the standard error of \\(\\beta_j\\) to grow, and hence smaller \\(t\\)-statistic. As a result, we may fail to reject \\(H_0: \\beta_j=0\\). This in turn means the power of Hypothesis test, the probability of correctly detecting a non-zero coefficient is reduced by colinearity. To detect colinearity,\n\nuse the correlation matrix of predictors. Large value of the matrix in absolute value indicates highly correlated variable pairs. But this approach cannnot detect multicolinearity.\nUse VIF (Variance inflation factor, VIF \\(\\ge 1\\)) to detect multicolinearity. It is possible for colinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This is the multicolinearity situation.\n\nVIF is the ratio of the variance of \\(\\hat{\\beta}_j\\) when fitting the full model divided by the variance of \\(\\hat{\\beta}_j\\) if fit on its own. It can be calculated by \\[\n  \\text{VIF}(\\hat{\\beta}_j) =\\frac{1}{1-R^2_{X_j|X_{-j}}}\n  \\] Where \\(R^2_{X_j|X_{-j}}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors. A VIF value exceeds 5 or 10 (i.e., \\(R^2_{X_j|X_{-j}}\\) close to 1) indicates colinearity.\nTo remedy a colinearity problem:\n\ndrop a redundant variable (variables with colinearity should have similar VIF values. )\nCombine the colinear variables into a single predictor, e.g., taking the average of the standardized versions of those variables.\n\n\nClaims of causality should be avoided for observational data.\n\n\n3.2.2 Assessing existence of linear relationship\n\ntest Hypothesis (test if there is a linear relationship between the response and predictors) \\[\nH_0: \\beta_1=\\beta_2=\\cdots = \\beta_p=0\n\\] \\[\nH_a: \\text{at least one } \\beta_j \\text{ is non-zero.}\n\\] using \\(F\\)-statistic \\[\nF=\\frac{\\text{SSB/df(B)}}{\\text{SSW/df(W)}}=\\frac{(\\text{TSS}-\\text{RSS})/p}{\\text{RSS}/(n-p-1)}\\sim F_{p,n-p-1}\n\\] If \\(H_0\\) is true, \\(F\\approx 1\\); if \\(H_a\\) is true, \\(F&gt;&gt;1\\). \\(F\\)-statistic adjust with \\(p\\). Note that one cannot conclude if an individual \\(t\\)-statistic is significant, then there is at least one predictor is related to the response, especially when \\(p\\) is large. This is related to multiple testing. The reason is that when \\(p\\) is large, there is \\(\\alpha\\) (eg 5%) chance that a predictor will have a small \\(p\\)-value by chance. When \\(p&gt;n\\), \\(F\\)-statistic cannot be used.\n\nIf the goal is to test that a particular subset of \\(q\\) of the coefficients are zero, that is, (for convenience, we put the \\(q\\) variables chosen at the end of the variabale list) \\[\nH_0: \\beta_{p-q+1} = \\beta_{p-q+2}=\\cdots = \\beta_p=0\n\\tag{3.2}\\] In this case, use \\[\nF = \\frac{(\\text{RSS}_0-\\text{RSS})/q}{\\text{RSS}/(n-p-1)}\\sim F_{q,n-p-1}\n\\] where, \\(\\text{RSS}_0\\) is the residual sum of squares of a second model that uses all variables except those last \\(q\\) variables. When \\(q=1\\), \\(F\\)-statistic in Equation 3.2 is the square of the \\(t\\)-statistic of that variable. The \\(t\\)-statistic reported in a regression model gives the partial effect of adding that variable, while holding other variables fixed.\n\n\n3.2.3 Assess the accuracy of the future prediciton\n\nconfidence interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from the population average \\(f(X)\\) because the coefficients \\(\\hat{\\beta}_{j}\\) are estimated, It quantifies reducible error around the predicted average response \\(\\hat{f}(X)\\), does-not include \\(\\epsilon\\).\nprediction interval: Indicate how far away \\(\\hat{Y}=\\hat{f}(X)\\) is from \\(Y\\). predict an individual response \\(Y\\approx \\hat{f}(X)+\\epsilon\\). Prediction interval is always wider than the confidence interval, because it includes irreducible error \\(\\epsilon\\).\n\n\n\n3.2.4 Assessing the overall accuracy of the model\n\nRSE. To this end, first define the lack of fit measure Residual Standard Error \\[\n\\text{RSE} = \\sqrt{\\frac{1}{n-p-1}\\text{RSS}} = \\sqrt{\\frac{1}{n-p-1}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2} \\approx \\sigma=\\sqrt{\\text{Var}(\\epsilon)}\n\\] It is the average amount in \\(\\hat{Y}\\) that a response deviates from the true regression line (\\(\\beta_0+\\beta_1 X\\)). Note, RSE can increase with more variables if the decrease of RSS doesnot offset the increase of \\(p\\).\nApproach 2: Using R-squared (fraction of variance in \\(Y\\) explained by \\(X\\)), which is independent of of the scale of \\(Y\\), and \\(0\\le R^2 \\le 1\\): \\[\nR^2 =\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}} = 1-\\frac{\\text{RSS}}{\\text{TSS}}\n\\] where, \\(\\text{TSS}=\\sum_{i=1}^n(y_i- \\bar{y})\\). When \\(R^2\\) is near 0 indicates that 1) either the linear model is wrong 2) or th error variance \\(\\sigma^2\\) is high, or both. \\(R^2\\) measures the linear relationship between \\(X\\) and \\(Y\\). If computed on the training set, when adding more variables, the RSS always decrease, hence \\(R^2\\) will always increase.\n\nFor simple linear regression, \\(R^2=r_{xy}^2\\), where the sample correlation measures the linear relationship between variables \\(X\\) and \\(Y\\). See the formula \\(r_{xy}\\) above Equation 3.1. For multiple linear regression, \\(R^2=(\\text{Cor}(Y, \\hat{Y}))^2\\). The fitted linear model maximizes this correlation among all possible linear models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "href": "ch3.html#model-selectionvariable-selections-balance-training-errors-with-model-size",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.3 Model Selection/Variable Selections: balance training errors with model size",
    "text": "3.3 Model Selection/Variable Selections: balance training errors with model size\n\nAll subsets (best subsets) regression: compute the least square fit for all \\(2^p\\) possible subsets and then choose among them based on certain criterion that balance training error and model size\nForward selection: Start from the null model that only contains \\(\\beta_0\\). Then find the best model containing one predictor that minimizing RSS. Denote the variable by \\(\\beta_1\\). Then continue to find the best model with the lowest RSS by adding one variable from the remaining predictors, and so on. Continue until some stopping rule is met: e.g., when all remaining variables have a \\(p\\)-value greater than some threshold.\nBackward selection: start with all variables in the model. Remove the variable with the largest \\(p\\)-value (least statistically significant). The new \\((p-1)\\) model is fit, and remove the variable with the largest \\(p\\)-value. Continue until a stopping rule is satisfied, e.g., all remaining variables have \\(p\\)-value less than some threshold.\nMixed selection: Start with forward selection. Since the \\(p\\)-value for variables can become larger as new predictors are added, at any point if the \\(p\\)-value of a variable in the model rises above a certain threshold, then remove that variable. Continue to perform these forward and backward steps until all variables in the model have a sufficiently low \\(p\\)-value, and all variables outside the model would have a large \\(p\\)-value if added to the model.\nBackward selection cannot be used if \\(p&gt;n\\). Forward selection can always be used, but might include variables early that later become redundant. Mixed selection can remedy this problem.\nothers (Chapter 6): including Mallow’s \\(C_p\\), AIC (Akaike Informaton Criterion), BIC, adjusted \\(R^2\\), Cross-validation, test set performance.\nnot valid: we could look at individual \\(p\\)-values, but when the number of variables \\(p\\) is large, we likely to make a false discoveries.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#handle-categorical-variables-factor-variables",
    "href": "ch3.html#handle-categorical-variables-factor-variables",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.4 Handle categorical variables (factor variables)",
    "text": "3.4 Handle categorical variables (factor variables)\nFor a categorical variable \\(X_i\\) with \\(m\\) levels, create one fewer dummy variables (\\(x_{ij}, 1\\le j \\le m-1\\))&gt;. The level with no dummy variable is called the baseline. The coefficient corresponding to a dummy variable is the expected difference in change in \\(Y\\) when compared to the baseline, while holding other predictors fixed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#adding-non-linearity",
    "href": "ch3.html#adding-non-linearity",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.5 Adding non-linearity",
    "text": "3.5 Adding non-linearity\n\n3.5.1 Modeling interactions (synergy)\nWhen two variables have interaction, then their product \\(X_iX_j\\) can be added into the regression model, and the product maybe considered as a single variable for inference, for example, compute its SE, \\(t\\)-statistics, \\(p\\)-value, Hypothesis test, etc.\nIf we include an interaction in a model, then the Hierarchy principle should be followed: always include the main effects, even if the \\(p\\)-values associated with their coefficients are not significant. This is because without the main effects, the interactions are hard to interpret, as they would also contain the main effect.\n\n\n3.5.2 Adding terms of transformed predictors\n\nPolynomial regression: Add a term involving \\(X_i^k\\) for some \\(k&gt;1\\).\nother forms: Adding root or logarithm terms of the predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "href": "ch3.html#outliers-unusual-y_i-that-is-far-from-haty_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))",
    "text": "3.6 Outliers (Unusual \\(y_i\\) that is far from \\(\\hat{y}_i\\))\nIt is typical for an outlier that does not have an unusual predictor value (with low levarage) to have little effect on the least squares fit, but it will increase RSE, hence deteriorate CI, \\(p\\)-value and \\(R^2\\), thus affecting interpreting the model.\nAn outlier can be identified by computing the \\[\\text{studentized residual}=\\frac{e_i}{\\text{RSE}_i}\\] A studentized residual great than 3 may be considered as an outlier.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#high-leverage-points-unusual-x_i",
    "href": "ch3.html#high-leverage-points-unusual-x_i",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.7 High leverage points (unusual \\(x_i\\))",
    "text": "3.7 High leverage points (unusual \\(x_i\\))\nHigh leverage points tend to have sizeable impact on the regression line. To quantify the observation’s leverage, one needs to compute the leverage statistic \\[h_i = \\frac{1}{n}+ \\frac{(x_i-\\bar{x})^2}{\\sum_{j=1}^n (x_j-\\bar{x})^2}.\\] \\(1/n \\le h_i\\le 1\\) and \\(\\text{Ave}(h_i)=(p+1)/n\\). A large value of this statistic (for example, great than \\((p+1)/n\\)) indicates an observation with high leverage. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#compared-to-knn-regression",
    "href": "ch3.html#compared-to-knn-regression",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.8 Compared to KNN Regression",
    "text": "3.8 Compared to KNN Regression\nKNN regression is a non-parametric method that makes prediction at \\(x_0\\) by taking the average in a \\(K\\)-point neightborhood \\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_{x_0}}{y_i}\n\\] A small value of \\(K\\) provides more flexible model with low bias but high variance while a larger value of \\(K\\) provides smoother fit with less variance. An optimal value of \\(K\\) depend on the bias-variance tradeoff. For non-linear data set, KNN may provides better fit than a linear regression model. However, in higher dimension (e.g., \\(p\\ge 4\\)), even for nonlinear data set, KNN may perform much inferior to linear regression, because of the curse of dimensionality, as the \\(K\\) observations that are nearest to \\(x_0\\) may in fact far away from \\(x_0\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#homework-indicates-optional",
    "href": "ch3.html#homework-indicates-optional",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.9 Homework (* indicates optional):",
    "text": "3.9 Homework (* indicates optional):\n\nConceptual: 1–6\nApplied: 8–15. at least one.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch3.html#code-gist",
    "href": "ch3.html#code-gist",
    "title": "3  Chapter 3: Linear Regression",
    "section": "3.10 Code Gist",
    "text": "3.10 Code Gist\n\n3.10.1 Python\ndir() # provides a list of objects at the top level name space\ndir(A) # display addtributes and methods for the object A\n' + '.join(X.columns) # form a string by joining the list of column names by \"+\"\n\n\n3.10.2 Numpy\nnp.argmax(x) # identify the location of the largest element\nnp.concatenate([x,y],axis=0) # concatenate two arrays x and y. \n\n\n\n3.10.3 Pandas\nX = pd.DataFrame(data=X, columns=['a','b'])\n\npd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']}) # make a dataframe using a dictionary\nBoston.columns.drop('medv','age') # drop the elements 'medv' and 'age' from the list of column names\n\npd.DataFrame({'vif':vals},\n                   index=X.columns[1:]) # form a df by specifying index labels\n\nX.values  # Convert dataframe X to numpy array\nX.to_numpy() # recommended to replace the above method\nDataFrame.corr(numeric_only=True) # correlations between columns \nx.sort_values(ascending=False)\npd.to_numeric(auto_df['horsepower'], errors='coerce') # if error, denote it by \"NaN\".\nauto_df.dropna(subset= ['horsepower', 'mpg',], inplace=True) # looking for NaN in the columns in `subset`, otherwise, all columns\n\nauto_df.drop('name', axis=1, inplace=True)\n\nleft2.join(right2, how=\"left\") #join two databases by index. \nleft1.join(right1, on=\"key\") # left-join by left1[\"key\"] and the index of right1. \npd.concat([s1, s4], axis=\"columns\", join=\"outer\")\n\n\n\n3.10.4 Graphics\nxlim = ax.get_xlim() # get the x_limit values xlim[0], xlim[1]\nax.axline() # add a line to a plot\nax.axhline(0, c='k', ls='--'); # horizontal line\nline, = ax.plot(x,y,label=\"line 1\") # \"line 1\" is the legend\n# alternatively the label can be set by \nline.set_label(\"line 1\")\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nax.legend(loc=\"upper left\", fontsize=25) # adding legendes\nax.annotate(i,xy=(fitted[i],residuals[i])) # annote at the xy position with i. \n\n\nplt.style.use('seaborn') # pretty matplotlib plots\nplt.rcParams.update({'font.size': 16})\nplt.rcParams[\"figure.figsize\"] = (8,7)\n\nplt.rc('font', size=10)\nplt.rc('figure', titlesize=13)\nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=13)\nplt.rc('legend', fontsize=8) # adjust legend globally\n    \n\n\n3.10.5 Using Sns\nsns.set(font_scale=1.25) # set font size 25% larger than default\nsns.heatmap(corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10})\nax = sns.regplot(x=x, y=y)\n\n\n3.10.6 Using Sklearn\nfrom sklearn.linear_model import LinearRegression\n## Set the target and predictors\nX = auto_df['horsepower']\n\n### To get polynomial features\npoly = PolynomialFeatures(interaction_only=True,include_bias = False)\nX = poly.fit_transform(X)\n\ny = auto_df['mpg']\n\n## Reshape the columns in the required dimensions for sklearn\nlength = X.values.shape[0]\nX = X.values.reshape(length, 1) #both X and y needs to be 2-D\ny = y.values.reshape(length, 1)\n\n## Initiate the linear regressor and fit it to data using sklearn\nregr = LinearRegression()\nregr.fit(X, y)\nregr.intercept_\nregr.coef_\n\npred_y = regr.predict(X)\n\n\n3.10.7 Using statsmodels and ISLP\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n                         \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\n#Training\nBoston = load_data(\"Boston\") \n#hand-craft the design matrix X\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]), #design matrix. intercept column\n                  'lstat': Boston['lstat']}) \n#the following is the preferred method to create X\ndesign = MS(['lstat']) # specifying the model variables. Automatically add an intercept, adding \"intercept=False\" if no intercept. \ndesign = design.fit(Boston) # do intial computation as specified in the model object design by MS(), such as means or sd. This attached some statistics to the `design` object, and need to be applied to the new data for prediciton\n\nX = design.transform(Boston) # apply the fitted transformation to the data to create X\n#alternatiely, \nX = design.fit_transform(Boston) # this combines the .fit() and .transform() two lines\n\ny = Boston['medv']\nmodel = sm.OLS(y, X) # setup the model\nmodel = smf.ols('mpg ~ horsepower', data=auto_df) # alternatively use smf formula, y~x\nsmf.ols(\"y ~ x -1\" , data=df).fit() # \"-1\" not inclding the intercept\nresults = model.fit() # results is a dictionary:.summary(), .params \n\nresults.summary()\nresults.params # coefficients\nresults.resid # reisdual array\nresults.rsquared # R^2\nresults.pvalues\nnp.sqrt(results.scale) # RSE\nresults.fittedvalues # fitted \\hat(y)_i at x_i in the traning set\n\n\nsummarize(results) # summzrize() is from ISLP to show the esstial results from model.fit()\n\n# Makding prediciton \nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})  # new test-set containing data where to make predicitons\nnewX = design.transform(new_df) # apply the same transform to the test-set\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean #predicted values\nnew_predictions.conf_int(alpha=0.05) #for the predicted values\n\nnew_predictions.conf_int(obs=True, alpha=0.05) # prediction intervals by setting obs=True\n\n# Including an interaction term\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston) #interaction term ('lstat', 'age')\n\n# Adding a polynomial term of higher degree\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston) # Note poly is from ISLP, # adding deg1 and deg2 terms. by default poly creates ortho. poly. not including an intercept. \n# Given a qualitative variable, `ModelSpec()` generates dummy\nvariables automatically, to avoid collinearity with an intercept, the first column is dropped in the design matrix generated by 'ModelSpec()` by default.\n\n# Compare nested models using ANOVA\nanova_lm(results1, results3) # result1 is the result of linear model, an result3 is the result of a larger model\n\n# Identify high leverage x\ninfl = results.get_influence() \n# hat_matrix_diag calculate the leverate statistics\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage\n\n# Calculate VIF\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])] #excluding column 0 because it's all 1's in X.\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif # VIF exceeds 5 or 10 indicates a problematic amount of colinearity\n\nUseful Code Snippets\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n# Plot scatter plot with a regression line\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n# Plot residuals vs. fitted values (note, not vs x, therefore works for multiple regression)\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n# Alternatively\nsns.residplot(x=X, y=y, lowess=True, color=\"g\", ax=ax)\n\n# Plot the smoothed residuals~fitted by LOWESS\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nsmoothed = lowess(residuals,fitted) # Note the order (y,x)\nax.plot(smoothed[:,0],smoothed[:,1],color = 'r')\n\n# QQ plot for the residuas (obtain studentized residuals for identifying outliers)\nimport scipy.stats as stats\nsorted_student_residuals = pd.Series(smf_model.get_influence().resid_studentized_internal)\nsorted_student_residuals.index = smf_model.resid.index\nsorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)\ndf = pd.DataFrame(sorted_student_residuals)\ndf.columns = ['sorted_student_residuals']\n\n#stats.probplot() #assess whether a dataset follows a specified distribution\ndf['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0] \n    \nx = df['theoretical_quantiles']\ny = df['sorted_student_residuals']\nax.scatter(x,y, edgecolor = 'k',facecolor = 'none')\n\n# Plot leverage statistics\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag) # identify the location of the largest levarage",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 3: Linear Regression</span>"
    ]
  },
  {
    "objectID": "ch4.html#linear-regression-and-classification",
    "href": "ch4.html#linear-regression-and-classification",
    "title": "4  Chapter 4: Classification",
    "section": "4.1 Linear regression and Classification",
    "text": "4.1 Linear regression and Classification\n\nFor a binary classification, one can use linear regression and does a good job. In this case, the linear regression classifier is equivalent to LDA, because \\[\nP(Y=1|X=x)= E[Y|X=x]\n\\] However, linear regression may not represent a probability as it may give a value outside the interval \\([0,1]\\).\nWhen there are more than two classes, linear regression is not appropriate, because any chosen coding of the \\(Y\\) variable imposes an ordering and fixed differences among categories, which may not be implied by the data set. If the coding changes, a dramatic function will be fitted, which is not reasonable. One should turn to multiclass logistic regression or Discriminant Analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#logistic-regression",
    "href": "ch4.html#logistic-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nLogistic regression is a discriminative learning, because it directly calculates the conditional probability \\(P(Y|X)\\) to make classification.\n\n4.2.1 Binary classification\nwith a single variable Logistic regression simply convert the linear regression to probability by \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\beta_0+\\beta_1 X}}{1+ e^{\\beta_0+\\beta_1X}}.\n\\] Note the logit or log odds is linear \\[\n\\log\\left( \\frac{p(X)}{1-p(X)}  \\right) =\\beta_0 +\\beta_1 X.\n\\] Increasing \\(X\\) by one unit, changes the log odds by \\(\\beta_1\\). Equivalently, it multiplied the odds by \\(e^{\\beta_1}\\). The rate of change of \\(p(X)\\) is no longer a constant, but depends on the current value of \\(X\\). Positive \\(\\beta_1\\) implies increasing \\(p(X)\\), and vice vesa.\nThe parameters are estimated by maximizing the liklihood \\[\n\\ell(\\beta_0, \\beta_1) =\\prod_{i: y_i=1}p(x_i) \\prod_{i:y_i=0}(1-p(x_i))\n\\] With the estimated parameters \\(\\hat{\\beta_j}, j=0,1\\), one can calculate the probability \\[\np(X)=Pr(Y=1|X) =\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1} X}}{1+ e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}\n\\]\n\n\n4.2.2 with multiple variables\nIn this case, simply let the logit be a linear function of \\(p\\) variables.\nNote when there are multiple variables, it’s possible to have variables confounding (especially when two variables are correlated): the coefficient of a variable may changes significantly or may change sign, this is because the coefficient represents the rate of change in \\(Y\\) of that variable when holding other variable constants. The coefficient reflects the effect when other variables are hold constant, how the variable affects \\(Y\\), and this effect may be different than when only this variable is used in the model.\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the logit model, similar to a linear regression that includes a non-linear term.\n\n\n\n\n4.2.3 Multi-class logistic regression (multinomial regression) with more than two classes\nin this case, we use the softmax function to model \\[\n\\text{Pr} (Y=k|X) =\\frac{e^{\\beta_{0k}+\\beta_{1k}X_1+ \\cdots + \\beta_{pk}X_p}}{\\sum_{\\ell=1}^K e^{\\beta_{0\\ell}+\\beta_{1\\ell}X_1+ \\cdots + \\beta_{p\\ell}X_p}} =a_k\n\\] for each class \\(k\\). Note \\(\\Sigma_k a_k=1\\) and the cross-entropy loss function is given by \\(-\\log \\ell(\\beta)= -\\Sigma_k \\mathbb{1}_k \\log a_k\\), where \\(\\beta\\) represents all the parameters.\nThe log odds between \\(k\\)th and \\(k'\\)th classes equals \\[\n\\log(\\frac{\\text{Pr}(Y=k|X=x)}{\\text{Pr}(Y=k'|X=x)})=(\\beta_{k0}-\\beta_{k'0}) + (\\beta_{k1}-\\beta_{k'1}) + \\cdots + (\\beta_{kp}-\\beta_{k'p})\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "href": "ch4.html#discriminant-classifier-approximating-optimal-bayes-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier",
    "text": "4.3 Discriminant Classifier: Approximating Optimal Bayes Classifier\nApply the Bayes Theorem, the model \\[\n\\text{Pr}(Y=k|X=x)=\\frac{\\text{Pr}(X=x|Y=k)\\cdot \\text{Pr}(Y=k)}{\\text{Pr}(X=x)}=\\frac{\\pi_k f_k(x)}{\\sum_{\\ell =}^ K \\pi_{\\ell}f_\\ell(x)}\n\\] where \\(\\pi_k=\\text{Pr(Y=k)}\\) is the marginal or prior probability for class \\(k\\), and \\(f_k(x)=\\text{Pr}(X=x|Y=k\\)) is the density for \\(X\\) in class \\(k\\). Note the denominator is a normalizing constant. So when making decisions, effectively we compare \\(\\pi_kf_k(x)\\), and assign \\(x\\) to a class \\(k\\) with the largest \\(\\pi_kf_k(x)\\).\nDiscriminant uses the full liklihood \\(P(X,Y)\\) to calculate \\(P(Y|X)\\) to make a classification, so it’s known as generative learning.\n\nwhen \\(f_k\\) is chosen as a normal distribution with constant variance (\\(\\sigma^2\\)) for \\(p=1\\) or correlation matrix \\(\\Sigma\\) for \\(p&gt;1\\), this leads to the LDA. For \\(p=1\\), the discriminant score is given by \\[\n\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log (\\pi_k)\n\\] when \\(K=2\\) and \\(\\pi_1=\\pi_2=0.5\\)m then the decision boundary is given by \\[\nx=\\frac{\\mu_1+\\mu_2}{2}.\n\\] When \\(p\\ge 2\\), assume that \\(X=(X_1, X_2, \\cdots, X_p)\\) is drawn from a multivariate Gaussian distribution \\(X \\sim N(\\mu_k, \\Sigma)\\), with a class-specific mean vector and a a common variance matrix. \\[\n\\delta_k(x) =x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k +\\log \\pi_k=c_{k0}+c_{k1}x_1+\\cdots +c_{kp}x_p.\n\\] The score function (posterior probability) is linear in \\(x\\). With \\(\\hat{\\delta}_k(x)\\) for each \\(k\\), it can be converted to the class probability by the softmax function \\[\n\\hat{\\text{Pr}}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{\\ell=1}^K e^{\\hat{\\delta}_{\\ell}(x)}}\n\\] The \\(\\pi_k\\), \\(\\mu_k\\) and \\(\\sigma\\) are estimate the follwing way: \\[\n\\hat{\\pi}_k =\\frac{n_k}{n}\n\\] \\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i:y_i=k} x_i\n\\] \\[\n\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2=\\sum_{k=1}^{K}\\frac{n_k-1}{n-K}\\hat{\\sigma}^2_k\n\\] where \\(\\hat{\\sigma}_k^2=\\frac{1}{n_k-1} \\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\) is the estimated variance for the \\(k\\)-th class.\n\n\n\n\n\n\n\nNote\n\n\n\nOne can include a nonlinear term such as a quadratic term in the LDA model, similar to a linear regression that includes a non-linear term.\n\n\n\nwhen each class chooses a different \\(\\Sigma_k\\), then it’s QDA. It assumes an observation from the \\(k\\)-th class is \\(X\\sim N(\\mu_k, \\Sigma_k)\\).The score function has a quadratic term \\[\n\\delta_k(x)=-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)+\\log \\pi_k -\\frac{1}{2}\\log |\\Sigma_k|\n\\] QDA has much more parameters \\(Kp(p+1)/2\\) to estimate compared to LDA (\\(Kp\\)), hence has higher flexibility and may lead to higher variance. When there are few training examples, LDA tend to perform better and reducing variance is crucial. When there is a large traning set, QDA is recommended as variance is not a major concern. LDA is a special case of QDA.\nwhen the features are modeled independently, i.e., there is no association between the \\(p\\) predictors, \\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\), the method is naive Bayes, and \\(\\Sigma_k\\) are diagonal. Any classifier with a linear decision boundary is a special case of NB. So LDA is a special case of NB. To estimate \\(f_kj\\), one can\n\nassume that \\(X_j|Y=k \\sim N(\\mu_{jk,\\sigma^2_{jk}})\\), that is, a class specific covariance but is diagonal. QDA’s \\(\\Sigma_k\\) is not diagonal. If we model \\(f_{kj}(x_j)\\sim N(\\mu_{kj}+\\sigma_j^2)\\) (Note \\(\\sigma_j^2\\) is shared among clases), In this case NB is a special case of LDA that has a diagonal \\(\\Sigma\\) and\nuse a non-parametric estimate such as histogram (or a smooth kernel density estimator) for the observations of the jth Predictor within each class.\nIf \\(X_j\\) is qualitative, then one can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\nCan applied to mixed feature vectors (qualitative and quantitative). NB does not assume normally distributed predictors.\nDespite strong assumptions, performs well, especially when \\(n\\) is not large enough relative to \\(p\\), when estimating the joint distribution is difficult. It introduces some biases but reduces variance, leading to a classifier that works quite well as a result of bias-variance trade-off.\nUseful when \\(p\\) is very large.\nNB is a generalized additive model.\nNeigher NB nor QDA is a special case of the other. Because QDA contains interaction term \\(x_ix_j\\), while NB is purely additive, in the sense that a function of \\(x_i\\) is added to a function of \\(x_j\\). Therefore, QDA potentially is a better fit when the interactions among predictors are important.\n\n\n\n4.3.1 Why discriminant analysis\n\nWhen the classes are well-separated, the parameter estimation of logistic regression is unstable, while LDA does not suffer from this problem.\nif the data size \\(n\\) is small and the distribution of \\(X\\) is approximately normal in each of the classes, then LDA is more stable than logistic regression. Also used when \\(K&gt;2\\).\nwhen there are more than two classes, LDA provides low-dimensional views of the data hence popular. Specifically, when there are \\(K\\) classes, LDA can be viewed exactly in \\(K-1\\) dimensional plot. This is because it essentially classifies to the closest centroid, and they span a \\(K-1\\) dimensional plane.\nFor a two-class problem, the logit of \\(p(Y=1|X=x\\)) by LDA (generative learning) is a linear function in \\(X\\), the same as a logistic regression (discriminative learning). The difference lies in how the parameters are estimated. But in practice, they are similar.\nLDA assumes the predictors follow a multivariable normal distribution with a shared \\(\\Sigma\\) among classes. So when this assumption holds, we expect LDA performs better; and Logistic regress performs better when this asuumption does not hold.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#knn",
    "href": "ch4.html#knn",
    "title": "4  Chapter 4: Classification",
    "section": "4.4 KNN",
    "text": "4.4 KNN\nKNN is a non-parametric method and doesnot assume a shape for the decision boundary. KNN assign the class of popularity to \\(X=x\\) in a \\(K\\)-neighborhood.\n\nKNN dominates LDA and Logistic Regression when the decision boundary is highly non-linear, provided \\(n\\) is large and \\(p\\) is small. As KNN breaks down when \\(p\\) is large.\nKNN requires large \\(n&gt;&gt;p\\), this is because KNN is non-parametric and tends to reduce bias but increase variance.\nWhen the decision boundary is non-linear but \\(n\\) is only modest and \\(p\\) is not very small, QDA may outperform KNN. This is because QDA provides a non-linear boundary while taking advantage of a parametric form, which means that if requires smaller size for accurate classification.\nUnlike logistic regression, KNN does not tell which predictors are more importnat: We dont get a table of coefficients.\nWhen the decision boundary is linear, LDA or logistic regression may perform better, when the boundary is moderately non-linear, QDA or NB may perform better; For a much more complicated decision boundary, KNN may perform better.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#poisson-regression",
    "href": "ch4.html#poisson-regression",
    "title": "4  Chapter 4: Classification",
    "section": "4.5 Poisson Regression",
    "text": "4.5 Poisson Regression\nWhen \\(Y\\) is discrete and non-negative, a linear regression model is not satisfactory, even with the transformation of \\(\\log (Y)\\), because \\(\\log\\) does not allow \\(Y=0\\).\n\nPoisson Regression: typically used to model counts, \\[\n\\text{Pr}(Y=k)= \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\qquad k=0,1,2, \\cdots,\n\\] where, \\(\\lambda = E(Y)= \\text{Var}(Y)\\). This means that if \\(Y\\) follows a Poissson distribution, the larger the mean of \\(Y\\), the larger its variance. Posisson regression can handle this when variance changes with mean, but linear regression cannot, because it assumes contant variance.\n\nAssume \\[\n\\log(\\lambda(X_1, X_2, \\cdots, X_p))=\\beta_0+\\beta_1X_1+\\cdots +\\beta_pX_p\n\\] Then one can use maximum likelihood \\[\n\\ell(\\beta_0, \\beta_1, \\cdots, \\beta_p)=\\prod_{i=1}^n \\frac{e^{-\\lambda(x_i)}\\lambda(x_i)^{y_i}}{y_i!}\n\\] to estimate the parameters.\n\nInterpretation: An increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y)=\\lambda\\) by a factor (percentage) of \\(\\exp(\\beta_j)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#generalized-linear-models-glm",
    "href": "ch4.html#generalized-linear-models-glm",
    "title": "4  Chapter 4: Classification",
    "section": "4.6 Generalized Linear Models (GLM)",
    "text": "4.6 Generalized Linear Models (GLM)\nPerform a regression by modeling \\(Y\\) from a particular member of the exponential family (Gaussian, Bernoulli, Poisson, Gamma, negative binomial), and then transform the mean of \\(Y\\) to a linear function.\n\nUse predictors \\(X_1, \\cdots, X_p\\) to predict \\(Y\\). Assume \\(Y\\) conditional on \\(X\\) follow some distribution: For linear regression, assume \\(Y\\) follows a normal distribution; for logistic regression, assume \\(Y\\) follows a Bernoulli (multinomial distribution for multi-class logistic regression) distribution; For poisson distribution, assume \\(Y\\) follows a poisson distribution.\nEach approach models the mean of \\(Y\\) as a function of \\(X\\) using a linking function \\(\\eta\\) to transform \\(E[Y|X]\\) to a linear function.\n\nfor linear regression \\[\nE(Y|X)= \\beta_0+\\beta_1 X_1+\\cdots +\\beta_p X_p\n\\] \\(\\eta(\\mu) =\\mu\\)\nfor logistic regression \\[\nE(Y|X)=P(Y=1|X)=\\frac{e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}}\n\\] \\(\\eta(\\mu) = \\log (\\mu/(1-\\mu))\\)\nfor Poisson regression \\[\nE(Y|X) = \\lambda(X) = e^{\\beta_0+\\beta_1X_1+\\cdots+\\beta_pX_p}\n\\] \\(\\eta(\\mu) = \\log(\\mu)\\).\nGamma regression and negative binomial regression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#assessment-of-a-classifier",
    "href": "ch4.html#assessment-of-a-classifier",
    "title": "4  Chapter 4: Classification",
    "section": "4.7 Assessment of a classifier",
    "text": "4.7 Assessment of a classifier\n\nConfusion matrix\nOverall error rate: equals to \\[\n\\frac{FP+FN}{N+P}\n\\]\nClass-specific performance: One can adjust the decision boundary (posterior probability threshold) to improve class specific performance at the expense of lowered overall performance.\n\npercentage of TP detected among all positives\n\\[\\text{sensitivity (recall, power)} = TPR = \\frac{TP}{TP+FN}=\\frac{TP}{P}= 1-\\text{Type II error}=1-\\beta\\] this is equal to \\(1- FNR\\), where, FNR is The fraction of positive examples that are classified as negatives \\[\nFNR = \\frac{FN}{FN+TP}=\\frac{FN}{P}\n\\]\npercentage of TN detected among all negatives \\[\\text{specificity}= TNR = \\frac{TN}{TN+FP}=\\frac{TN}{N}\\] This is equal to \\(1-FPR\\), where, False positive rate (FPR): the fraction of negative examples (N) that are classified as positive: \\[\nFPR=\\frac{FP}{FP+TN}=\\frac{FP}{N} = \\text{Type I error} (\\alpha)\n\\]\nROC (receiver operating characteristic curve): plot true positive rate (TPR=1-Type II error) ~ false positive rate (FPR= 1- specificity=Type I error) as a threshold for the posterior probability of positive class changes from 0 to 1. The point on the ROC curve closest to the point (0,1) corresponds to the best classifier.\nAUC (area under the ROC): Overall performance of a classified summarized over all thresholds. AUC measures the probability a random positive example is ranked higher than a random negative example. A larger AUC indicates a better classifier.\n\nclass-specific prediction performance\n\n\\[\\text{precision} = \\frac{TP}{TP+FP}=\\frac{TP}{\\text{predicted postives}}=1-\\text{false discovery proportion}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#homework",
    "href": "ch4.html#homework",
    "title": "4  Chapter 4: Classification",
    "section": "4.8 Homework:",
    "text": "4.8 Homework:\n\nConceptual: 1,2,3,4, 5,6,7,8, 9, 10, 12\nApplied: 13, 14*,15*,16*",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch4.html#code-gist",
    "href": "ch4.html#code-gist",
    "title": "4  Chapter 4: Classification",
    "section": "4.9 Code Gist",
    "text": "4.9 Code Gist\n\n4.9.1 Python\n\n\n4.9.2 Numpy\nnp.where(lda_prob[:,1] &gt;= 0.5, 'Up','Down')\nnp.argmax(lda_prob, 1) #argmax along axis=1 (col)\nnp.asarray(feature_std) # convert to np array\nnp.allclose(M_lm.fittedvalues, M2_lm.fittedvalues) \n#check if corresponding elts are equal within rtol=1e-5 and atol=-1e08\n\n\n4.9.3 Pandas\nSmarket.corr(numeric_only=True)\ntrain = (Smarket.Year &lt; 2005)\nSmarket_train = Smarket.loc[train] # equivalent to Smarket[train]\nPurchase.value_counts() # frequency table\nfeature_std.std() #calculate column std\nS2.index.str.contains('mnth')\nBike['mnth'].dtype.categories # get the categories of the categorical data\nobj2 = obj.reindex([\"a\", \"b\", \"c\", \"d\", \"e\"])# rearrange the entries in obj according to the new index, introducing missing values if any index values were not already present. \n\n\n4.9.4 Graphics\nax_month.set_xticks(x_month) # set_xticks at the place given by x_month\nax_month.set_xticklabels([l[5] for l in coef_month.index], fontsize=20)\nax.axline([0,0], c='black', linewidth=3,  \n          linestyle='--', slope=1);#axline method draw a line passing a given point with a given slope. \n\n\n4.9.5 ISLP and Statsmodels\nfrom ISLP import confusion_table\nfrom ISLP.models import contrast\n\n# Logistic Regression using sm.GLM() syntax similar to sm.OLS()\ndesign = MS(allvars)\nX = design.fit_transform(Smarket)\ny = Smarket.Direction == 'Up'\nglm = sm.GLM(y,\n             X,\n             family=sm.families.Binomial())\nresults = glm.fit()\nsummarize(results)\nresults.pvalues\nprobs = results.predict() #without data set, calculate predictions on the training set. \nresults.predict(exog=X_test) # on test set\n# Prediction on a new dataset\nnewdata = pd.DataFrame({'Lag1':[1.2, 1.5],\n                        'Lag2':[1.1, -0.8]});\nnewX = model.transform(newdata)\nresults.predict(newX)\nconfusion_table(labels, Smarket.Direction) #(predicted_labels, true_labels)\nnp.mean(labels == Smarket.Direction) # calculate the accuracy\n\nhr_encode = contrast('hr', 'sum') #coding scheme for categorical data: the unreported coefficient for the missing level equals to the negative ofthe sum of the coefficients of all other variables. In this a coefficient for a level may be interpreted as the differnece from the mean level of response. \n\n#Poisson Regression \nM_pois = sm.GLM(Y, X2, family=sm.families.Poisson()).fit()\n#`family=sm.families.Gamma()` fits a Gamma regression\nmodel.\n\n\n\n4.9.6 sklearn\nfrom sklearn.discriminant_analysis import \\\n     (LinearDiscriminantAnalysis as LDA,\n      QuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n#LDA\nlda = LDA(store_covariance=True) #store the covariance of each class\nX_train, X_test = [M.drop(columns=['intercept']) # drop the intercept column\n                   for M in [X_train, X_test]]\nlda.fit(X_train, L_train) # LDA() model will automatically add a intercept term\n\nlda.means_ # mu_k (n_classes, n_features)\nlda.classes_\nlda.priors_ # prior probability of each class\n#Linear discrimnant vectors\nlda.scalings_ #Scaling of the features in the space spanned by the class centroids. Only available for ‘svd’ and ‘eigen’ solvers.\n\nlda_pred = lda.predict(X_test) #predict class labels\nlda_prob = lda.predict_proba(X_test) #ndarray of shape (n_samples, n_classes)\n\n#QDA\nqda = QDA(store_covariance=True)\nqda.fit(X_train, L_train)\nqda.covariance_[0] #estimated covariance for the first class\n\n# Naive Bayes\nNB = GaussianNB()\nNB.fit(X_train, L_train)\nNB.class_prior_\nNB.theta_ #means for (#classes, #features)\nNB.var_ #variances (#classes, #features)\nNB.predict_proba(X_test)[:5]\n\n# KNN\nknn1 = KNeighborsClassifier(n_neighbors=1)\nX_train, X_test = [np.asarray(X) for X in [X_train, X_test]]\nknn1.fit(X_train, L_train)\nknn1_pred = knn1.predict(X_test)\n\n# When using KNN one should standarize each varaibles\nscaler = StandardScaler(with_mean=True,\n                        with_std=True,\n                        copy=True) # do calculaton on a copy of the dataset\nscaler.fit(feature_df)\n\n#train test split\nX_std = scaler.transform(feature_df)\n(X_train,\n X_test,\n y_train,\n y_test) = train_test_split(np.asarray(feature_std),\n                            Purchase,\n                            test_size=1000,\n                            random_state=0)\n\n# Logistic Regression\nlogit = LogisticRegression(C=1e10, solver='liblinear') #use solver='liblinear'to avoid warning that the alg doesnot converge.  \nlogit.fit(X_train, y_train)\nlogit_pred = logit.predict_proba(X_test)\n\n\n\n\n4.9.7 Useful code snippet\n# Tuning KNN\nfor K in range(1,6):\n    knn = KNeighborsClassifier(n_neighbors=K)\n    knn_pred = knn.fit(X_train, y_train).predict(X_test)\n    C = confusion_table(knn_pred, y_test)\n    templ = ('K={0:d}: # predicted to rent: {1:&gt;2},' +  # &gt; for right alighment\n            '  # who did rent {2:d}, accuracy {3:.1%}')\n    pred = C.loc['Yes'].sum()\n    did_rent = C.loc['Yes','Yes']\n    print(templ.format(\n          K,\n          pred,\n          did_rent,\n          did_rent / pred))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 4: Classification</span>"
    ]
  },
  {
    "objectID": "ch5.html#how-to-estimate-test-error",
    "href": "ch5.html#how-to-estimate-test-error",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.1 how to estimate test error",
    "text": "5.1 how to estimate test error\n\nuse a large designated test set, but often not available.\nmake adjustment to the training error to estimate the test error, e.g., Cp statistic, AIC and BIC.\nvalidation set approach: estimate the test error by holding out a subset of the training set, also called a validation set.\n\nthe estimate of the test error can be highly variable, depending on the random train-validation split.\nOnly a subset of the training set is used to fit the model. Since statistical methods tend to perform worse when trained on a smaler data set, which suggests the validation error tends to overestimate the test error compared to the model that uses the entire training set.\n\nK-fold Cross-Validation: randomly divide the data into \\(K\\) equal-sized parts \\(C_1, C_2, \\cdots, C_K\\). For each \\(k\\), leave out part \\(k\\), fit the model on the remaining \\(K-1\\) parts (combined, \\((K-1)/K\\) of the original traning set), and then evaluate the model on the part \\(k\\). Then repeat this for each \\(k\\), and weighted average of the errors is computed: \\[\nCV_{(K)} = \\sum_{k=1}^K \\frac{n_k}{n}\\text{MSE}_k\n\\] where \\(\\text{MSE}_k=\\sum_{i\\in C_k}(y_i\\ne \\hat{y}_i)/n_k\\).\n\nFor classification problem, simply replace \\(\\text{MSE}_k\\) with the misclassificaiton rate \\(\\text{Err}_k =\\sum_{i\\in C_k}I(y_i\\ne \\hat{y}_i)/n_k\\).\nThe estimated standard error of \\(CV_k\\) can be calculated by \\[\n\\hat{\\text{SE}}(CV_k)=\\sqrt{\\frac{1}{K}\\sum_{k=1}^K\\frac{(\\text{Err}_k-\\overline{\\text{Err}_k})^2}{K-1}}\n\\]\nThe estimated error tends bias upward because it uses only \\((K-1)/K\\) of the training set. This bias is minimized with \\(K=n\\) (LOOCV), but LOOCV estimate has high variance due to the high correlation between folds.\n\nLOOCV: it’s a special case of K-fold CV with \\(K=n\\). For least squares linear or polynomial regression, the LOOCV error can be computed by \\[\n\\text{CV}_{(n)}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{y_i-\\hat{y}_i}{1-h_i} \\right)^2\n\\] Where \\(h_i\\) is the leverage statistic of \\(x_i\\). There is no randomness in the error. The leverage \\(1/n\\le h_i\\le 1\\), reflects the amount an observation influences its own fit. The above formula doesn’t hold in genearl, in which case the model has to refit \\(n\\) times to estimate the test error.\nfor LOOCV, the estimate from each fold are highly correlated, hence their average can have high variance.\nbetter choice is \\(K=5\\) or \\(K=10\\) for bias-variance trade-off, because large \\(k\\) leads to low bias but high variance due to the increased correlation between models. Despite the estimated test error sometimes underestimate the true test error, they then to be close to identify the correct flexibility where the test error is minimum.\nBootstrap: Primarily used to estimate the standard error, or a CI (called bootstrap percentile) of an estimate . Repeatedly sampling the training set with replacement and obtain a bootstrap set of the the same size as the original training set. One can fit a model and estimate a parameter with each bootstrap data set, and then estimate the standard error using the estimated parameters by the bootstrap model, assuming there are \\(B\\) bootstrap data sets: \\[\nSE_B(\\hat{\\alpha})=\\sqrt{\\frac{1}{B-1}\\sum_{r=1}^B (\\hat{\\alpha}^{*r}-\\bar{\\hat{\\alpha}}^*)^2   }\n\\]\n\nNote sometimes sampling with replacement must take caution, for example, one can’t simply sample a time series with replacement because the data are sequential.\nEstimate prediction error: Each bootstrap sample has significant overlap with the original data, in fact, about 2/3 of the original data points appear in each bootstrap sample. If we use the original data set as the validation set, This will cause the bootstrap to seriously underestimate the true prediction error. To fix this, one can only use predictions on those samples that do not occur (by chance) in a bootstrap sample.\nBootstrap vs. Permutation test: permutation methods sample from an estimated null distribution for the data, and use this to estimate \\(p\\)-values and False Discovery Rates for hypothesis tests.\nThe bootstrap can be used to test a null hypothesis in simple situation. Eg. If \\(H_0: \\theta=0\\), we can check whether the confidence interval for \\(\\theta\\) contains zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#homework",
    "href": "ch5.html#homework",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.2 Homework",
    "text": "5.2 Homework\n\nConceptual: 1,2,3,4\nApplied: 5–9, at least one.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch5.html#code-gist",
    "href": "ch5.html#code-gist",
    "title": "5  Chapter 5: Resampling Methods",
    "section": "5.3 Code Gist",
    "text": "5.3 Code Gist\n\n5.3.1 Python\nnp.empty(1000) #create an array without initializing\nquartiles = np.percentile(arr, [25, 50, 75])\n\n\n5.3.2 Numpy\nc = np.power.outer(row, col) # mesh of row[i]^col[j] power. \n# random choice \nrng = np.random.default_rng(0)\nalpha_func(Portfolio,\n           rng.choice(100, # random numbers are selected from arange(100)\n                      100, #size\n                      replace=True))\n\n    \n\n\n5.3.3 Pandas\nnp.cov(D[['X','Y']].loc[idx], rowvar=False) #cov compute corr of variables. rowvar-False: cols are vars.\n\n\n5.3.4 Graphics\n\n\n5.3.5 ISLP and statsmodels\n# function that evalues MSE for training a model\ndef evalMSE(terms,    #predictor variables\n            response, #response variable\n            train,\n            test):\n\n   mm = MS(terms)\n   X_train = mm.fit_transform(train)\n   y_train = train[response]\n\n   X_test = mm.transform(test)\n   y_test = test[response]\n\n   results = sm.OLS(y_train, X_train).fit()\n   test_pred = results.predict(X_test)\n\n   return np.mean((y_test - test_pred)**2)\n\n# Compare polynomial models of different degrees\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)],\n                       'mpg',\n                       Auto_train,\n                       Auto_valid)\nMSE\n\n# Estimating the accuracy of a LR model using bootstrap\n\n# Compute the SE of the boostraped values computed by func                      \ndef boot_SE(func,\n            D,\n            n=None,\n            B=1000,\n            seed=0):\n    rng = np.random.default_rng(seed)\n    first_, second_ = 0, 0\n    n = n or D.shape[0]\n    for _ in range(B):\n        idx = rng.choice(D.index,\n                         n,\n                         replace=True)\n        value = func(D, idx)\n        first_ += value\n        second_ += value**2\n    return np.sqrt(second_ / B - (first_ / B)**2) #compute var. \ndef boot_OLS(model_matrix, response, D, idx):\n    D_ = D.loc[idx]\n    Y_ = D_[response]\n    X_ = clone(model_matrix).fit_transform(D_) #clone create a deep copy. \n    return sm.OLS(Y_, X_).fit().params\n    \nquad_model = MS([poly('horsepower', 2, raw=True)]) #raw=True: not normalize the feature\nquad_func = partial(boot_OLS,\n                    quad_model,\n                    'mpg')\nboot_SE(quad_func, Auto, B=1000)\n\n\n\n5.3.6 sklearn\nfrom functools import partial\nfrom sklearn.model_selection import \\\n     (cross_validate,\n      KFold,\n      ShuffleSplit)\nfrom sklearn.base import clone\nfrom ISLP.models import sklearn_sm #wrapper to feed a sm model to sklearn\n\n#Cross Validation\nhp_model = sklearn_sm(sm.OLS,\n                      MS(['horsepower']))\nX, Y = Auto.drop(columns=['mpg']), Auto['mpg']\ncv_results = cross_validate(hp_model,\n                            X,\n                            Y,\n                            cv=Auto.shape[0]) #cv=K.loocv. Can use cv=KFold()object\ncv_err = np.mean(cv_results['test_score']) # test_score: MSE\ncv_err\n\n# Use KFold to partition instead of using an integer. \ncv_error = np.zeros(5)\ncv = KFold(n_splits=10,\n           shuffle=True,#shuffle before splitting\n           random_state=0) # use same splits for each degree\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M,\n                          X,\n                          Y,\n                          cv=cv)\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\n\n# using ShuffleSplit() method \nvalidation = ShuffleSplit(n_splits=10,\n                          test_size=196,\n                          random_state=0)\nresults = cross_validate(hp_model,\n                         Auto.drop(['mpg'], axis=1),\n                         Auto['mpg'],\n                         cv=validation)\nresults['test_score'].mean(), results['test_score'].std()\n\n\n#View skleanrn fitting results using model.results_\nhp_model.fit(Auto, Auto['mpg']) # hp_model is a sklearn model sk_model.fit(X, Y) for trainning\nmodel_se = summarize(hp_model.results_)['std err'] #summarize is an ISLP function\nmodel_se\n\n\n\n5.3.7 Useful code snippet",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5: Resampling Methods</span>"
    ]
  },
  {
    "objectID": "ch6.html#code-snippet",
    "href": "ch6.html#code-snippet",
    "title": "6  Chapter 6: Linear Model Selection and Regrularization",
    "section": "6.1 Code Snippet",
    "text": "6.1 Code Snippet\n\n6.1.1 Python\n\n\n6.1.2 Numpy\n\n\n6.1.3 Pandas\n\n\n6.1.4 Graphics\n\n\n6.1.5 ISLP and statsmodels\n\n\n6.1.6 sklearn",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 6: Linear Model Selection and Regrularization</span>"
    ]
  },
  {
    "objectID": "class_project.html",
    "href": "class_project.html",
    "title": "7  Class Project",
    "section": "",
    "text": "** Goal**\nto use various ML algorithms to predict a meaningful target \\(Y\\) by classification algorithms or regression algorithms. Present it at COS Research Sympsium in the end of April.\nData set: real-world stock price and volume data. We could start with just one stock, e.g. APAL. S&P 500 index, DJ index.\nsome ideas: * Create one model for each stock. * create a single model for all stockes. Needs to embed each stock in a feature space. Research?\nStartup code: refer to the page https://github.com/ywanglab/Predicting_stock_movement/blob/main/Time_series_stock_data_analysis_ver2.ipynb. Perform some EDA to feel the data.\nreference ticker symbols: https://gist.github.com/quantra-go-algo/ac5180bf164a7894f70969fa563627b2\nQuestions: Which are the \\(X\\) variables? price, volume, return, day of week, month of year, etc. What is the \\(Y\\) variable? next-day price, next-day return, next-five-day average price, next-five-day-average return, etc.\nModels\n\nLinear regression:\n\nincluding continuous variables (price, volume), categorical variables (day-of-week, month-of-year)\ntransforming \\(X\\) (for including non-linear relation between \\(Y\\) and \\(X\\)) or \\(Y\\) (when \\(Y\\) is heteroschedatic, i.e., with varied \\(\\epsilon_i\\))\nplot residual plot to see if \\(Var(\\epsilon_i)\\) is changing. If yes, may appeal to transforming \\(Y\\), e.g., \\(\\log Y\\), \\(\\sqrt{Y}\\).\ninvestigate outliers (points with unusual \\(Y\\)-values) using the residual plot or looking at studentized residual.\ninvestigate high leverage points (with unusual \\(x\\) values), by calculating leverage statistics.\nInvestigate if there is colinearity among the variables by calculating VIF.\n\nclassification: predicting directions of the stock price movement. binary (with two direction), or multinomial (more than two values: e.g., up, same, down), LDA, QDA\nregularization of the parameters\nselection of variables: forward, backward, mixture, regularization, cross-validation\ndecision tree: random forest, boosting\nSVM: support vector machines\nNN",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Class Project</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "James, G., D. Witten, T. Hastie, R. Tibshirani, and J. Taylor. 2023.\nAn Introduction to Statistical Learning. USA: Springer. https://hastie.su.domains/ISLP/ISLP_website.pdf.",
    "crumbs": [
      "References"
    ]
  }
]