# Chapter 7: Moving Beyond Linearity

Often the linearity assumption of Y of X is good but it may have limited predictive power, as often a linear model is just an approximation. Imporovement can be obtained by reducing the model complexity (hence variance) with lasso, ridge, PCAR or PLR, etc.  when it's not, 
- Polynomials
- step functions
- splines
- local regression, 
- generalized additive models
offer much more flexibility. All the above nonlinear methods falls into the *basis function* approach where we fit the following function 
$$
y_i=\beta_0+\beta_1b_1(x_i) +\cdots + \beta_K b_K(x_i) +\epsilon_i
$$
where the $b_j$, $j=1, \cdots, K$ are pre-defined basis functions that transform $X$ to a feature $b_j(X)$. Each approach corresponding to a choice of particular family of basis functions. The model can then fit with *OLS*. 

## Polynomials
The basis functions are simply the polynomial functions of different degrees. 
Polynomial terms of higher powers for $X_j$ or interaction terms $X_iX_j$ are used, but the model is still a linear model in the coefficients $\beta_j$. The optimum degree $d$ can be chosen by cross-validation. polynomial terms can be included in either a linear regression model or  a logistic regression model. 

In practice hardly a degree greater than 3 or 4 is used because a higher degree polynomial exhibits high degree of oscillation, especially near the boundary (Runge's phenomenon). This is because a polynomial imposes a *global structure*. 

The standard error at a point $x_0$is calculated by
$$
SE[\hat{f}(x_0)] = \ell_0^T \hat{\mathbf{C}} {\ell}_0
$$
where, $\ell_0^T =(1, x_0, x_0^2, \cdots, x_0^d)$, and $\hat{\mathbf{C}}$ is the covariance matrix of the estimated coefficients $\beta_j$, $j=0, 1, \cdots, d$ obtained from the OLS. 


## Step functions 
A step function is a piece-wise constant function. Cut a $X$ variable into $K+1$ regions using $K$ cut points and then either use one-hot coding with $K+1$ dummy variables (and no intercept, in this case, each coefficient can be interpreted as the average value in that region) or create $K$ dummy variables with an intercept to represent all those regions (in this case, the average value in that region equals to the intercept plus the coefficient). Choice of *cut-points* (knots) can be problematic. Binning the $X$ variable amounts to convert a continuous variable into an *ordered categorical variable*. 

The basis functions are simply *indicator  functions* on each region:
$$
b_j(x_i)= I(c_j\le x_i< c_{j+1}).
$$

## Piececwise polynomials
It overcomes the disadvantage of polynomial basis which imposes a *global structure*. It fits separate low-degree polynomials over different regions of $X$ separated by *knots*.  

## Splines
Splines are piece-wise polynomials of degree $d$   that are  continuous up to $d-1$ derivatives at each knot.   E.g. a cubic spline with $K$ knots has continuity up to second derivative at each knot, and has degree of freedom of $K+4$. 

- linear spline: with knots $\xi_k$, $k=1, \cdots, K$ is a piece-wise linear polynomial that is continuous at each knot. 
$$
y= \beta_0+\beta_1 b_1(x)+\cdots + \beta_{K+1}b_{K+1}(x)+\epsilon, 
$$
where, $b_k$ are **basis functions** defined by 
\begin{align}
b_1(x) & = x \\
b_{k+1}(x) & = (x- \xi_k)_{+}, \qquad k=1, \cdots K
\end{align}
Here the *positive part* is defined by 
$$x_{+}=\begin{cases}
x, &  \text{ if } x>0 \\
0 &  \text{ otherwise}
\end{cases}
$$

- cubic splines: with knots $\xi_k$, $k=1, \cdots, K$ is a piecewise cubic polynomials with continuous derivatives up to order 2 at each knot. 

$$
y= \beta_0+\beta_1 b_1(x)+\cdots + \beta_{K+3}b_{K+3}(x)+\epsilon, 
$$
**knot placement**: General principle is that placing more knots in places where the function might vary most rapidly. In practice, it is common  to place them  at uniform quantiles of the observed $X$. This can be done by specifying a dof, and then let the  algorithm to calculate the knots at uniform quantiles. Note a natural spline have more internal knots than a regression spline for the same dof. 

  For a **regular cubic spline**, the **basis functions** are defined by 
\begin{align}
b_1(x) & = x \\
b_2(x) & = x^2 \\
b_3(x) & = x^3 \\
b_{k+3}(x) & = (x- \xi_k)^3_{+}, \qquad k=1, \cdots K
\end{align}

  **dof**: $K+4$ number of parameters (including the intercept). 
  A regression spline can have high variance at the outer range of the predictors. To remedy this, one can use a *natural cubic spline*. 

  a **natural cubic spline** extrapolates linearly ( as a linear function) beyond the internal knots. This adds $2\times 2$ extra constrains. A natural cubic spline allows to put more internal knots for the same degree of freedom as a regular cubic spline. 
    
  **dof**: $K+2$ ($K$ only counts the internal knots; including the intercept). 

  For a **smoothing spline**: it is the solution $g$ to the following problem:
  $$
  \text{minimize}_{g\in S}\sum_{i=1}^n(y_i-g(x_i))^2 +\lambda \int g''(t)dt
  $$

The first term is the *loss* RSS and encourages $g(x_i)$ matches $y_i$. The second term is the *penalty* that  penalize the *variability* in $g$ (measured by $g''(t)$) by a tuning parameter $\lambda \ge 0$. If $\lambda=0$ (no constraints on $g$), then the solution is just an interpolating polynomial. If $\lambda\to \infty$, then $g$ is a linear function (because its second derivative is zero). $\lambda$ controls the bias-variance trade-off. 

The smoothing spline is in fact a natural spline with knots at unique values of $x_i$. But it is **different** than the natural spline. It is a *shrunk* version of a natural cubic spline, otherwise it would have too large (*nominal*) dof (number of parameters) because it has knots at unique values of $x_i$.   It avoids the knot selection issue and leaving a single $\lambda$ to tune. An *effective degrees of freedom* can be calculated for a smoothing spline as 
$$
df_\lambda =\sum_{i=1}^n {\{\mathbf{S}_\lambda}\}_{ii}, 
$$
where $\mathbf{S}_\lambda$ is a $n\times n$ matrix determined by $\lambda$ and $x_i$ such that the vector of $n$ fitted values can be written as 
$$
\hat{\mathbf{g}}_\lambda =\mathbf{S}_\lambda \mathbf{y}.
$$
$df_\lambda$ decreases from $n$ to 2 as $\lambda$ increases from 0 to $\infty$. 

The LOO cross-validation error can be efficiently computed  by 
$$
\text{RSS}_{cv}(\lambda) =\sum_{i=1}^n (y_i-\hat{g}_\lambda ^{(-i)}(x_i))^2=\sum_{i=1}^n \left[ \frac{y_i-\hat{g}_\lambda (x_i)}{1-\{ {\mathbf S}_\lambda\}_{ii}} \right]^2
$$
- Local Regression: a non-parametric method. It is similar to spline, but allowing regions overlap. With a sliding weight function of *span* $s$, fit separate (constant, linear, quadratic, for instance) fits over the range of $X$ by weighted least squares. 

The span $s$ plays the similar role as $\lambda$ in a smoothing spline, it controls the flexibility of the local regression. The smaller $s$ is, the more *local and wiggle* will be the fit. 

Local regression is a *memory based* procedure, because like KNN, all training data are needed each time when making a prediction. 

Local regression can be generalized to *varying coefficient models* that fits a multiple linear regression model that is global in some variables but local in another, such as time. 

Local regression can be naturally extends to $p$-dimension using a $p$-dimensional neighborhood, but really used when $p$ is larger than 3 or 4 because there will be generally very few training examples near $x_0$ (curse of dimensionality)

- GAM (Generalized Additive Models): can be considered as an extension of multiple linear regression, replacing each feature $\beta_jX_j$ with an nonlinear function $f_j(X_j)$. 
  $$
  y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) +\cdots + f_p(x_{ip})  + \epsilon
  $$
GAM can mix different  $f_j$, for example, a spline, or a linear term or even include low order interactive terms. The coefficients are hard to interpret, but the fitted values are of interest. 

GAM can be used in fitting a logistic regression model, that is 
$$
\log \frac{p(X)}{1-p(X)} =\beta_0+f_1(X_1)+f_2(X_2)+\cdots +f_p(X_p)
$$

When fitting a GAM, and if OLS can not be used (such as when a smoothing spline is used), then the *back fitting* iterative method can be used: randomly initialize all variable coefficients; repeatedly hold all but one variable fixed, and perform a simple linear regression on that single variable, and update the corresponding coefficients until convergence. Convergence is typically very fast. 

**Pros and Cons of GAM**

  * flexible to model $f_j$, eliminating the need to try different transformations on each variable 
  * potentially more accurate prediction
  * because the model is additive, can easily examine the effects of $X_j$ on $Y$ by holding all of the other variables fixed. 
  * The smoothness of $f_j$ can be summarized by the effective dof. 
  * interaction terms $X_jX_k$ can be added. 
  * low dimensional interaction functions of the form $f_{jk}(X_j, X_k)$ can be added. Such term can be fit using a two-dimensional smoothers such as local regression or two dimensional splines. 
  
  
  
  

## Homework:
- Conceptual: 1--5
- Applied: At least one. 

## Code Snippet
### Python
```


```

### Numpy
```
Wage['education'].cat.categories # .cat is the categorical method accessor
Wage['education'].cat.codes
pd.crosstab(Wage['high_earn'], Wage['education'])

np.column_stack([Wage_['age'],
                         Wage_['year'],
                         Wage_['education'].cat.codes-1])

Xs = [ns_age.transform(age),
      ns_year.transform(Wage['year']),
      pd.get_dummies(Wage['education']).values] # -> 5 education levels: 1-hot coding
X_bh = np.hstack(Xs)

```

### Pandas
```
cut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints. pd.cut is similar
```


### Graphics
```
ax.legend(title='$\lambda$');

```

### ISLP and statsmodels
```


```

### sklearn

### Useful code snippets
#### plot a model fit with confidence interval
```
def plot_wage_fit(age_df, 
                  basis, # ISL model object
                  title):

    X = basis.transform(Wage)
    Xnew = basis.transform(age_df)
    M = sm.OLS(y, X).fit()
    preds = M.get_prediction(Xnew)
    bands = preds.conf_int(alpha=0.05)
    fig, ax = subplots(figsize=(8,8))
    ax.scatter(age,
               y,
               facecolor='gray',
               alpha=0.5)
    for val, ls in zip([preds.predicted_mean,
                      bands[:,0],
                      bands[:,1]],
                     ['b','r--','r--']):
        ax.plot(age_df.values, val, ls, linewidth=3)
    ax.set_title(title, fontsize=20)
    ax.set_xlabel('Age', fontsize=20)
    ax.set_ylabel('Wage', fontsize=20);
    return ax
```
#### Fitting with a step function

```
cut_age = pd.qcut(age, 4) # cut based on the 25%, 50%, and 75% cutpoints
# note pd.get_dummies(cut_age) is the X matrix
summarize(sm.OLS(y, pd.get_dummies(cut_age)).fit()) 

```

#### Fitting a spline 
```
#specifying internal knots

bs_age = MS([bs('age',
                internal_knots=[25,40,60],
                name='bs(age)')]) #rename the variable names 
Xbs = bs_age.fit_transform(Wage) # Xbs == bs_age above
M = sm.OLS(y, Xbs).fit()
summarize(M)

# specifying df 
bs_age0 = MS([bs('age',
                 df=3, # df count does not include intercept. df=degree+ #knots
                 degree=0)]).fit(Wage)
Xbs0 = bs_age0.transform(Wage)
summarize(sm.OLS(y, Xbs0).fit())

BSpline(df=3, degree=0).fit(age).internal_knots_

# Fit a natural spline
ns_age = MS([ns('age', df=5)]).fit(Wage) #df=degree+ #knots -2
M_ns = sm.OLS(y, ns_age.transform(Wage)).fit()
summarize(M_ns)

# fit a smoothing spline
X_age = np.asarray(age).reshape((-1,1))
gam = LinearGAM(s_gam(0, lam=0.6)) #gam is the smoothing spline model with a given lambda
gam.fit(X_age, y)

#Fiting a smoothing spline with an optimized lambda
gam_opt = gam.gridsearch(X_age, y)


# Fitting a smoothin spline by specifying a df (not including intercept)
fig, ax = subplots(figsize=(8,8))
ax.scatter(X_age,
           y,
           facecolor='gray',
           alpha=0.3)
for df in [1,3,4,8,15]:
    lam = approx_lam(X_age, age_term, df+1) # find the lambda corresponding to a df. 
    age_term.lam = lam # update lambda
    gam.fit(X_age, y)
    ax.plot(age_grid,
            gam.predict(age_grid),
            label='{:d}'.format(df),
            linewidth=4)
ax.set_xlabel('Age', fontsize=20)
ax.set_ylabel('Wage', fontsize=20);
ax.legend(title='Degrees of freedom');




```
### GAM
```
### manually contruct basis 
ns_age = NaturalSpline(df=4).fit(age) #df counts do not include intercepts. -> 4 columns
ns_year = NaturalSpline(df=5).fit(Wage['year']) # -> 5 cols
Xs = [ns_age.transform(age),
      ns_year.transform(Wage['year']),
      pd.get_dummies(Wage['education']).values] # -> 5 education levels: 1-hot coding
X_bh = np.hstack(Xs)
gam_bh = sm.OLS(y, X_bh).fit()

### Examinge partial effect

age_grid = np.linspace(age.min(),
                       age.max(),
                       100)
X_age_bh = X_bh.copy()[:100] # Take the first 100 rows of X_bh
# calculate the row mean and make it a row vector in the shape of 1Xp, then broadcast
X_age_bh[:] = X_bh[:].mean(0)[None,:] 
X_age_bh[:,:4] = ns_age.transform(age_grid)# replace the first 4 cols with basis functions evalued at age_grid
preds = gam_bh.get_prediction(X_age_bh) #gam_bh is the GAM model with all 14 basis
bounds_age = preds.conf_int(alpha=0.05)
partial_age = preds.predicted_mean
center = partial_age.mean() # center of the prediction 
partial_age -= center # center the prediction for better viz
bounds_age -= center
fig, ax = subplots(figsize=(8,8))
ax.plot(age_grid, partial_age, 'b', linewidth=3)
ax.plot(age_grid, bounds_age[:,0], 'r--', linewidth=3)
ax.plot(age_grid, bounds_age[:,1], 'r--', linewidth=3)
ax.set_xlabel('Age')
ax.set_ylabel('Effect on wage')
ax.set_title('Partial dependence of age on wage', fontsize=20);


### Using a smoothing spline and pygam package
#### Specifying lambda
#### default \lambda = 0.6 is used.
gam_full = LinearGAM(s_gam(0) + # spline smoothing applies to the first col of the feature matrix
                     s_gam(1, n_splines=7) + # smoothing applied to the 2nd col 
                     f_gam(2, lam=0)) # smothing applied to the 3rd col: a factor col
Xgam = np.column_stack([age,  #stack as columns
                        Wage['year'],
                        Wage['education'].cat.codes]) 
gam_full = gam_full.fit(Xgam, y)

gam_full.summary() # verbose summary

#### Plot partial effect using a plot_gam from ISLP.pygam
fig, ax = subplots(figsize=(8,8))
plot_gam(gam_full, 0, ax=ax) # 0: partial plot of the first component: age
ax.set_xlabel('Age')
ax.set_ylabel('Effect on wage')
ax.set_title('Partial dependence of age on wage - default lam=0.6', fontsize=20);

### Specifying df
age_term = gam_full.terms[0]
age_term.lam = approx_lam(Xgam, age_term, df=4+1)
year_term = gam_full.terms[1]
year_term.lam = approx_lam(Xgam, year_term, df=4+1)
gam_full = gam_full.fit(Xgam, y)

#### Plot partial effect
fig, ax = subplots(figsize=(8, 8))
ax = plot_gam(gam_full, 2)
ax.set_xlabel('Education')
ax.set_ylabel('Effect on wage')
ax.set_title('Partial dependence of wage on education',
             fontsize=20);
ax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);

```
#### Anova for GAM
```
gam_0 = LinearGAM(age_term + f_gam(2, lam=0)) # note age_term is a s_gam with df=4 defined above 
gam_0.fit(Xgam, y)
gam_linear = LinearGAM(age_term +
                       l_gam(1, lam=0) +
                       f_gam(2, lam=0))
gam_linear.fit(Xgam, y)
anova_gam(gam_0, gam_linear, gam_full)

```

#### Logistic GAM
```
gam_logit = LogisticGAM(age_term + 
                        l_gam(1, lam=0) +
                        f_gam(2, lam=0))
gam_logit.fit(Xgam, high_earn)

```

#### LOESS
```
lowess = sm.nonparametric.lowess
fig, ax = subplots(figsize=(8,8))
ax.scatter(age, y, facecolor='gray', alpha=0.5)
for span in [0.2, 0.5]:
    fitted = lowess(y,
                    age,
                    frac=span,
                    xvals=age_grid)
    ax.plot(age_grid,
            fitted,
            label='{:.1f}'.format(span),
            linewidth=4)
ax.set_xlabel('Age', fontsize=20)
ax.set_ylabel('Wage', fontsize=20);
ax.legend(title='span', fontsize=15);
```