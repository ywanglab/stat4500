# Chapter 6: Linear Model Selection and Regrularization

Linear models are *interpretable* and often shows small variance. They are fitted by *OLS*. There are other methods that can either provide alternatives or improve linear regression models in terms of *prediction accuracy* (especially when $p>n$) and automatic *feature selection* for improved interpretability. 

There are three classes of methods:

- subset selection: pick a subset of the $p$ predictors that best explains the response. 
- Shrinkage (regularization). With an added regularizing term, estimated parameters are shrunken to zero relative the OLS estimates. If $L^2$ regularization is used, all coefficients are shrunk toward zero by the same proportion; while if $L^1$ is used, then some coefficients will become zero, leading to actual *variable selection* or *sparse representation*. 
- Dimension reduction. Project $p$-predictors to a $M$ ($M<p$) dimensional subspace. Each new direction is a linear combination (or projection) of the $p$-variables. These $M$-projections can then be used to fit a linear regression model(**PCR** if the $M$-projections are obtained in an unsupervised way; or **PLR** if these $M$-projections are obtained in a supervised way))

## Best Subset Selecttion
**Algorithm**

1. Fit the data with the *null model* $\mathcal{M}_0$, which contains no predictors. This model simply set $Y=\text{mean}(y_i)$. 
2. for $k=1, 2, \cdots, p$: fit $p \choose k$ models containing exactly $k$ predictors. Pick the best one that having the smallest RSS or largest $R^2$ (or *deviance* for classification problem, i.e., $-2\max \log (\text{likelihood})$ on the *training set*, called $\mathcal{M}_k$. Note for each categorical variable with $L$-level, there are $L-1$ dummy variables. 
3. Select the best one among $\mathcal{M}_0, \cdots, \mathcal{M}_p$ using cross-validation or other measures such as $C_p (AIC)$, $BIC$ or adjusted $R^2$. If cross-validation is used, then Step 2 is repeated on each training fold, and the validation errors are averaged to select the best $k$. The the model $\mathcal{M}_k$ fit on the full training set is delivered for chosen $k$. 

Best subset selection suffers
- high computation: needs to compute $2^p$ models
- overfitting due to the large search space of models

## Stepwise selection
Both Forward and Backward   selection are stepwise selection. They are used when $p$ is large.  They searches over $1+p(p+1)/2$ models  and are *greedy* algorithm and is not guaranteed to find the best possible model out of all $2^p$ models.

- Backward selection requires $n>p$ (so that the full model can be fit);
- Forward selection can be used when $n<p$ but only fits up to models with $n$ variables. 
- One can combine forward and backward selection to a hybrid selection. 

### Forward Stepwise Selection
Adding one variable at at time that offers the greatest additional improvement. 

**Algorithm**

1. Fit the data with the *null model* $\mathcal{M}_0$, which contains no predictors. This model simply set $Y=\text{mean}(y_i)$. 
2. for $k=1, 2, \cdots, p-1$: 
  * fit all $p-k$ models that augment the predictors in $\mathcal{M}_k$ with one additional predictor. 
  * Pick the best one that having the smallest RSS or largest $R^2$ on the training set, called $\mathcal{M}_{k+1}$. 
3. Select the best one among $\mathcal{M}_0, \cdots, \mathcal{M}_p$ using cross-validation or other measures such as $C_p (AIC)$, $BIC$ or adjusted $R^2$. 


### Backward Stepwise Selection
It begins with the full model with all variables, and iteratively removing one variable at at time. 

**Algorithm**

1. Fit the data with the *full model* $\mathcal{M}_p$, which contains all predictors. 
2. for $k=p, p-1, \cdots, 1$: 
  * fit all $k$ models that contains all but one of the predictors in $\mathcal{M}_k$. 
  * Pick the best one that having the smallest RSS or largest $R^2$ on the training set, called $\mathcal{M}_{k-1}$. 
3. Select the best one among $\mathcal{M}_0, \cdots, \mathcal{M}_p$ using cross-validation or other measures such as $C_p (AIC)$, $BIC$ or adjusted $R^2$. 

## Model selection 

Models with all predictors always have the smallest $RSS$ or largest $R^2$ on the *training set*. Therefore they are not suitable to choose the best one among models with different number of predictors. 

We ought to *estimate the test error* on a test set. This may be done 
- indirectly by adjusting the training error to account for the bias due to overfitting: $C_p$ (equivalently, AIC in case of linear model with Gaussian errors), BIC and adjusted $R^2$. 

  * Mallow's $C_p$: 
  $$
  C_p =\frac{1}{n}(RSS+2d\hat{\sigma}^2)
  $$
  where, $d$ us the number of parameters and $\hat{\sigma}^2 \approx Var{\epsilon}$, typically estimated by using the *full model* containing all variables. $C_p$ is an unbiased estimate of test MSE. 
  
  * AIC is defined for models fit by maximmum likelihood. 
  $$
  AIC = -2\log L + 2d
  $$
  where, $L$ is the maximum likelihood function for the estimated model. For linear regression with Gaussian error, $AIC\propto C_p$. 
  
  
  * BIC
  $$
  BIC = \frac{1}{n}( RSS + \log (n)d \hat{\sigma}^2 )
  $$
  Since $\log n>$ for $n>7$, the BIC places a higher penalty on models with many variables, and hence select smaller models than $C_p$. 
  
  * Adjusted $R^2$ (larger value is better)
  
    $$
    \text{Adjusted }R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}
    $$
    $RSS/(n-d-1)$ may increase or decrease depends on $d$. Unlike $R^2$, adjusted $R^2$ pays a price for the inclusion of unnecessary variables in a model. 
    
    $C_p$, AIC, BIC, adjusted $R^2$,are not appropriate in high-dimentional setting, as the estimated $\hat{\sigma}^2 \approx 0$ (when $p\ge n$). 
    
- directly by cross-validation (or validation). It does not require estimate $\sigma^2$. It has a wide range of usage, as it may difficult to estimate $d$ or $\sigma^2$. One can choose the model that has the smallest test error or using the *one-standard-error rule* to  select the model that has a smaller size: 
  * calculate the SE of the estimated test MSE for each model size. 
  * identify the lowest test MSE 
  * choose the smallest model for which its test MSE is within one SE of the lowest point. 

## Shrinkage methods for Variable selection
The shrinkage offers an alternative to selecting variables by adjusting a hyperparameter that trades-off RSS and the model parameter magnitudes. Shrinkage methods will produce a different set of coefficients for a different $\lambda$. Cross-validation may be used to select the best $\lambda$. After the $\lambda$ is selected, one can fit a final model using the entire training data set. 

The reason shrinkage methods may perform better than OLS is rooted in bias-variance trade-off:  as $\lambda$ increases, the flexibility of the model decreases b ecause of shrunk coefficients, leading to decreased variance but increased bias. 

### Ridge regression: minimize the following objective 
$$
RSS + \lambda \sum_{j=1}^p \beta_j^2
$$
The ridge regression is equivalent to 
$$
\text{minimize } RSS \qquad \text{subject to } \sum_{j=1}^p \beta_j^2 \le s
$$
for some $s\ge 0$. 

- It encourages the model parameters to shrink toward zero and find a balance between RSS and model parameter magnitudes. Cross-validation is used to find the best tuning parameter $\lambda$. When $\lambda$ is large, $\beta_j\to 0$. Note, Ridge shrinks all coefficients and include all $p$ variables. 

- The OLS coefficients estimates are *scale equivariant*: regardless of how $X_j$ is scaled, $X_j\hat{\beta}_j$ remain the same: if $X_j$ is multiplied by $c$, this will simply leads to $\hat{\beta}_j$ be scaled by a factor of $1/c$. 

- In contrast, when multiplying $X_j$ by a factor, this may significantly change the ridge coefficients. Ridge coefficients depends on $\lambda$ and the scale of $X_j$, and may even on the scaling of other predictors.  Therefore, it is best practice to *standardize the predictors* before fitting a ridge model: 
$$
\tilde{x}_{ij} =\frac{x_{ij}}{\frac{1}{n} \sum_{i=1}^n(x_{ij} - \bar{x}_j)}
$$
- Ridge regression works best in situations where the OLS estimates have high variance, especially when $p$ is large. 
- Ridge will include all $p$ variables in the final model. 

### The Lasso (Least Absolute Shrinkage and Selection Operator)

  * The Lasso replaces the $\ell^2$ error with $\ell^1$ penalty. Lasso can force some coefficients to become exactly zero when $\lambda$ is large enough. Thus it can actually performs *variable selection* hence better interpretation. Again, cross-validation is employed to select $\lambda$. 
  
  * The reason Lasso can perform variable selection is because the objective function is equivalent to 
  
  $$\text {minimizing RSS}, \text{subject to } \sum_{j=1}^p |\beta_j| \le s
  $$
for some $s$. The contour of RSS in general only touch the $\ell_1$ ball at its vertex, at which a minimum is obtained with some variables vanishes. In contrast, in the ridge situation, the $\ell_2$ ball is round, and in general, the contour of the RSS function only touches the sphere at a surface point where a minimum is obtained with no variable vanishes. 

- Neither ridge nor the lasso will universally dominate the other. When the response depends on a small number of predictors, one may expect lasso performs better; but in practice, this is never known in advance. 

- Combining ridge and lasso leads to *elastic net* method. 
- it is well known that ridge tends to give similar values coefficient values to correlated variables, while lasso may give quite different coefficient values to correlated variables. 
- ridge regression shrinks all coefficients by the same proportion. While lasso perform **soft-thresholding**, shrink all coefficients by similar amount, and sufficient small coefficients are shrunk all the way to zero. 

  both ridge and lasso can be considered as computationally feasible approximation to the *best subset selection* which can be equivalently formulated as:
  $$
  \text{minimize } RSS \qquad \text{subject to } \sum_{j=1}^p I(\beta_j\ne 0) \le s.
  $$
- **Bayesian formulation**: Both ridge and lasso can be interpreted as maximize the **posterior probability** (MAP)
$$
p(\beta|X,Y)\propto f(Y|X,\beta) p(\beta|X)=f(Y|X,\beta)p(\beta)
$$
where $p(\beta)= \prod_{j=1}^p g(\beta_j)$ with some density function $g$ is the believed prior on $\beta$. 
  * if $g$ is Gaussian with mean zero and standard deviation a function of $\lambda$, then it follows the solution $\beta$ given by the ridge is the same as maximizing the posterior $p(\beta|X,Y)$, that is, $\beta$ is the posterior mode. In fact, $\beta$ is also the posterior mean. Since the Gaussian prior is flat at  near zero, ridge assumes the coefficients are randomly distributed about zero. 
  * if $g$ is double-exponential (Laplace) with mean zero and scale parameter a function of $\lambda$, then it follows the solution $\beta$ given by the lasso is the same as maximizing the posterior $p(\beta|X,Y)$, that is, $\beta$ is the posterior mode. In this case $\beta$ is **not** the posterior mean. Since the Laplacian prior is steeply peaked at zero, lasso expects a priori that many coefficients are (exactly) zero. 
  
## Dimension reduction methods: transforming $X_j$. 
  There are two types of dimension reduction methods for regression: a) PCA regression, b) Partial list squares PLS. Both are designed to handle when the OLS breaks down due to that  there are large number of correlated variables. 
  
### PCA regression: first use PCA to obtain $M$- PCA as linear combinations (directions) of the original $p$ predictors: 
  $$
  Z_m =\sum_{j=1}^p \phi_{mj} X_j, \qquad 1\le m \le M,  
  $${#eq-PCA}
where, the $\phi_{mj}$ are called **PCA loadings**, and subject to the norm $\sum_{j=1}^p\phi_{mj}^2=1$ for each $m$. Note $Z_m$ is a vector of length equal to the length of $X_j$, which is the number of data points $n$. The component of $Z_m$: $z_{im}$, $1\le i \le n$ are called **PCA scores**. $z_{im}$ is a *single number summary* of the $p$ predictors with the $m$-th PCA for the $i$-th observation. PCA is not a feature selection method.  

The first PCA defines the direction that contains the largest variance in $X$, and minimize the sum of squared perpendicular distances to each point (the projection error on the PCA), that is it defines the line that is *as close as possible* to the data; (In fact, the first PCA is given by the eigenvector of the largest eigenvalue of the covariance matrix $\frac{1}{n-1}X^TX$). The second PCA is orthogonal to the first PCA and has the second largest variance and is uncorrelated with the first, and so on. These directions are obtained in an *unsupervised way*, as $Y$ is not used to obtain these components. Consequently, there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response. 

PCA is typically conducted after standardizing the data $X$, as without scaling, the high variance variables will tend to have higher influence on the obtained PCAs. 

We then use OLS to fit a linear regression model 
$$
y_i =\theta_0 +\sum_{m=1}^M \theta_m z_{im}+\epsilon_i, \qquad i=1,2,\cdots, n
$${#eq-PCAR}

After substitute @eq-PCA into equation @eq-PCAR, one can find that 
$$
\sum_{m=1}^M \theta_mz_{im} =\sum_{j=1}^p \beta_jx_{ij}
$$
with 
$$
\beta_j = \sum_{m=1}^M \theta_m\phi_{mj}.
$${#eq-pcar-beta}
Eq. @eq-pcar-beta has the potential to bias the coefficient estimates, but selecting $M<< p$ can significantly reduce the variance. 
So model @eq-PCAR is a special case of linear regression subject to the constants @eq-pcar-beta.  

PCR and ridge are closely related and one can think of ridge regression as a continuous version of PCR.

### Partial Least Squares

Similar to PCAR, PLS also first identifies a new set of features $Z_1, Z_2, \cdots, Z_m$, each of which is a linear combinations of the original features, and then fits a linear model via OLS with these new $M$ features. 

But PLS identifies these new features in a *supervised way*, that is, PLS uses $Y$ in order to identify the new features that not only approximate the old features well, but also are *related to the response*, i.e., these new features explain both the response and the predictors. 

First PLS standardizes the $p$ predictors. PLS identifies the first component 
$Z_1 = \sum_{j=1}^p \phi_{1j}X_j$ by choosing $\phi_{1j}=<X_j, Y>$, the coefficient from the simple linear regression of $Y$ onto $X_j$. Since this coefficient is equal to the correlation between $Y$ and $X_j$, PLS places the highest weight on the variables that are most strongly related to $Y$. The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response. 

Next, PLS orthogonalize each $X_j$ with respect to $Z_1$, that is, replace each $X_j$ with the residual by regressing $X_j$ on $Z_1$, and then form the new direction $Z_2$ using the residuals of the $X$ variables. This amounts to using the informaton of $X$ not explained by $Z_1$. And the process repeats. 

When $p$ is large, especially $p>n$, the forward selection method, shrinkage methods (lasso or ridge), PCR, PLR fit a *less flexible* model, hence particularly useful in performing regression in high-dimensional settings. 

## Homework:
- Conceptual: 1--4
- Applied: At least one. 

## Code Snippet
### Python
```
np.isnan(Hitters['Salary']).sum()

```

### Numpy
```
np.linalg.norm(beta_hat) #L2 norm. ord=1: L1  ord='inf': max norm.
```

### Pandas
```
Hitters.dropna();
soln_path = pd.DataFrame(soln_array.T,
                         columns=D.columns,
                         index=-np.log(lambdas))
soln_path.index.name = 'negative log(lambda)'
```


### Graphics
```
ax.errorbar(np.arange(n_steps), 
            cv_mse.mean(1), #mean of each row (model)
            cv_mse.std(1) / np.sqrt(K), #estimate standard error of the mean
            label='Cross-validated',
            c='r') # color red
            
ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--') # plot a verticalline
```

### ISLP and statsmodels
```
#Estimate Var(epsilon)

design = MS(Hitters.columns.drop('Salary')).fit(Hitters)
design.terms # to see the variable names in the design matrix
Y = np.array(Hitters['Salary'])
X = design.transform(Hitters)
sigma2 = OLS(Y,X).fit().scale  #.scale: RSE: residual standard error estimating 

# Forward Selection using ISLP.models and a scoring function
from ISLP.models import \
     (Stepwise,
      sklearn_selected,
      sklearn_selection_path)
strategy = Stepwise.first_peak(design,
                               direction='forward',
                               max_terms=len(design.terms))
hitters_Cp = sklearn_selected(OLS,
                               strategy,
                               scoring=neg_Cp)
                               #default scoring MSE, will choose all variables
hitters_Cp.fit(Hitters, Y) # the same as hitters_Cp.fit(Hitters.drop('Salary', axis=1), Y)
hitters_Cp.selected_state_

#Forward selection using cross-validation
strategy = Stepwise.fixed_steps(design,
                                len(design.terms),
                                direction='forward')
full_path = sklearn_selection_path(OLS, strategy) #using default scoring MSE
full_path.fit(Hitters, Y) # there are , 19 variables, 20 models
Yhat_in = full_path.predict(Hitters)

#calculate in-sample mse

mse_fig, ax = subplots(figsize=(8,8))
insample_mse = ((Yhat_in - Y[:,None])**2).mean(0) #Y[:,None]: add a second axis, create a column vector
                        #[yw] mean(0): calculate mean along row, i.e., for each col. mean(1): calculate mean for each row

#Cross-validation
K = 5
kfold = skm.KFold(K,
                  random_state=0,
                  shuffle=True)
Yhat_cv = skm.cross_val_predict(full_path,
                                Hitters,
                                Y,
                                cv=kfold)
# Cross-validation mse
cv_mse = []
for train_idx, test_idx in kfold.split(Y):
    errors = (Yhat_cv[test_idx] - Y[test_idx,None])**2
    cv_mse.append(errors.mean(0)) # column means
cv_mse = np.array(cv_mse).T

#validation approach using ShuffleSplit
validation = skm.ShuffleSplit(n_splits=1, # only split one time. 
                              test_size=0.2,
                              random_state=0)
for train_idx, test_idx in validation.split(Y):
    full_path.fit(Hitters.iloc[train_idx], #note needing to use iloc
                  Y[train_idx])
    Yhat_val = full_path.predict(Hitters.iloc[test_idx])
    errors = (Yhat_val - Y[test_idx,None])**2
    validation_mse = errors.mean(0)

```

### sklearn
```
rom sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression

#Best subset selection using 10bnb

D = design.fit_transform(Hitters)
D = D.drop('intercept', axis=1) #needs to drop intercept
X = np.asarray(D)
path = fit_path(X, 
                Y,
                max_nonzeros=X.shape[1]) #fit_path: a funciton from l0nb. use all variables
                # max_nonzeros: max nonzero coefficients in the fitted model.

# Ridge Regression
soln_array = skl.ElasticNet.path(Xs, # standardized, no intercept
                                 Y,
                                 l1_ratio=0., #ridge
                                 alphas=lambdas)
# Using pipline
ridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)
scaler = StandardScaler(with_mean=True,  with_std=True)
pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])
pipe.fit(X, Y)
ridge.coef_

# Validation

validation = skm.ShuffleSplit(n_splits=1,
                              test_size=0.5,
                              random_state=0) # validation is a generator
ridge.alpha = 0.01
results = skm.cross_validate(ridge,
                             X,
                             Y,
                             scoring='neg_mean_squared_error',
                             cv=validation) # using the strategy defined in validation
-results['test_score']

# GridSearchCV()
param_grid = {'ridge__alpha': lambdas}
grid = skm.GridSearchCV(pipe,
                        param_grid,
                        cv=validation, # or use cv=kfold (5-fold CV defined separately)
                        scoring='neg_mean_squared_error') #default scoring=R^2
grid.fit(X, Y)
grid.best_params_['ridge__alpha']
grid.best_estimator_
grid.cv_results_['mean_test_score']
grid.cv_results_['std_test_score']


#Plot CV MSE
ridge_fig, ax = subplots(figsize=(8,8))
ax.errorbar(-np.log(lambdas),
            -grid.cv_results_['mean_test_score'],
            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))
ax.set_ylim([50000,250000])
ax.set_xlabel('$-\log(\lambda)$', fontsize=20)
ax.set_ylabel('Cross-validated MSE', fontsize=20);

# Use ElasticNetCV()
ridgeCV = skl.ElasticNetCV(alphas=lambdas, # ElasticNetCV accepts a sequence of alphas
                           l1_ratio=0,
                           cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler), # scaling is done once. 
                         ('ridge', ridgeCV)])
pipeCV.fit(X, Y)
tuned_ridge = pipeCV.named_steps['ridge']
tuned_ridge.mse_path_
tuned_ridge.alpha_ # best alpha
tuned_ridge.coef_

# Evaluating test Error of Cross-validated Ridge 

outer_valid = skm.ShuffleSplit(n_splits=1, 
                               test_size=0.25,
                               random_state=1)
inner_cv = skm.KFold(n_splits=5,
                     shuffle=True,
                     random_state=2)
ridgeCV = skl.ElasticNetCV(alphas=lambdas, # a sequence of lambdas
                           l1_ratio=0,
                           cv=inner_cv) # K-fold validation
pipeCV = Pipeline(steps=[('scaler', scaler),
                         ('ridge', ridgeCV)]);
                         
                         
results = skm.cross_validate(pipeCV, 
                             X,
                             Y,
                             cv=outer_valid,
                             scoring='neg_mean_squared_error')
-results['test_score']

# Lasso regression
lassoCV = skl.ElasticNetCV(n_alphas=100, #test 100 alpha values
                           l1_ratio=1,
                           cv=kfold)
pipeCV = Pipeline(steps=[('scaler', scaler),
                         ('lasso', lassoCV)])
pipeCV.fit(X, Y)
tuned_lasso = pipeCV.named_steps['lasso']
tuned_lasso.alpha_
tuned_lasso.coef_
np.min(tuned_lasso.mse_path_.mean(1)) # miminum avg mse

#to get the soln path
lambdas, soln_array = skl.Lasso.path(Xs, # standarsized, no -intercept
                                    Y,
                                    l1_ratio=1,
                                    n_alphas=100)[:2]

#PCA and PCR
pca = PCA(n_components=2)
linreg = skl.LinearRegression()
pipe = Pipeline([('scaler', scaler), 
                 ('pca', pca),
                 ('linreg', linreg)])
pipe.fit(X, Y)
pipe.named_steps['linreg'].coef_
pipe.named_steps['pca'].explained_variance_ratio_

# perform Grid search
param_grid = {'pca__n_components': range(1, 20)} #PCA needs n_components >0
grid = skm.GridSearchCV(pipe,
                        param_grid,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
grid.fit(X, Y)

# cross-validation a null model
cv_null = skm.cross_validate(linreg,
                             Xn,
                             Y,
                             cv=kfold,
                             scoring='neg_mean_squared_error')
-cv_null['test_score'].mean()

#PLS
pls = PLSRegression(n_components=2, 
                    scale=True) # standarsize the data 
pls.fit(X, Y) # X has no-intercept 

# Cross-validation
param_grid = {'n_components':range(1, 20)}
grid = skm.GridSearchCV(pls,
                        param_grid,
                        cv=kfold,
                        scoring='neg_mean_squared_error')
grid.fit(X, Y)


```